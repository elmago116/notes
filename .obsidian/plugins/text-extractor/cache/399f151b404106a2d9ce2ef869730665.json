{"path":"Clippings/PDF/2301.11182v2.pdf","text":"Towards a semantic approach in GLAM Labs: the case of the Data Foundry at the National Library of Scotland Journal Title XX(X):1–15 ©The Author(s) 0000 Reprints and permission: sagepub.co.uk/journalsPermissions.nav DOI: 10.1177/ToBeAssigned www.sagepub.com/ SAGE Gustavo Candela 1,2 Abstract GLAM organisations have been exploring the benefits of publishing their digital collections in a wide variety of forms since the 2000s. Many institutions, and in particular libraries, have adopted the Semantic Web and Linked Data principles to their main catalogues. Recent advances in technology and innovative approaches concerning the reuse of the digital collections by means of computational access have paved the way for the creation of Labs within GLAM organisations. In this work, we present a framework to transform the datasets made available by GLAM organisations under open licenses into LOD. The framework has been applied to three metadata datasets made available by the Data Foundry at the National Library of Scotland. The results of this work are publicly available and can be applied to other domains such as digital humanities and data science. Keywords Semantic Web, Linked Open Data, Collections as Data, Digital Libraries, Data Quality 1 Introduction During the last decade, technological advances have paved the way for a new context in the GLAM (Galleries, Libraries, Archives and Museums) sector, in which their rich collections are expected to play an important role in the research community. Institutions are adapting to this new environment in terms of services, collections, metadata and skills. 1–3 New approaches are focused on improving the access of the digital collections by using computational methods and increasing datafication based on Data Science, Artificial Intelligence and Machine Learning. 4,5 In this context, GLAM Labs have emerged as a new trend addressing the publication under open licenses and reuse of the digital collections in innovative and creative ways. 6 For example, the Data Foundry at the National Library of Scotland publishes data openly and in a variety of file formats, 1 LC Labs 2 supports digital transformation at the Library of Congress and KB Lab makes available for the public experimental tools and datasets based on the collections published by the National Library of the Netherlands. 3 The Semantic Web was introduced early in the 2000s as an extension of the Web based on standards to make it available as machine-readable data. 7 The Semantic Web enables the publication of Linked Open Data (LOD) by using URIs to identify the resources and creating links to external repositories in order to enrich them. 8 Many organisations have explored the benefits of adopting the Semantic Web principles to their catalogues including the Library of Congress, 4 the Biblioth`eque nationale de France5 and the Biblioteca Virtual Miguel de Cervantes, 9 amongst others. While organisations provide digital collections as datasets ready for reuse, these are made available in many cases 1National Library of Scotland 2Centro Biblioteca Virtual Miguel de Cervantes, Universidad de Alicante (Spain) using traditional standards such as MARC (Machine- Readable Cataloging), less expressive vocabularies such as Dublin Core and text formats such as CSV. In this sense, the application of the Semantic Web principles by GLAM organisations has several benefits including: i) the description of the information with machine-readable, standard and controlled vocabularies; ii) the enrichment with external repositories; and iii) the reuse by third-party actors. In addition, this is a key aspect in order to be part of international and collaborative initiatives such as the Linked Open Data Cloud 6 and Wikidata. The objective of the present study is to introduce a framework to transform into LOD the datasets made available by GLAM organisations under open licenses following best practices. 4,10 The framework was then applied to three metadata datasets made available on the Data Foundry at the National Library of Scotland. The results of this study are publicly available and can be applied to other domains such as digital humanities and data science. The main contributions of this paper are as follows: (a) a framework to transform metadata datasets into LOD following best practices; (b) a compilation of Jupyter Notebooks to reproduce the transformation; and (c) the results of the application to a selected set of datasets made available by the National Library of Scotland. These contributions are intended to encourage GLAM institutions to adopt LOD as a key element for publishing their datasets. The paper is organised as follows: after a brief description of the state of the art in Section 2, Section 3 describes the framework employed to transform the datasets into LOD. The application of the method and results are shown in Section 4. The paper concludes with an outline of the adopted method, and general guidelines on how to use the results and future work. Prepared using sagej.cls [Version: 2017/01/17 v1.20]arXiv:2301.11182v2 [cs.DL] 20 Sep 2023 2 Journal Title XX(X) 2 Related work The following subsections describe previous approaches of publication of LOD by GLAM organisations, examples of reuse and data quality assessments. 2.1 Publishing datasets in GLAM institutions The publication of digital collections by GLAM institutions has increased during the last decade. These may include different material including maps, images, metadata, OCR text, sound or video. Datasets are available in a wide variety of repositories such as GitHub, Zenodo or dedicated websites. In this context, Labs have emerged as a new area in GLAM institutions to make available for the public digital collections under open licenses in order to promote their reuse in innovative ways. Labs provide diverse datasets published under open licenses, prototypes of tools that reuse the contents, APIs, software, collections of reproducible Jupyter Notebooks, etc. In addition, datasets are in general classified by type of material such as metadata, digitised content, maps, etc. For example, the Data Foundry at the National Library of Scotland provides digitised collections, metadata in MARC and Dublin Core, maps and spatial data and organisational information as datasets. Table 1 shows an overview of GLAM Labs made available by institutions. Many organisations have started to explore the benefits of applying the Semantic Web and Linked Data principles to their main catalogues following best practices. 10 Table 2 shows an overview of LOD repositories made available by relevant GLAM institutions. Previous works have addressed a preliminary analysis of how the transformation could be performed such as the National Bibliography of Scotland. 11 While the different controlled vocabularies available and used to describe the bibliographic information share the common goal to help users to find, identify, select, and obtain the resources they need, in some cases, classes were not part of all models or have been renamed. 12 Some examples of controlled vocabularies are: • Bibliographic Framework (BIBFRAME): it was developed by the Library of Congress to produce linked metadata in RDF (Resource Description Framework). 13 • Bibliographic Ontology (BIBO): 14 provides main concepts and properties for describing citations and bibliographic references such as books and articles on the Semantic Web. • CIDOC Conceptual Reference Model (CIDOC CRM): 15 it represents an ontology for cultural heritage information. It describes the concepts and relations relevant to the documentation of cultural heritage. • Europeana Data Model (EDM): 16 the formal specifi- cation of the classes and properties that are used in Europeana. • Functional Requirements of Bibliographic Records (FRBR): 17 a conceptual model published by IFLA originally in 1997 known as WEMI (Work, Expres- sion, Manifestation, and Item). 18 The FRBR model was complemented and further developed with FRAD (Functional Requirements for Authority Data) and FRSAD (Functional Requirements for Subject Author- ity Data). An ontology has been published based on this model. 7 • IFLA Library Reference Model (LRM): 19 the original models FRBR, FRAD, and FRSAD were replaced by the IFLA LRM model. It was developed to resolve inconsistencies between the three separate models. • Resource, Description and Access (RDA): 8 a compi- lation of entities and properties as controlled vocabu- laries based on RDA is available for the public at the RDA Registry as RDA element sets and RDA value vocabularies in RDF. • Schema.org: 9 a collaborative approach to create, maintain, and promote schemas for structured data on the Internet. Other approaches are based on the aggregation of bibliographic catalogues and authority files from several institutions such as the Library of Congress and the national libraries of Norway and Finland. 10 In addition, innovative linked data editors within the library domain have been developed in order to facilitate the adoption of controlled vocabularies such as BIBFRAME Editor 11 and Sinopia.12 2.2 Data quality assessment and reuse Several aspects can be considered when reusing digital collections such as the license used and the data quality. Previous works are focused on the definition of data quality criteria classified by dimensions to assess LOD repositories. 20–22 Many of these criteria are assessed by using SPARQL queries. More advanced approaches are based on the definition of node constraints in the form of Shape Expressions (ShEx) to assess RDF datasets that can be used as additional documentation of the repositories. 23 Other initiatives are based on the data quality evaluation of traditional metadata formats for bibliographic records, such as MARC. 24 Table 3 shows an overview of previous approaches to assess the data quality of LOD repositories. Recently, there has been a significant focus on the application of Machine Learning (ML), Computer Vision and Artificial Intelligence to the digital collections published by GLAM institutions. 1,29–31 Several reuse approaches based on the LOD provided by GLAM institutions addressed the creation of visualisations charts such as maps. Others initiatives such as the RDA Entity Finder13 enable users to browse through the bibliographic Work, Expression, Manifestation and Item entities by using web interfaces. Other approaches have proposed the use of the LOD for Digital Humanities techniques based on knowledge discovery. 32 With regard to the data enrichment, Wikidata has played a relevant role in GLAM institutions. Properties have been created to link the resources such as authors and works. 21 Wikidata follows a collaborative edition approach in which the community is able to enrich the resources. Moreover, Wikidata enables the storage of events, datasets or projects by defining a model to store the information (e.g., the Coding da Vinci cultural hackathons14 and the information about the members and computational access projects of the International GLAM Labs Community). In addition, Wikibase, the software that serves as storage Prepared using sagej.cls Candela, G. 3 Table 1. Overview of GLAM Labs published by relevant GLAM institutions. Institution URL Austrian National Library https://labs.onb.ac.at/en/dataset/lod Biblioteca Nacional de Espa˜na https://bnelab.bne.es Biblioteca Virtual Miguel de Cervantes https://data.cervantesvirtual.com Biblioth`eque nationale de France https://api.bnf.fr Biblioth`eque nationale du Luxembourg https://data.bnl.lu British Library https://www.bl.uk/projects/british-library-labs German National Library https://www.dnb.de/EN/lds Library of Congress https://labs.loc.gov National Library of the Netherlands https://lab.kb.nl National Library of Scotland https://data.nls.uk Royal Danish Library https://labs.kb.dk Royal Library of Belgium https://www.kbr.be/en/projects/digital-research-lab Table 2. Overview of LOD repositories published by GLAM organisations. Institution Vocabulary URL Austrian National Library EDM, BIBFRAME, RDA https://labs.onb.ac.at/en/dataset/lod Biblioteca Nacional de Espa˜na FRBR http://datos.bne.es Biblioteca Virtual Miguel de Cervantes RDA https://data.cervantesvirtual.com Biblioth`eque nationale de France FRBR https://data.bnf.fr Biblioth`eque nationale du Luxembourg - https://data.bnl.lu BNB Linked Data Platform BIBO https://bnb.data.bl.uk Europeana EDM https://pro.europeana.eu/page/sparql German National Library BIBFRAME https://www.dnb.de/EN/lds Library of Congress BIBFRAME https://id.loc.gov National Digital Data Archive of Hungary DC, FOAF, schema.org, DBpedia http://lod.sztaki.hu National Library of Finland schema.org, BIBFRAME https://data.nationallibrary.fi National Library of the Netherlands schema.org, LRM https://data.bibliotheken.nl National Library of Sweden KB Base Vocabulary https://libris.kb.se/sparql Rijksmuseum EDM, SKOS https://data.rijksmuseum.nl/controlled-vocabularies Table 3. Overview of previous works regarding LOD data quality assessment classified by approach. Domain Approach description Libraries SPARQL and others 21,25 Libraries ShEx 26 General ShEx 23 General SPARQL and others 20,22,27 General Ontology approach 28 system for Wikidata, has been explored and analysed as a potential tool for cataloguing purposes in libraries. 33 Other repositories with which to enrich a GLAM dataset may include Virtual International Authority File (VIAF), the GeoNames gazetteer, Getty Vocabularies 15 or Library of Congress Subject Headings (LCSH). 16 These efforts provide extensive demonstration of how digital collections can be made available for the public and transformed to LOD using different vocabularies and assessment methods. Nevertheless, while previous works are based on the publication of the main catalogues of the institutions as LOD, to our best knowledge, none of the work to date have considered the use of the datasets made available by GLAM Labs. In addition, none of the previous approaches have been focused on the datasets provided by the National Library of Scotland. This analysis will be useful for the GLAM community to identify and agree best practices regarding the publication of datasets as LOD. 3 A Linked Open Data framework to enhance digital collections in GLAM institutions This section introduces the Linked Open Data framework to enhance digital collections in GLAM institutions. The scope of this study is limited to metadata datasets provided by GLAM institutions under open licenses, such as the ones published in GLAM Labs. The steps of the framework are described below. 3.1 Selecting a dataset The selection of a dataset is the first step and refers to the identification of a digital collection that will be used. GLAM institutions provide digital collections in several locations such as websites, code repository platforms, datasets repositories, etc. Recently, GLAM Labs have emerged as innovative section in GLAM institutions in which open datasets can be found. Relevant aspects to consider when choosing a dataset are for example: i) using known open licenses such as Creative Commons; ii) providing additional documentation such as formats available, provenance information and examples of use; and iii) using a permanent identifier to ease the citation. 4,34 In addition, data quality is becoming a key element when using advanced methods based on NLP, ML and AI. 35,36 Recently, carbon footprint information has become a crucial element for example in terms of processing, training AI models, and generation and curation of the datasets that can be additionally considered when selecting a dataset for reuse. 29,37 Prepared using sagej.cls 4 Journal Title XX(X) 3.2 Data source analysis This step includes the preliminary analysis of the original contents provided by the dataset. In some cases, this information might be provided as additional documentation such as the case of the Theatre Posters published by the Data Foundry at the National Library of Scotland. 17 Other types of useful documentation may include research guides about the datasets, such as the case of Chronicling America,18 and Jupyter Notebooks that explores the content and provides a preliminary idea of the information (e.g., dates, titles, etc.). 38 When this information is not available, the difficulty depends on the original data format. For example, CSV files can be easily explored using the Python software Pandas.* Other formats based on XML such as MARCXML and Dublin Core may require other tools to extract the contents. Several aspects can be considered for the analysis including: • the total number of resources. • the types of resources included (e.g., writer, maps, works, paintings, etc.). • the number of different metadata fields. • the different values contained in a particular metadata field (e.g., roles or places). • main subjects or topics provided by the contents. When using large datasets in terms of size this step may require an additional task to split the original source or to transform it to an manageable format. For instance, Pandas enables users to explore large data sets efficiently using the chunksize attribute. Other approaches are based on the use of Hadoop and Spark. 39 This process can provide as an output useful information about the size of the dataset and the expressiveness of the vocabularies used. 3.3 Extracting information The extraction of information consists on the identification of the key metadata elements that are relevant to be included in the final dataset. Some examples include title, author, publisher, copyright, sources, etc. Depending on the original format the metadata fields may be accessed in different ways. For example, a title in Dublin Core is provided in a dc:title metadata field while in MARCXML the title is provided by the field 245. Several software libraries can be used to extract the metadata such as Pandas, XSLT 19 and pymarc 20 for MARCXML. Note that when using a transformation tool that automatically generates the RDF output, this step is not strictly required as is described in the next step. 3.4 Data modelling and transformation to RDF The adoption of the Semantic Web concepts requires the use of standard and controlled vocabularies that describe how the information is modelled in terms of classes and properties. RDF provides the basis to model and define the data by using triples. While best practices recommend reusing existing vocab- ularies to foster the connection of resources, 40 there exist tools to facilitate the creation of new vocabularies. 41 Some examples of vocabularies are introduced in Section 2. *https://pandas.pydata.org Table 4. Overview of approaches and tools for working with and creating RDF. Tool Description Apache Jena 21 Java software library BIBFRAME Edi- tor HTML interface for editing BIBFRAME metadata EasyRdf 22 PHP software library marc2bibframe2 23 XSLT script to transform from MARC to BIBFRAME OpenRefine 24 Transformation interface RDF JavaScript Libraries 42 RDF JavaScript Libraries RDFLib 25 Python software library Table 5. Overview of previous works concerning the automatic transformation of the original sources into RDF vocabularies. Reference Approach description Marimba 44 Convert MARC records to FRBR marc2bibframe2 43 Convert MARC records to BIBFRAME MARC2RDA 45 Mapping between MARC and RDA MINT 46 Cross-domain metadata to EDM marc2dc † Convert MARC records to DC When using controlled vocabularies, classes represent a particular type in a domain. For example, the class author represents a writer in a library. Instances of a particular class are identified by URIs that may follow patterns (e.g., /author/{id}). Other approaches are based on the use of generic URIs (e.g., identifiers). The transformation to LOD relies on a mapping process using as input the original sources and generating as an output the metadata described using controlled vocabularies. Each resource is identified by an URI following the patterns established. Then, the different properties are assigned to the resource. The transformation can be performed using different methods and tools. For example, many software libraries in several programming languages (e.g., Java, Python or PHP) are available to work with RDF. However, the use of these software libraries requires highly specialised and extensive technical skills. Table 4 presents an overview of methods and tools for working with and creating RDF. Other approaches are based on the use of tools that transform the original metadata into a specific vocabulary. For instance the Library of Congress provides a set of tools for the transformation from MARC to BIBFRAME. 43 The use of these tools requires the adjustment of several configuration parameters including the domain name (by default http://example.org), the record that contain the identifier or the serialization format to obtain the output result. Table 5 shows an overview of previous works addressing the transformation of MARC into RDF controlled vocabularies. In addition, there are tools to enable users to edit the data using a specific vocabulary. For example, the BIBFRAME Editor is an HTML user-friendly interface that enables users to edit the metadata. 26 Recently, mappings between traditional metadata formats and advanced controlled vocabularies have been published.27 In addition, dedicated websites have been published to Prepared using sagej.cls Candela, G. 5 describe the classes and properties that can be used to describe the information as RDF.28 Previous works have addressed the comparison of bibliographic models such as LRM and BIBFRAME. 47 3.5 Data enrichment Linked Data enables the interconnection of resources by using the property owl:sameAs. The use of this property eases the enrichment process by adding contextual information from external repositories. In other cases, new properties are specifically created to add links from a particular dataset. For example, Wikidata provides different properties for each repository.29 Section 2 provides a list of potential repositories to use for enriching a dataset. In order to identify resources in text and link them to external repositories several techniques can be used. For example, NER (Named Entity Recognition) enable the identification of entities in the text such as authors, organisations and locations. More advanced approaches are based on the identification and linking to repositories such as Wikidata and DBpedia. 48 An example of entity linking tool is DBpedia spotlight that can be used to automatically annotating mentions of DBpedia resources in text. 49 Other software libraries such as spaCy30 and CoreNLP 31 enable the creation of pipelines and the training of models for NER and entity linking methods. 3.6 Quality assessment Data quality has become a crucial aspect when publishing a digital collection. Several techniques can be used in this sense based on manual and automatic tasks, and data quality criteria as is described in Section 2. While SPARQL is a powerful language to query an RDF dataset (e.g., see, for example, Listing 1, more sophisticated approaches are based on the mining of ShEx schemas to define node constraints to be tested against RDF datasets. Listing 2 shows an example of ShEx schema to validate resources typed as foaf:Person and schema:Person. SELECT (COUNT( d i s t i n c t ? s ) a s ? t o t a l ) WHERE { ? s dc : s u b j e c t ? s u b j e c t . FILTER r e g e x ( ? s u b j e c t , ” G a e l i c ” ) } Listing 1: SPARQL query to retrieve the number of resources containing a subject with the text ”Gaelic”. ex : P e r s o n { r d f : t y p e [ f o a f : P e r s o n ] ; r d f : t y p e [ schema : P e r s o n ] ; f o a f : name x s d : s t r i n g ; s k o s : p r e f L a b e l x s d : s t r i n g ; schema : name x s d : s t r i n g } Listing 2: Example of ShEx schema to validate resources typed as foaf:Person and schema:Person. Additional aspects to consider when assessing a LOD repository include the inclusion of labels in multiple languages, the use of a public SPARQL endpoint, the provision of machine-readable licensing information or the validation of the external URIs. 21,22 3.7 Publication and Exploration The final step corresponds to the publication of the datasets in a public repository. Recently, several platforms have become popular in the research community such as DataCite, Zenodo and Hugging Face. The use of permanent identifiers such as DOI (Digital Object Identifier) is crucial to support the citation. LOD repositories are in many cases made available using a public SPARQL endpoint. This requires the set up of an environment to install an RDF storage system. Some examples include Virtuoso, 32 Jena TDB 33 and RDF4J. 34 In addition, RDF datasets can be described by means of controlled vocabularies such as the Vocabulary of Interlinked Datasets (VoID). 50 This information can be useful for data cataloging and archiving of the datasets as well as to provide additional metadata to users. Listing 3 shows an example of VoID description describing an overview of the properties available. Platforms such as the Linked Open Data Cloud enable the inclusion of the LOD datasets in aggregators that can be useful in terms of visibility. The publication can include examples of use based on prototypes and tools. New approaches have recently emerged to show how to explore a dataset that enable the combination of code and text and can be executed in the cloud based on the use of Jupyter Notebooks. 35 4 Results This section presents the application of the framework proposed in Section 3 to three datasets provided by the Data Foundry at the National Library of Scotland: • Moving Image Archive (MIA): this dataset represents the descriptive metadata from the Moving Image Archive catalogue which is the Scotland’s national collection of moving image. 51 The dataset is provided as a zip file containing a MARCXML and a Dublin Core files. The dataset is available under a license Creative Commons Zero 1.0 Universal. • National Bibliography of Scotland (NBS): this dataset corresponds to the bibliographic records for the National Bibliography of Scotland and part of an ongoing programme of work to create and expand the bibliography. 52 The dataset references materials published in Scotland from National Library of Scotland’s main catalogue. • Bibliography of Scottish Literature in Translation (BOSLIT): it was a Voyager database that was maintained by the BOSLIT Committee including information about Scottish literature in translation which aimed to serve the needs of academic researchers, writers and translators, libraries, schools, literature administrators and general readers. 53 We have selected the Data Foundry since it is one of the most relevant GLAM Labs/services providing digital collections as datasets amenable for computational use. 54–57 Prepared using sagej.cls 6 Journal Title XX(X) : Mo v in g I ma g eA r ch i ve a v o i d : D a t a s e t ; d c t e r m s : t i t l e ” Moving Image A r c h i v e ” ; d c t e r m s : d e s c r i p t i o n ”RDF d a t a e x t r a c t e d from t h e Moving Image A r c h i v e d a t a s e t ” ; d c t e r m s : l i c e n s e < h t t p s : / / c r e a t i v e c o m m o n s . o r g / p u b l i c d o m a i n / mark / 1 . 0 / > ; d c t e r m s : p u b l i s h e r : NLS ; d c t e r m s : c o n t r i b u t o r :GC; d c t e r m s : s o u r c e < h t t p s : / / d a t a . n l s . uk / d a t a / m e t a d a t a − c o l l e c t i o n s / moving −image − a r c h i v e / >; d c t e r m s : s o u r c e < h t t p s : / / g i t h u b . com / h i b e r n a t o r 1 1 / n l s >; d c t e r m s : m o d i f i e d ” 2022 −11 −09 ” ˆ ˆ x s d : d a t e ; v o i d : f e a t u r e < h t t p : / / www. w3 . o r g / n s / f o r m a t s / T u r t l e >; v o i d : dataDump < h t t p s : / / raw . g i t h u b u s e r c o n t e n t . com / h i b e r n a t o r 1 1 / n l s / m a s t e r / r d f / d a t a s e t E n r i c h e d . t t l >; v o i d : v o c a b u l a r y < h t t p : / / xmlns . com / f o a f / 0 . 1 / > ; v o i d : v o c a b u l a r y < h t t p : / / www. e u r o p e a n a . eu / s c h e m a s / edm / >; v o i d : v o c a b u l a r y < h t t p s : / / schema . o r g / >; v o i d : c l a s s e s 7 ; v o i d : e x a m p l e R e s o u r c e < h t t p s : / / e x a m p l e . o r g / f i l m /0001 >; v o i d : p r o p e r t i e s 2 3 ; v o i d : t r i p l e s 2 6 3 4 7 6 ; Listing 3. Example of VoID description for the Moving Image Archive dataset. Table 6. Overview of the RDF datasets generated from the Data Foundry. Description MIA NBS BOSLIT Classes 7 190 124 Properties 23 185 129 External links 75 4274 611 Triples 263476 47882223 4503316 The datasets have been selected according to the following criteria: i) they are included in the metadata datasets section; ii) they are available under open licenses; and iii) they are available as MARCXML and Dublin Core format. The vocabularies presented in Section 2 were analysed in order to identify the most appropriate vocabulary to describe the metadata provided by each of the datasets. As a result, we used as main vocabularies Schema.org for the Moving Image Archive and BIBFRAME for the National Bibliography of Scotland and BOSLIT datasets. Table 6 presents an overview of the features of the final datasets. The code repository including a collection of reproducible Jupyter Notebooks is available on GitHub. 36 Each of the Jupyter Notebooks included in the collection describes a step in the framework applied to the dataset. The LOD datasets generated are available as dump files. 4.1 Data modelling and transformation to LOD The three datasets were transformed to LOD using different techniques and vocabularies as is described below. For the Moving Image Archive dataset, the transformation was based on the vocabulary Schema.org, using as main entity the class VideoObject. Schema.org was used since it was adopted by the web community as the shared vocabulary among search engines and provides a rich set of properties to describe multimedia resources such as videos. 58 Additional classes and properties from EDM and FOAF have been used to describe the resources. Note that the original identifier is used to create the URIs in the RDF dataset (e.g., http://example.org/(filmRef)0002#Instance) Following previous approaches, 9,44,47 the MARCXML datasets National Bibliography of Scotland and BOSLIT were transformed into BIBFRAME using a XSLT template that was applied to each original record. The final RDF dataset was loaded into a Apache Jena TBS RDF storage system. Figure 1 shows an overview of the steps followed. The XSLT converter application process the fields of each MARC record and build the two main elements, a bf:Work and a bf:Instance. In addition, the pro- cess generates a bflc:adminMetaData property of the bf:Work to include provenance information and gener- ates the bf:hasItem properties of the bf:Instance. URL patterns are based on the baseuri parameter (default http://example.org/), the record ID of the MARC record (by default the value of the 001 field), and a hash URI for the new element. The output is serialized as RDF/XML. For elements that are not typed as bf:Work or bf:Instance, the hash URI is constructed from the element class, the field number, and the position of the field in the MARC record (e.g., http://example.org/9923749153804341#Agent100-12). Fig- ure 2 shows an overview of the main classes used to model the data based on BIBFRAME. In BIBFRAME, the class bf:Hub is used as an abstract resource that functions as a bridge between two works. MARC metadata fields (e.g., 130 and 240) aiming at storing information about works that has appeared under varying titles may generate an additional resource typed as bf:Hub including the uniform title and the author when available. Figure 3 shows an example of how the metadata representing translations of the work Treasure island in the BOSLIT dataset are modelled according to the BIBFRAME vocabulary. 4.2 Data quality assessment Following previous approaches, 21–23 a validation of the RDF datasets was performed in terms of several dimensions as is described below. Table 10 shows the results obtained. With regard to the accuracy dimension, the syntactic validity of RDF documents was assessed based on a random sample of 100 resources per dataset and using the W3C RDF Prepared using sagej.cls Candela, G. 7 Figure 1. Overview of the steps to transform into RDF the National Bibliography of Scotland and BOSLIT datasets using as main vocabulary BIBFRAME. Figure 2. Overview of the main BIBFRAME classes used to model data based on the National Bibliography of Scotland. Figure 3. Example of BBIFRAME data modelling based on the work Treasure island. Validation Service.37 All the documents were assessed as correct. With regard to the semantic validity of literals criterion, regular expressions were used to test textual descriptions included such as roles and text dates. A sample of 100 resources per dataset was tested using a manual revision in order to identify inconsistencies. For example, some inconsistencies in text descriptions concerning roles (e.g., auhtor, orgasnizer, ”.”, etc.) were identified in the National Bibliography of Scotland dataset. Some textual metadata fields in the Moving Image Archive provide a list of abbreviations (e.g., ”a.d.” is the abbreviation for ”art director”) that will require a text processing task to extract all the information. Additional examples of abbreviated roles (e.g., ed.) are included in the BOSLIT dataset. The criterion semantic validity of triples was evaluated using a gold standard as a reference in order to assess whether the metadata was correct. The metadata of a random sample of 50 resources were manually evaluated against VIAF and Wikidata in order to assess whether the metadata information such as title, date of birth, date of death and name was correct. All the datasets satisfied this criterion. Prepared using sagej.cls 8 Journal Title XX(X) With regard to the check of duplicate entities criterion, some potential issues were identified in the National Bibliography of Scotland and BOSLIT RDF datasets: • since the RDF transformation is focused on work records, similar authors that potentially are the same are treated as different resources by using different URLs. See, for example, Listing 4. The use of external identifiers such as VIAF can help in this sense to facilitate the clustering of records. • some BIBFRAME works obtained as output in the transformation process included a bf:expressionOf property linking to a bf:Hub resource. According to the BIBFRAME documentation, a resource typed as bf:Hub represent an abstract resource that functions as a bridge between two Works. For example, MARC fields 130 38 and 24039 store standardised strings such as uniform titles that generates a bf:Hub resource including the main author, the title and the language, when available in the original sources. Since the URL patterns for the bf:Hub and the bf:Work are based on the identifier of the work, the XSLT template may generated repeated bf:Hub resources with the same information but with different URL. Regarding the dimension trustworthiness, all the datasets were manually curated in a closed system. While provenance information about the datasets is provided in the Data Foundry, however, no information is available about RDF statements or resources. Consistency was measured using several SPARQL queries to identify constraints provided by the vocabularies based on statements such as owl:disjointWith. For example, this enables the identification of resources that are typed as Person and Corporate Body. All the datasets satisfied the constraints. In addition, a collection of ShEx schemas has been automatically generated to test against the RDF datasets using the tool sheXer. 23 The ShEx schemas can be used as additional documentation for users and curators to better understand how the data is modelled. Concerning the relevancy dimension, none of the datasets supports the ranking of statements, entities or relations, which could be used to, for example, state the order of contributors in a work. With regard to the completeness dimension, the schema completeness criterion measures the extent to which classes and relations are included in a dataset. Following previous approaches, a gold standard has been defined with the common classes and properties that could be included based on vocabularies such as BIBFRAME and Schema.org (see Table 8). With respect to the column completeness criterion, it measures the rate of instances having a specific property, averaged for all the properties shown in Table 8. The Moving Image Archive dataset obtained a lower score since authors are included in many cases as text descriptions using the metadata field schema:creditText. With reference to the population completeness criterion, it determines the extent to which the datasets covers a basic population. The coverage of resources was compared with a list of resources provided by Wikidata: i) a list of Scottish poets, essayists, novelists and writers for the National Bibliography of Scotland and BOSLIT datasets (see Listing 5); and ii) a list of Scottish filmmakers and explorers for the Moving Image Archive. Timeliness dimension includes several criteria to measure for a digital object the extent to which it is sufficiently up- to-date. The criterion frequency measures if the resource includes metadata about when was created, stored, accessed or cited. The frequency of updates was consulted in all the datasets. However, none of the datasets provide information about the validity period or the modification date of statements. Regarding the easy of understanding dimension, user- friendly URLs for the resources are used in all the datasets. For example, Table 9 shows the URL patterns used to identify the resources in the Moving Image Archive RDF dataset. All the resources provided a rdfs:label property to describe them and text descriptions were only provided in English. In addition, the datasets used a understandable RDF serialization format such as Turtle. In the particular case of the National Bibliography of Scotland and BOSLIT datasets, the tool marc2bibframe only generated RDF/XML as a result of the transformation using the XSLT template. With regard to the interoperability dimension, the datasets do not use blank nodes or RDF reification. Only one serialization format is provided for dataset. All the datasets used external vocabularies to describe the metadata including EDM, FOAF, BIBFRAME and Schema.org. Concerning the accessibility dimension, since the URLs are not resolvable due to the use of a testing domain URL, many criteria in this dimension are not satisfied. While the datasets are not available by means of a public SPARQL endpoint, they are available as dump files including metadata. The licencing dimension includes a criterion that measures the extent to which the datasets provide a machine-readable licence. All the datasets satisfied this criterion. The interlinking dimension provides two criteria to measure the external links included in the datasets. The interlinking via owl:sameAs criterion computes the rate of instances having at least one external link. In addition to the property owl:sameAs, other properties were identified that link to external repositories such as bf:role, bf:language and bf:geographicCoverage, in particular for the National Bibliography of Scotland and BOSLIT datasets. With regard to the validity of external URIs criterion, the number of HTTP errors were computed for a random sample of 500 URLs shown in triples linking to external repositories. In particular, some issues were identified in the National Bibliography of Scotland dataset regarding the interlinking dimension: • the automatic transformation process uses the MARC List for Languages controlled vocabulary to create links using the original text information. However, in some cases, the links created are based on non-existent identifiers (e.g., see, for example, the white space in the following URL http://id.loc.gov/vocabulary/geographicAreas/e-uk- %20st and http://id.loc.gov/vocabulary/languages/d). Prepared using sagej.cls Candela, G. 9 PREFIX b f :< h t t p : / / i d . l o c . gov / o n t o l o g i e s / b i b f r a m e /> PREFIX r d f s :< h t t p : / / www. w3 . o r g / 2 0 0 0 / 0 1 / r d f − schema#> SELECT ? l a b e l ? a WHERE { ? s b f : c o n t r i b u t i o n ? c . ? c b f : a g e n t ? a . ? a r d f s : l a b e l ? l a b e l . FILTER r e g e x ( s t r ( ? l a b e l ) , ” S t e v e n s o n , R o b e r t L o u i s ” ) } LIMIT 10 Listing 4. Example of SPARQL query to retrieve the works related with the author Robert Louis Stevenson from the National Bibliography of Scotland. Table 7. Overview of the results retrieved using the SPARQL query shown in Listing 4. Label URL Stevenson, Robert Louis, 1850-1894 http://example.org/9929751083804341#Agent100-9 Stevenson, Robert Louis, 1850-1894 http://example.org/9923749153804341#Agent100-12 Stevenson, Robert Louis, 1850-1894 http://example.org/9923749153804341#Agent800-28 Stevenson, Robert Louis, 1850-1894 http://example.org/9915244463804341#Agent100-13 Stevenson, Robert Louis, 1850-1894 http://example.org/9944502973804341#Agent100-10 SELECT DISTINCT ? s L a b e l ? v i a f WHERE { VALUES ? o c c u p a t i o n { wd : Q36180 wd : Q49757 wd : Q6625963 wd : Q11774202 } ? s wdt : P31 wd : Q5 . ? s wdt : P106 ? o c c u p a t i o n . ? s wdt : P19 wd : Q22 . ? s wdt : P214 ? v i a f SERVICE w i k i b a s e : l a b e l { bd : s e r v i c e P a r a m w i k i b a s e : l a n g u a g e ” en ” . } } LIMIT 50 Listing 5. SPARQL query to retrieve Scottish poets, essayists, novelists and writers from Wikidata. Table 8. Classes and properties selected to assess the completeness criteria. Pattern Description Author name, date of birth, date of death Work/Video title, date of publication, author Organisation name Place name Table 9. URL patterns used to identify the resources in the Moving Image Archive RDF dataset. Pattern Description /author/name authors /film/id videos /location/name geographic locations /organisation/name organisations • resources typed as bf:Work are linked to bf:Agent resources by using a resource typed as bf:Contribution including the name of the author and the role. The Relators terms vocabulary 40 provided by the Library of Congress includes a list of roles that the XSLT template is able to map against the original sources. Up to 15 roles were mapped including, for example, http://id.loc.gov/ vocabulary/relators/aut and http: //id.loc.gov/vocabulary/relators/ctb. The XSLT template generated additional roles that were included uniquely as text descriptions instead of URLs (e.g., translator, illustrator, printer, honoree, presenter, etc.). 4.3 Exploration Enriching the data with external repositories such as Wikidata and GeoNames enables the addition of contextual information that can be useful for different purposes including visualisation and data analysis. For example, Figure 4 shows the map visualisation as a result of the SPARQL query that retrieves from Wikidata all the locations provided by the Moving Image Archive dataset. The transformation process to BIBFRAME included an enrichment step by using the controlled vocabularies provided by the Library of Congress such as MARC List for Languages, 59 MARC List for Geographic Areas 60 and MARC Relators terms. In addition, Wikidata provides the property P4801 to add identifiers for an item based on the controlled vocabularies maintained by the Library of Congress that can be used to add contextual information.41 Listing 6 shows and example of SPARQL query to retrieve Prepared using sagej.cls 10 Journal Title XX(X) Table 10. Summary of results according to the data quality criteria to assess LOD classified by dimensions based on previous works. 21,22 Dimension Criterion MIA NBS BOSLIT Accuracy Syntactic validity of RDF documents 1 1 1 Syntactic validity of literals 0.98 0.96 0.91 Semantic validity of triples 1 1 1 Check of duplicate entities 1 1* 1* Trustworthiness On dataset level 1 1 1 On statement level 0 0 0 Using unknown and empty values 0 0 0 Consistency Consistency of schema restrictions during insertion of new statements 0 0 0 Consistency of statements with respect to class constraints 1 1 1 Consistency of statements with respect to relations constraints 1 1 1 Relevancy Creating a ranking of statements 0 0 0 Completeness Schema completeness 1 1 1 Column completeness 0.61 0.79 0.69 Population completeness 0.35 0.5 0.3 Timeliness Frequency 0.5 0.5 0.5 Specification of the validity period of statements 0 0 0 Specification of the modification date of statements 0 0 0 Ease of understanding Description of resources 1 1 1 Labels in multiple languages 0 0 0 Understandable RDF serialization 1 1 1 Self-describing URIs 1 1 1 Interoperability Avoiding blank nodes and RDF reification 1 1 1 Provisioning of several serialization formats 0.5 0.5 0.5 Using external vocabulary 1 1 1 Interoperability of proprietary vocabulary 1 1 1 Accessibility Dereferencing possibility of resources 0 0 0 Availability of the repository 0 0 0 Availability of a public SPARQL endpoint 0 0 0 Provisioning of an RDF export 1 1 1 Support of content negotiation 0 0 0 Linking HTML sites to RDF serializations 0 0 0 Provisioning of metadata 1 1 1 Licensing Provisioning machine-readable licensing information 1 1 1 Interlinking Interlinking via owl:sameAs 0.004 0,82 0,74 Validity of external URIs 1 0,66 1 the works in Spanish from the National Bibliography of Scotland dataset. The BOSLIT dataset is a source of information about Scottish literature in translation. In this way, we explored the dataset in order to analyse how the translations of a work are described using the vocabulary BIBFRAME. Listing 7 shows the number of translations in several languages for the works Treasure island and Strange case of Doctor Jekyll and Mister Hyde and the results are shown in Table 12. In addition, Listing 8 shows a SPARQL query to explore the translation relationships established between the resources in the BOSLIT dataset and the results are shown in Table 13. 4.4 Discussion Popular and relevant vocabularies often are accompanied of documentation, examples of use, and generation and edition tools. However, while the use of automatic tools eases the transformation process into RDF, there is need for additional customization in terms of features such as URL patterns and the use of external repositories to create links. The process can be constrained by the level of configuration allowed by the tool. In addition, the use of this type of tools relies on the assumption that the original data is of high quality. Moreover, understanding the output of the automatic process, in particular how the data is modelled, may be a cumbersome task. In this way, RDF software libraries can be useful in this context to model the data. The identification of unique items in an RDF dataset can be improved by including a manual revision. In some cases, the lack of information provided in the original sources (e.g., birth year) can make this process more complex. Linked Data promotes the use of URIs to identify resources. Resolvable URIs for the resources require the use of a domain (e.g., https://data.nls.uk/). In this work, we have used the domain https://example.org when evaluating the framework and for testing purposes. In addition, RDF storage systems are required for an efficient access to the data when using large datasets. Access and exploration of datasets using traditional metadata formats is constrained due to the use of the textual descriptions. The use of expressive languages to model the data enables the analysis of large datasets using a wide diversity of concepts (e.g., Person, Work, etc.) and properties as access points. In some cases, metadata fields (e.g., roles, dates, etc.) provide textual descriptions that can be further processed to provided a more expressive description of the information. In addition, the use of LOD enables the creation Prepared using sagej.cls Candela, G. 11 Table 11. Overview of the results retrieved using the SPARQL query shown in Listing 6. URL Title http://example.org/9944730413804341#Work El Palacio de Holyroodhouse http://example.org/999356403804341#Work La gente y los lugares http://example.org/9929767743804341#Work El Ingenioso Hidalgo Don Quixote de la Mancha. (Del Ingenioso Caballero Don Quixote de la Mancha.) http://example.org/9919385013804341#Work Una Gram´atica colonial del Quichua del Ecuador Figure 4. Map visualisation created from a SPARQL query in Wikidata using the metadata provided by the Moving Image Archive dataset. Table 12. Number of translations per language of the works Strange case of Doctor Jekyll and Mister Hyde and Treasure island included in the BOSLIT dataset. Title No. of works Strange case of Doctor Jekyll and Mister Hyde. Italian 61 Strange case of Doctor Jekyll and Mister Hyde. Spanish 61 Strange case of Doctor Jekyll and Mister Hyde. French 56 Treasure island. Spanish 172 Treasure island. German 137 Treasure island. Italian 116 Treasure island. French 113 Treasure island. Russian 64 Treasure island. Japanese 46 Treasure island. Dutch 42 of innovative visualisations that can be useful not only to explore the data but also to gain insight. 5 Conclusions Over the past few years, there has been a growing interest in publishing and reusing the digital collections made available by GLAM institutions. The adoption of the Semantic Web and Linked Data principles provide several benefits to the organisations in terms of interoperability and enrichment. Based on previous work, we defined a framework described in Section 3 for transforming metadata datasets published by relevant GLAM institutions to LOD. The framework was applied to three datasets published by the National Library of Scotland. The evaluation showed that the framework can be useful for other organisations willing to publish datasets as LOD following best practices. Future work to be explored includes the evaluation of additional datasets, the improvement of the framework to include additional types of datasets such as OCR and the exploration of data spaces to include the final datasets. Acknowledgements This work has been funded by The National Librarian’s Research Fellowship in Digital Scholarship 2022-23 at the National Library Scotland. Notes 1. https://data.nls.uk/ 2. https://labs.loc.gov/ 3. https://lab.kb.nl/ 4. https://id.loc.gov/ 5. https://data.bnf.fr/ 6. https://lod-cloud.net/ 7. https://vocab.org/frbr/core 8. http://www.rdaregistry.info/ 9. https://schema.org/ 10. https://share-vde.org/ 11. https://github.com/lcnetdev/bfe/ 12. https://sinopia.io/ 13. https://lab.kb.nl/tool/rda-entity-finder 14. https://www.wikidata.org/wiki/Wikidata: WikiProject Coding da Vinci 15. https://www.getty.edu/research/tools/ vocabularies/ 16. https://www.loc.gov/aba/publications/ FreeLCSH/freelcsh.html 17. https://data.nls.uk/data/metadata- collections/theatre-posters/ 18. https://guides.loc.gov/chronicling- america-topics 19. See, for example, https://github.com/vioil/ Transforming-MARCXML-with-XSLT/blob/ master/marc-to-dc.xsl 20. https://pypi.org/project/pymarc/ 21. https://jena.apache.org/ 22. https://www.easyrdf.org/ 23. https://github.com/lcnetdev/marc2bibframe2 24. https://openrefine.org/ Prepared using sagej.cls 12 Journal Title XX(X) PREFIX b f :< h t t p : / / i d . l o c . gov / o n t o l o g i e s / b i b f r a m e /> SELECT d i s t i n c t ? work ? t i t l e WHERE {? work b f : l a n g u a g e < h t t p : / / i d . l o c . gov / v o c a b u l a r y / l a n g u a g e s / spa> . ? work b f : t i t l e ? r e s T i t l e . ? r e s T i t l e b f : m a i n T i t l e ? t i t l e } Listing 6. Example of SPARQL query to retrieve the works in Spanish from the National Bibliography of Scotland. PREFIX b f :< h t t p : / / i d . l o c . gov / o n t o l o g i e s / b i b f r a m e /> SELECT ? m a i n T i t l e (COUNT( ? work ) a s ? t o t a l ) WHERE { ? work b f : e x p r e s s i o n O f ? exp . ? exp b f : t i t l e ? t i t l e . ? t i t l e b f : m a i n T i t l e ? m a i n T i t l e } GROUP BY ? m a i n T i t l e HAVING ( ? t o t a l >1) ORDER BY DESC ( ? t o t a l ) LIMIT 20 Listing 7. Number of translations in several languages included in the dataset BOSLIT for the works Treasure island and Strange case of Doctor Jekyll and Mister Hyde. PREFIX b f :< h t t p : / / i d . l o c . gov / o n t o l o g i e s / b i b f r a m e /> SELECT ? work ? w o r k M a i n T i t l e ? exp WHERE { ? work b f : t i t l e ? w o r k T i t l e . ? w o r k T i t l e b f : m a i n T i t l e ? w o r k M a i n T i t l e . ? work b f : e x p r e s s i o n O f ? exp . ? exp b f : t i t l e ? e x p T i t l e . ? e x p T i t l e b f : m a i n T i t l e ” S t r a n g e c a s e o f D o c t o r J e k y l l and M i s t e r Hyde . I t a l i a n ” } LIMIT 20 Listing 8. Translations of the work Strange case of Doctor Jekyll and Mister Hyde. Italian included in the dataset BOSLIT. Results are shown in Table 13. Table 13. Editions in Italian of the work Strange case of Doctor Jekyll and Mister Hyde included by the BOSLIT dataset using BIBFRAME as main vocabulary. The bf:Work is related to a bf:Hub resource describing the uniform title of the main work. Work Title Hub http://example.org/15726#Work Lo strano caso del dottor Jekyll e del dottor [sic] Hyde ; Il signore di Ballantrae http://example.org/15726#Hub240-10 http://example.org/9803#Work Lo strano caso del dottor Jekyll e del signor Hyde http://example.org/9803#Hub240-9 http://example.org/9962#Work Lo strano caso del dottor Jekyll e del signor Hyde http://example.org/9962#Hub240-8 http://example.org/16238#Work Il dottor Jekyll http://example.org/16238#Hub240-9 http://example.org/12727#Work Il dottor Jekill [sic] http://example.org/12727#Hub240-9 http://example.org/12333#Work Lo strano caso del dottor Jekill [sic] http://example.org/12333#Hub240-9 25. https://rdflib.readthedocs.io 26. https://github.com/lcnetdev/bfe/ 27. See, for example, https://www.loc.gov/bibframe/ mtbf/ 28. See, for example, https://www.iflastandards.info/ lrm/lrmer 29. See, for example, the property P268 for the Biblioth`eque nationale de France. 30. https://spacy.io/ 31. https://stanfordnlp.github.io/CoreNLP/ 32. https://virtuoso.openlinksw.com/ 33. https://jena.apache.org/documentation/tdb/ 34. https://rdf4j.org/ 35. See, for example, https://glam-workbench.net/ 36. https://github.com/hibernator11/nls- fellowship-2022-23 37. https://www.w3.org/RDF/Validator/ 38. https://www.loc.gov/marc/bibliographic/ bd130.html 39. https://www.loc.gov/marc/bibliographic/ bd240.html 40. https://id.loc.gov/vocabulary/relators.html 41. https://www.wikidata.org/wiki/Property: P4801 Prepared using sagej.cls Candela, G. 13 References 1. Padilla T. Responsible Operations: Data Science, Machine Learning, and AI in Libraries, 2019. URL https:// doi.org/10.25333/xk7z-9g97. 2. Smith-Yoshimura K. Transitioning to the Next Generation of Metadata, 2020. URL https://doi.org/10.25333/ rqgd-b343. 3. Research Libraries UK. A manifesto for the digital shift in research libraries, 2020. URL https://www.rluk.ac.uk/ digital-shift-manifesto/. 4. Padilla T, Allen L, Frost H et al. Final Report — Always Already Computational: Collections as Data, 2019. DOI:10.5281/zenodo.3152935. URL https://doi.org/ 10.5281/zenodo.3152935. 5. Hansson K, Cerratto T and Dahlgren A. Datafication and cultural heritage: provocations, threats, and design opportunities. In Proceedings of 18th European Conference on Computer-Supported Cooperative Work. European Society for Socially Embedded Technologies (EUSSET). DOI:10.18420/ ecscw2020 ws05. URL http://dx.doi.org/10.18420/ ecscw2020 ws05. 6. Mahey M, Al-Abdulla A, Ames S et al. Open a GLAM lab. Doha, Qatar: International GLAM Labs Community, Book Sprint, 2019. ISBN 978-9927-139-07-9. DOI: 10.21428/16ac48ec.f54af6ae. URL http://dx.doi.org/ 10.21428/16ac48ec.f54af6ae. 7. Berners-Lee T, Connolly D, Stein LA et al. The Semantic Web, 2000. URL https://www.w3.org/2000/Talks/ 0906-xmlweb-tbl/text.htm. 8. Berners-Lee T. Linked Data, 2006. URL https:// www.w3.org/DesignIssues/LinkedData.html. 9. Candela G, Escobar P, Carrasco RC et al. Migration of a library catalogue into RDA linked open data. Semantic Web 2018; 9(4): 481–491. DOI:10.3233/SW-170274. URL https: //doi.org/10.3233/SW-170274. 10. Frosterus M, Hansson D, Dadvar M et al. Best Practices for Library Linked Open Data (LOD) Publication, 2021. DOI:10.5281/zenodo.4572501. URL https://doi.org/ 10.5281/zenodo.4572501. 11. Vincent H, Cunnea P and Pretto AD. From Scottish Bibliographies Online to National Bibliography of Scot- land: Reinventing a National Bibliography for the 21st Cen- tury, 2018. URL https://library.ifla.org/id/ eprint/2275/1/244-vincent-en.pdf. 12. Seikel M and Steele T. Comparison of Key Entities Within Bibliographic Conceptual Models and Implementations. Definitions, Evolution, and Relationships. Library Resources & Technical Services 2020; 64(1). URL https://journals.ala.org/index.php/lrts/ article/view/7345/10100. 13. Library of Congress. Bibliographic Framework Initiative, 2016. URL https://www.loc.gov/bibframe/. 14. D’Arcus B and Giasson F. Bibliographic Ontology Specifica- tion, 2009. URL https://www.bibliontology.com/. 15. Bekiari C, Bruseker G, Canning E et al. Volume A: Definition of the CIDOC Conceptual Reference Model, 2016. URL https://www.cidoc-crm.org/sites/default/ files/cidoc crm version 7.2.2%5B20%20Oct% 5D.pdf. 16. Europeana. Europeana Data Model, 2016. URL https: //pro.europeana.eu/page/edm-documentation. 17. International Federation of Library Associations and Institutions. Functional Requirements for Bibliographic Records, 2008. URL https://cdn.ifla.org/ wp-content/uploads/2019/05/assets/ cataloguing/frbr/frbr 2008.pdf. 18. Coyle K. Works, Expressions, Manifestations, Items: An Ontology. code4lib journal 2022; 53. URL https:// journal.code4lib.org/articles/16491. 19. Pat R, Patrick LB and ˇZumer Maja. IFLA Library Refer- ence Model: A Conceptual Model for Bibliographic Infor- mation, 2018. URL https://repository.ifla.org/ handle/123456789/40. 20. Zaveri A, Rula A, Maurino A et al. Quality assessment for Linked Data: A Survey. Semantic Web 2016; 7(1): 63–93. DOI: 10.3233/SW-150175. URL https://doi.org/10.3233/ SW-150175. 21. Candela G, Escobar P, Carrasco RC et al. Evaluating the quality of linked open data in digital libraries. J Inf Sci 2022; 48(1): 21–43. DOI:10.1177/0165551520930951. URL https://doi.org/10.1177/0165551520930951. 22. F¨arber M, Bartscherer F, Menne C et al. Linked data quality of DBpedia, Freebase, OpenCyc, Wikidata, and YAGO. Semantic Web 2018; 9(1): 77–129. DOI:10.3233/SW-170275. URL https://doi.org/10.3233/SW-170275. 23. Fern´andez- ´Alvarez D, Gayo JEL and Gayo-Avello D. Automatic extraction of shapes using sheXer. Knowl Based Syst 2022; 238: 107975. DOI: 10.1016/j.knosys.2021.107975. URL https: //doi.org/10.1016/j.knosys.2021.107975. 24. Kir´aly P. Towards an extensible measurement of metadata quality. In Antonacopoulos A and B¨uchler M (eds.) Proceedings of the 2nd International Conference on Digital Access to Textual Cultural Heritage, DATeCH 2017, G¨ottingen, Germany, June 1-2, 2017. ACM, pp. 111–115. DOI:10.1145/ 3078081.3078109. URL https://doi.org/10.1145/ 3078081.3078109. 25. Hidalgo-Delgado Y, Rodr´ıguez YAL, Rodr´ıguez JPF et al. Quality assessment of library linked data: a case study. In Knowledge Graphs and Semantic Web - Third Iberoamerican Conference and Second Indo-American Conference, KGSWC 2021, Kingsville, Texas, USA, November 22-24, 2021, Proceedings. pp. 93–108. DOI:10.1007/978-3-030-91305- 2\\ 8. URL https://doi.org/10.1007/978-3-030- 91305-2 8. 26. Candela G, Escobar P, S´aez D et al. A Shape Expression approach for assessing the quality of Linked Open Data in Libraries. Semantic Web 2021; DOI:10.3233/SW-210441. URL https://doi.org/10.3233/SW-210441. 27. Langer A, Siegert V, G¨opfert C et al. SemQuire - Assessing the Data Quality of Linked Open Data Sources Based on DQV. In Current Trends in Web Engineering - ICWE 2018 International Workshops, MATWEP, EnWot, KD-WEB, WEOD, TourismKG, C´aceres, Spain, June 5, 2018, Revised Selected Papers. pp. 163–175. DOI:10.1007/978-3-030-03056-8\\ 14. URL https://doi.org/10.1007/978-3-030- 03056-8 14. 28. Nayak A, Bozic B and Longo L. (linked) data quality assessment: An ontological approach. In Proceedings of the 15th International Rule Challenge, 7th Industry Track, and Prepared using sagej.cls 14 Journal Title XX(X) 5th Doctoral Consortium @ RuleML+RR 2021 co-located with 17th Reasoning Web Summer School (RW 2021) and 13th DecisionCAMP 2021 as part of Declarative AI 2021, Leuven, Belgium (virtual due to Covid-19 pandemic), 8 - 15 September, 2021. URL http://ceur-ws.org/Vol- 2956/paper17.pdf. 29. Lee BCG. The ”Collections as ML Data” Checklist for Machine Learning & Cultural Heritage, 2022. DOI:10.48550/ ARXIV.2207.02960. URL https://arxiv.org/abs/ 2207.02960. 30. Library BS. Qurator. automated curation technologies for the digitised cultural heritage, 2022. URL https:// ravius.sbb.berlin/. 31. Lorang E, Soh LK, Liu Y et al. Digital Libraries, Intelligent Data Analytics, and Augmented Description: A Demonstration Project, 2020. URL https:// digitalcommons.unl.edu/libraryscience/396/. 32. Hyv¨onen E. Using the Semantic Web in digital humanities: Shift from data publishing to data-analysis and serendipitous knowledge discovery. Semantic Web 2020; 11(1): 187– 193. DOI:10.3233/SW-190386. URL https://doi.org/ 10.3233/SW-190386. 33. Godby J, Smith-Yoshimura K, Washburn B et al. Creating Library Linked Data with Wikibase: Lessons Learned from Project Passage, 2019. URL https://doi.org/ 10.25333/faq3-ax08. 34. Candela G, S´aez MD, Escobar P et al. A benchmark of Spanish language datasets for computationally driven research. Journal of Information Science 2021; DOI:10.1177/ 01655515211060530. URL https://doi.org/10.1177/ 01655515211060530. https://doi.org/10.1177/ 01655515211060530. 35. Neudecker C. Cultural Heritage as Data: Digital Curation and Artificial Intelligence in Libraries. In Paschke A, Rehm G, Neudecker C et al. (eds.) Proceedings of the Third Conference on Digital Curation Technologies (Qurator 2022), Berlin, Germany, Sept. 19th-23rd, 2022, CEUR Workshop Proceedings, volume 3234. CEUR-WS.org. URL http:// ceur-ws.org/Vol-3234/paper2.pdf. 36. van Strien D, Beelen K, Ardanuy MC et al. Assessing the impact of OCR quality on downstream NLP tasks. In Rocha AP, Steels L and van den Herik HJ (eds.) Proceedings of the 12th International Conference on Agents and Artificial Intelligence, ICAART 2020, Volume 1, Valletta, Malta, February 22-24, 2020. SCITEPRESS, pp. 484–496. DOI: 10.5220/0009169004840496. URL https://doi.org/ 10.5220/0009169004840496. 37. Mariette J, Blanchard O, Bern´e O et al. An open-source tool to assess the carbon footprint of research. Environmental Research: Infrastructure and Sustainability 2022; 2(3): 035008. DOI:10.1088/2634-4505/ac84a4. URL https: //dx.doi.org/10.1088/2634-4505/ac84a4. 38. National Library of Scotland. Exploring The National Bibliography of Scotland, 2020. URL https://doi.org/ 10.34812/an7d-xk61. 39. Huang JY, Lange C and Auer S. Streaming transformation of xml to rdf using xpath-based mappings. In Proceedings of the 11th International Conference on Semantic Systems. SEMANTICS ’15, New York, NY, USA: Association for Computing Machinery. ISBN 9781450334624, p. 129–136. DOI:10.1145/2814864.2814880. URL https://doi.org/ 10.1145/2814864.2814880. 40. World Wide Web Consortium. Best Practices for Publishing Linked Data, 2014. URL https://www.w3.org/TR/ld- bp/. 41. Musen MA. The prot´eg´e project: a look back and a look forward. AI Matters 2015; 1(4): 4–12. DOI:10.1145/ 2757001.2757003. URL https://doi.org/10.1145/ 2757001.2757003. 42. Bergwinkl T, Luggen M, elf Pavlik et al. RDF/JS: Data model specification, 2022. URL https://rdf.js.org/data- model-spec/. 43. Library of Congress. marc2bibframe2, 2022. URL https: //github.com/lcnetdev/marc2bibframe2. 44. Vila-Suero D and G´omez-P´erez A. datos.bne.es and marimba: an insight into library linked data. Libr Hi Tech 2013; 31(4): 575–601. DOI:10.1108/LHT-03-2013-0031. URL https: //doi.org/10.1108/LHT-03-2013-0031. 45. RDA Registry. MARC2RDA, 2022. URL https:// github.com/uwlib-cams/MARC2RDA. 46. Charles V, Isaac A, Tzouvaras V et al. Mapping cross- domain metadata to the Europeana Data Model (EDM). In Aalberg T, Papatheodorou C, Dobreva M et al. (eds.) Research and Advanced Technology for Digital Libraries - International Conference on Theory and Practice of Digital Libraries, TPDL 2013, Valletta, Malta, September 22-26, 2013. Proceedings, Lecture Notes in Computer Science, volume 8092. Springer, pp. 484–485. DOI:10.1007/978-3-642-40501- 3\\ 68. URL https://doi.org/10.1007/978-3-642- 40501-3 68. 47. Aalberg T, Taller˚as K and Massey D. The impact of new bibliographic models on the search experience. Inf Res 2019; 24(Supplement). URL http://www.informationr.net/ir/24-4/colis/ colis1915.html. 48. Labusch K and Neudecker C. Entity linking in multilingual newspapers and classical commentaries with BERT. In Faggioli G, Ferro N, Hanbury A et al. (eds.) Proceedings of the Working Notes of CLEF 2022 - Conference and Labs of the Evaluation Forum, Bologna, Italy, September 5th - to - 8th, 2022, CEUR Workshop Proceedings, volume 3180. CEUR- WS.org, pp. 1079–1089. URL http://ceur-ws.org/ Vol-3180/paper-85.pdf. 49. Mendes PN, Jakob M, Garc´ıa-Silva A et al. DBpedia spotlight: shedding light on the web of documents. In Ghidini C, Ngomo AN, Lindstaedt SN et al. (eds.) Proceedings the 7th International Conference on Semantic Systems, I-SEMANTICS 2011, Graz, Austria, September 7-9, 2011. ACM International Conference Proceeding Series, ACM, pp. 1–8. DOI:10.1145/ 2063518.2063519. URL https://doi.org/10.1145/ 2063518.2063519. 50. World Wide Web Consortium. Describing Linked Datasets with the VoID Vocabulary, 2011. URL https:// www.w3.org/TR/void/. 51. National Library of Scotland. Moving Image Archive. URL https://data.nls.uk/data/metadata- collections/moving-image-archive/. 52. National Library of Scotland. National Bibliography of Scotland, 2019. URL https://doi.org/10.34812/ 7cda-ep21. 53. National Library of Scotland. Bibliography of Scottish Liter- ature in Translation, 2019. URL https://data.nls.uk/ Prepared using sagej.cls Candela, G. 15 data/metadata-collections/boslit/. 54. Ames S. Special collections as data: the National Library of Scotland’s Data Foundry. URL https: //cerlblog.wordpress.com/2020/08/13/ special-collections-as-data-the-national- library-of-scotlands-data-foundry/. 55. Ames S. Transparency, provenance and collections as data: The National Library of Scotland’s Data Foundry. LIBER Quarterly: The Journal of the Association of European Research Libraries 2021; 31(1): 1–13. DOI:10.18352/lq.10371. URL https: //liberquarterly.eu/article/view/10880. 56. Ames S. Digital Scholarship and the Data Foundry, 2020. DOI:10.5281/zenodo.3862050. URL https://doi.org/ 10.5281/zenodo.3862050. 57. Ames S and Lewis S. Disrupting the library: Digital scholarship and Big Data at the National Library of Scotland. Big Data & Society 2020; 7(2): 2053951720970576. DOI: 10.1177/2053951720970576. URL https://doi.org/ 10.1177/2053951720970576. https://doi.org/ 10.1177/2053951720970576. 58. Freire N, Robson G, Howard JB et al. Cultural heritage metadata aggregation using web technologies: Iiif, sitemaps and schema.org. Int J Digit Libr 2020; 21(1): 19–30. DOI: 10.1007/s00799-018-0259-5. URL https://doi.org/ 10.1007/s00799-018-0259-5. 59. Library of Congress. MARC List for Languages, 2012. URL https://id.loc.gov/vocabulary/ languages.html. 60. Library of Congress. MARC List for Geographic Areas, 2018. URL https://id.loc.gov/vocabulary/ geographicAreas.html. Prepared using sagej.cls","libVersion":"0.3.2","langs":""}