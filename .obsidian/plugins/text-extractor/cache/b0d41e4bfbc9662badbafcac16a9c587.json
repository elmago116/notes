{"path":"Clippings/PDF/SLADE-A-Method-for-Designing-HumanCentred-Learning-Analytics-Systems_2024_Association-for-Computing-Machinery.pdf","text":"SLADE: A Method for Designing Human-Centred Learning Analytics Systems Riordan Alfredo riordan.alfredo@monash.edu Monash University Australia Vanessa Echeverria vanessa.echeverria@monash.edu Monash University Australia Escuela Superior PolitÃ©cnica del Litoral Guayaquil, Ecuador Yueqiao Jin ariel.jin@monash.edu Monash University Australia Zachari Swiecki zach.swiecki@monash.edu Monash University Australia Dragan GaÅ¡eviÄ‡ dragan.gasevic@monash.edu Monash University Australia Roberto Martinez-Maldonado roberto.martinezmaldonado@monash.edu Monash University Australia ABSTRACT There is a growing interest in creating Learning Analytics (LA) systems that incorporate student perspectives. Yet, many LA sys- tems still lean towards a technology-centric approach, potentially overlooking human values and the necessity of human oversight in automation. Although some recent LA studies have adopted a human-centred design stance, there is still limited research on es- tablishing safe, reliable, and trustworthy systems during the early stages of LA design. Drawing from a newly proposed framework for human-centred artificial intelligence, we introduce SLADE, a method for ideating and identifying features of human-centred LA systems that balance human control and computer automation. We illustrate SLADEâ€™s application in designing LA systems to support collaborative learning in healthcare. Twenty-one third-year stu- dents participated in design sessions through SLADEâ€™s four steps: i) identifying challenges and corresponding LA systems; ii) prioritis- ing these LA systems; iii) ideating human control and automation features; and iv) refining features emphasising safety, reliability, and trustworthiness. Our results demonstrate SLADEâ€™s potential to assist researchers and designers in: 1) aligning authentic student challenges with LA systems through both divergent ideation and convergent prioritisation; 2) understanding studentsâ€™ perspectives on personal agency and delegation to teachers; and 3) fostering discussions about the safety, reliability, and trustworthiness of LA solutions. CCS CONCEPTS â€¢ Human-centered computing â†’ HCI design and evaluation methods. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. LAK â€™24, March 18â€“22, 2024, Kyoto, Japan Â© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-1618-8/24/03. . . $15.00 https://doi.org/10.1145/3636555.3636847 KEYWORDS Human-centered learning analytics; Human-centered AI; Double Diamond; Design Thinking ACM Reference Format: Riordan Alfredo, Vanessa Echeverria, Yueqiao Jin, Zachari Swiecki, Dragan GaÅ¡eviÄ‡, and Roberto Martinez-Maldonado. 2024. SLADE: A Method for De- signing Human-Centred Learning Analytics Systems. In The 14th Learning Analytics and Knowledge Conference (LAK â€™24), March 18â€“22, 2024, Kyoto, Japan. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3636555. 3636847 1 INTRODUCTION There is a growing interest in designing Learning Analytics (LA) sys- tems by incorporating the perspectives of educational stakeholders to various extents [10, 37, 52]. For example, students and teachers have contributed to the design of LA dashboards [1, 5, 17, 22], classroom orchestration [30, 59] and auto-grading systems[20]. Yet, many LA systems are still designed following approaches that overly emphasise technical aspects related to data analysis, often downplaying contextual needs, human values, and the agency re- quirements of students and teachers [58]. While some LA research has started adopting human-centred approaches [52], they have still overlooked specific requirements for human oversight over automation [43]. Beyond LA, there is strong interest in creating what is being termed human-centred AI (HCAI) systems [26, 55]. These systems are founded on three design aims: i) balancing high human control with high automation based on contextual needs; ii) empowering humans rather than emulating them; and iii) developing safe, reli- able, and trustworthy systems. These aims are crucial, as designing AI-powered systems that resonate with human values remains a challenge [36]. Notably, these design aims align with the sustain- ability goals of AI-powered educational technology [7] and mirror the LAKâ€™24 conference theme: Learning Analytics in the Age of Artificial Intelligence. Some recent work in LA has started to adopt foundations of HCAI conceptually, for example, to envision the fu- ture of LA and education technology research [60] and to frame the presentation of study results [61], and to organise current LA litera- ture [51]. Moreover, the term Human-Centered Learning Analytics 24Downloaded from the ACM Digital Library on April 8, 2025. LAK â€™24, March 18â€“22, 2024, Kyoto, Japan Alfredo et al. (HCLA) has also been growingly used to emphasise the LA commu- nityâ€™s interest in human factors and stakeholder participation [39]. Yet, little HCLA research has provided practical methods based on HCAI foundations for researchers and designers to create features in the LA system that are safe, reliable, and trustworthy while con- sidering studentsâ€™ agency and their voices in light of increasing AI automation [51]. Based on foundations of HCAI [55], we present SLADE (Safe, reliable, and trustworthy Learning Analytics Design Elicitation), a design method aimed at enabling the ideation and identification of the features of a safe, reliable, and trustworthy LA system that balances both human control and computer automation in collabora- tion with educational stakeholders. Inspired by the double-diamond design thinking approach, SLADE comprises four steps: (1) Dis- cover: ideating the LA systems, (2) Define: prioritising these systems based on their elaboration and significance in addressing learning challenges, (3) Develop: pinpointing the human control and automa- tion features of the chosen system, and (4) Deliver: steering the conversation towards a safe, reliable, and trustworthy LA system from these features. We empirically illustrate this method, working towards designing learning analytics systems with third-year un- dergraduate students (ğ‘› = 21) to support collaborative learning in healthcare education. 2 BACKGROUND 2.1 Human-centred Learning Analytics Human-Centred Learning Analytics (HCLA) is an emerging sub- field within LA, at the intersection of AI, learning analytics, and human-centred design. Its aim is to counter the limitations of bottom-up, data-driven analytics approaches that might overlook the contextual realities of the socio-technical LA system by pri- oritising studentsâ€™ and teachersâ€™ intentions, preferences, interests, values, and activities [10]. To realise this, HCLA researchers have relied on various human-centred design methods, such as design thinking [50, 52] and value-sensitive design [11, 58]. Yet, several challenges confront HCLA, particularly concerning the development of LA systems in collaboration with stakeholders [39]. Firstly, involving stakeholders is not always straightforward, and more empirical research is necessary to validate its effectiveness in LA creation [37]. Secondly, there is a dearth of methods available for LA researchers and designers to nurture design capacity among stakeholders, thereby promoting a culture of democratic decision- making and scaffolding the conversations around critical aspects of LA that can enable its co-creation [16, 53]. Thirdly, in some cases, the evaluation of ideas predominantly occurs post-study, bypassing the crucial involvement of teachers and students during the early ideation and development phases [9]. Furthermore, to our knowledge, there is scant guidance avail- able for LA researchers and designers about effectively integrating stakeholdersâ€™ voices and perspectives when defining features that balance human control with computer automation. Maintaining this balance is pivotal to ensure that user agency and control stay at the forefront of advanced LA system designs, as emphasised by various education-focused studies on human-AI collaboration [e.g., 29, 43, 59]. While some research has started to explore the potential integration of HCAI principles in the context of LA design [e.g., 60, 61], there is a lack of a comprehensive methodology that seam- lessly aligns HCAI principles with the primary objectives of HCLA, especially concerning the design of safe, reliable, and trustworthy LA systems [39]. 2.2 Foundations of Human-centred AI A human-centred AI (HCAI) approach highlights the complemen- tarity of human and machine intelligence in the design of effective AI systems [55]. By incorporating user feedback in the design pro- cess, the users can have a sense of ownership and control over AI systems, potentially fostering trust and acceptance [57]. Shneider- man [55], an HCAI pioneer, proposed a two-dimensional framework to distinguish between the various extents of human control and computer automation rather than treating human oversight and automation as mutually exclusive. As such, a single system might present a spectrum of features, each combining varying degrees tailored to diverse tasks. The concepts of human control and agency are essential when designing interfaces that promote an internal locus of control [44] (this is, the perception that individuals control their outcomes). In educational contexts, human control can vary: high, as seen when learners actively control the LA systems, or low, when they merely receive information without exerting any autonomy [6, 32]. Com- puter automation involves using technology or algorithms to optimise tasks previously done only by humans. This often encom- passes analytics from various methodologies [12], ranging from a high degree of automation employing predictive models powered by AI and machine learning [e.g. 15], to a lower degree that utilises rule- based strategies without substantial autonomy or adaptability be- yond their programmed constraints [e.g., 46]. The two-dimensional HCAI framework [55] organises these degrees of human control and automation into four distinct quadrants: Q1: LOW human control & LOW computer automation. This quadrant represents system features that offer limited user control and minimal automation. After receiving infor- mation from the system, users can only comprehend it but cannot personalise or modify it. An example in this quad- rant includes rule-based visual data stories, which provide feedback to students about their collaboration activities post- learning activity [e.g., 19]. Q2: HIGH human control & LOW computer automation. This quadrant represents features that enable users to per- sonalise and configure aspects of the information process, retaining agency within the learning environment. Examples would include personalised visualisation dashboards [e.g., 46], enabling learners to tailor descriptive analytics to review the attainment of learning objectives, and educator-driven data analytics systems [e.g., 22, 34], using educatorsâ€™ expertise and observations to guide instruction and feedback decisions. Q3: LOW human control & HIGH computer automation. This quadrant represents system features that heavily rely on automation to handle decision-making and/or action-taking processes with minimal user control. Examples encompass systems that record interaction logs from students or teach- ers and use them in predictive analytics features to deliver automated feedback [e.g., 17], as well as automated grading 25Downloaded from the ACM Digital Library on April 8, 2025. A Method for Designing Human-Centred LA Systems LAK â€™24, March 18â€“22, 2024, Kyoto, Japan [e.g., 20, 45]. Such systems might either fully or partially supplant the teacherâ€™s role in certain assessment tasks. Q4: HIGH human control & HIGH computer automation. This quadrant represents system features that enable man- ual operation while benefiting from automated assistance to enhance the decision-making process in the learning envi- ronment. Examples in this quadrant would include features in intelligent teaching assistants that support classroom or- chestration [e.g., 59], as well as sophisticated modelling or prediction in analytics reports or dashboards offering guid- ance to teachers or students [e.g., 1, 14]. Within this context, teachers and students can interpret the systemâ€™s information to make informed pedagogical decisions regarding subse- quent actions [30]. 2.3 Safe, Reliable, and Trustworthy LA. According to Schneiderman [55], components inside an AI system that provide a high degree of computer automation and human control (Q4) should consider the critical human-AI principles of safety, reliability and trustworthiness to maximise safe practices and minimise data misuse. Similarly, within LA, Holmes et al. [28] suggested that the design of LA systems should encompass a safety culture, meaning that researchers, designers and developers are encouraged to build strong connections with students and teach- ers, ensuring strong data privacy practices. When designing an LA system, it is important to involve stakeholders in deciding how data will be shared, being transparent about data collection, and ensuring data access for specific stakeholders [18, 56]. A reliable AI-powered analytics system produces the expected results (i.e., ac- curate information) when the user interacts with it [55]. Although no predictive model is perfect [4], LA system designers may want to embrace imperfection by investigating how students will act upon this imperfection (e.g., inaccurate or incomplete data) and whether this will hinder their learning [35]. Finally, an LA system is considered trustworthy when users can have confidence in the system beyond mere perceptions of trust. Aiming to foster greater trust and confidence among users, the system could actively seek feedback from users, learn from its mistakes, and adapt to improve its performance while aligning with user expectations [57]. Design- ers should prioritise transparency and accountability, aiming to understand system features that increase user trust [2, 47, 48, 56] 2.4 Research Questions and Contributions to LA Against the gaps in HCLA discussed in Section 2.1, we propose and illustrate the application of SLADE, a method that enables the ideation and identification of the features of a human-centred LA system that balances human control and computer automation and discussions with students around safety, reliability, and trustworthi- ness. To this end, we formulated the following research questions: â€¢ RQ1: How can we, in collaboration with students, elicit and prioritise LA systems that address their learning challenges? â€¢ RQ2: How can we engage students in developing LA features that strike a balance between human control and computer automation? â€¢ RQ3: How can we elicit studentsâ€™ perceptions on safety, reliability, and trustworthiness of an LA system? 3 SLADE: THE DESIGN ELICITATION METHODOLOGY Inspired by the Double Diamond model of human-centred de- sign, which suggests an iterative divergent-convergent structure [13][e.g., 34], SLADE seamlessly incorporates Shneidermanâ€™s HCAI framework tailored for human-centred, AI-powered, analytics solu- tions as elaborated in Section 2.2. These foundational methodologies complement each other: while human-centred design focuses on user needs and values, HCAI emphasises safety, reliability, and trust- worthiness in analytics systems, maintaining a balance between human control and automation. SLADEâ€™s contribution to human- centred LA lies in striking a balance between addressing user needs and leveraging data-intensive insights. It is mainly aimed at as- sisting LA researchers and designers in shaping human-AI system features within a learning environment, prioritising safety, reliabil- ity, and trustworthiness based on stakeholder perspectives. SLADE is structured into the following four steps, each associated with a distinct set of sample questions: 3.1 Step 1: Ideas generation. (Discover) This is a generic seed (divergent thinking) design step that is com- mon among other LA design approaches (e.g., [3, 30, 52]), and it is aimed at helping participants generate diverse, and potentially divergent, ideas of LA systems that may improve the learning experi- ence. Researchers use idea-generation methods, such as speculative fabulation [25] or participatory speed dating [30], which are often used in the first stage of design (see examples in [24]). This step could be initiated by asking participants: \"i) What are your current learning challenges in [learning context], and when do they com- monly emerge (for example, the learning phases: before, during, or after class [33])?\". Then, within this step, we also want participants to ideate LA systems that could potentially solve those challenges. For instance, researchers might employ speculative fabulation dur- ing system ideation with questions like \"ii) What supertools (LA systems) can you imagine to enhance your learning experience?\" or inquire about data needs with \"iii) What data/information is required by [the LA system] to address those challenges effectively?\". Therefore, the outcome of this step includes the generation of nu- merous (divergent) ideas (i.e., both learning/teaching challenges and LA systems) that will be further discussed in subsequent steps. 3.2 Step 2: Systems prioritisation. (Define) Given that many ideas could be generated in the previous step, it is desirable to narrow them down to the ideas that need prioriti- sation (i.e., convergent thinking). Participants could be asked to tag essential ideas based on their values and goals. For instance, they could answer, \"How important is this [idea] to you and why?\" and use the provided prioritisation tags: high, medium, or low, to clarify their decisions. The purpose of the LA system could be fur- ther emphasised by asking participants to make explicit connections between suggested LA systems and their learning challenges and data needs. LA researchers and designers could ask a question such as: \"How [system] can address your learning challenges?\" and utilise a technique such as concept mapping to make these connections visually explicit. Thus, once a list of ideas has been generated and learning challenges, data needs and envisaged LA system have been 26Downloaded from the ACM Digital Library on April 8, 2025. LAK â€™24, March 18â€“22, 2024, Kyoto, Japan Alfredo et al. Figure 1: SLADE design method, inspired by double-diamond design thinking [13]. mapped, participants can be invited to prioritise one crucial LA system idea for further discussion in the following steps. To aid in this selection, participants might be prompted to choose a system they can vividly envision and deem essential for their learning. 3.3 Step 3: Human control and automation features generation. (Develop) This step is based on Shneidermanâ€™s HCAI framework [55] and aims at identifying diverse features of the envisaged LA system and their degrees of human control and computer automation based on selected systems in the previous step. A set of questions can be formulated using the HCAI quadrants presented in Section 2.2. First, participants can be asked to envisage some functionalities, called \"features\", that [system] should have (e.g., \"What abilities should your [system] have? Or, what can this [system] do to sup- port learning?\") and the target user (i.e., teachers, students or other stakeholders). This can help to conceptualise the feature with par- ticipantsâ€™ ideas. Next, participants can be prompted to think about the degrees of computer automation of the proposed tool (e.g., Should your [system] be fully automated or should it have little to no automation? How?). Participants can then be asked about the extent of manual control required (e.g., How much control should a user have when interacting with the system?). Finally, participants can be asked about the desired frequency of human intervention to corroborate their responses regarding human control (e.g. How often does the tool require human control or intervention? Frequent or rare?)[43]. The outcome of this step is a set of specific features out of the selected LA system and the participantâ€™s desired level of control and automation. This information provides technical context for a more focused discussion in the next step. 3.4 Step 4: Safety, reliability, and trustworthiness discussion (Deliver) After collecting the extracted features inside an LA system, this step aims to converge the conversation by considering the ethical values of the ideated and prioritised LA system. Hence, the discussion towards a safe, reliable, and trustworthy LA is conducted, as defined in Section 2.3. Questions can be asked to participants to trigger discussion about safety (e.g., privacy â€“ \"Do you have any privacy concerns when [target users] using [system]? If so, what are they?\" â€“ and data sharing â€“ \"Who should see the information generated by [system]? Why or why not?\"), reliability (e.g. \"How accurate does the information generated from [system] need to be? Why or why not?\" ) and trust (e.g. \"Would you trust the information provided by [system]? Why or why not?\") [40, 49]. The outcome of this step is for students to evaluate the safety, reliability, and trustworthiness of features on their chosen LA system. This can, in turn, enable LA researchers or designers to establish design guidelines based on their perceptions. 4 METHODS 4.1 Participants and Procedure To illustrate the application of SLADE, a study was carried out with 21 third-year Bachelor of Nursing students (F: 17, M: 4, avg. age: 21; P1-P21) at a local university. All the students underwent a simulation-based learning scenario as part of their regular cur- riculum. This collaborative learning experience typically unfolds in a physical space equipped with medical apparatus and manikins that simulate real patients. Students worked in teams to tackle chal- lenges putting into practice their communication and clinical skills. After a learning task, their teacher led a reflective debrief. They were later invited to participate in design sessions for formulating 27Downloaded from the ACM Digital Library on April 8, 2025. A Method for Designing Human-Centred LA Systems LAK â€™24, March 18â€“22, 2024, Kyoto, Japan LA systems that could aid future collaborative simulations. Stu- dents were compensated with $35-dollar vouchers for their time dedicated to the design sessions. The design sessions were structured around the steps and in- cluded all the sample questions suggested in Section 3. Design ses- sions were led by a researcher and recorded using an online video conference platform (i.e., Zoom) and lasted around 60 Â± 15 minutes (avg. time: 62 minutes). An online collaborative board (i.e., Miro board) was employed to scaffold the session according to the four steps (Steps 1-4) so that students could write down their ideas us- ing digital sticky notes and elaborate more on these when needed. Sessions were transcribed verbatim, and all notes on the online collaborative board were gathered for analysis. In the beginning, each student was invited to conceptualise sev- eral ideas (i.e., challenges, LA systems, and data) of an LA system and their relevance to specific learning phase (for example: before, during and after class, although we let students use their own words to refer to these learning phases) based on their recent lived experi- ences (Step 1, see Section 3.1), employing a brainstorm graphic organisers technique [24]. Step 2 aims to connect and prioritise ideas for the discussion in the next step. For this, we employed the generate-sort-connect-elaborate technique [27]. Since ideas generation was done in Step 1, students continued by sorting all ideas by using the prioritisation tags (i.e., low, medium, or high priority) based on personal values and needs. Then, students were requested to generate a network by explicitly connecting learning challenges, LA systems, and data. Students were asked to elaborate further on any new ideas if they emerged through those connections. Finally, each participant was asked to pick only one LA system to focus on in the next step. After a student picked one main system from the previous step, sample questions in Steps 3 and 4 (see Sections 3.2 and 3.3, respectively) were queried in an interview format [24]. 4.2 Analysis To answer RQ1, we conducted thematic analysis [8] and affinity diagramming [24] to analyse qualitative data. We gathered all ideas (i.e., challenges, LA systems, and data/information) in the form of notes that contained a short description and learning phases where these commonly happen from the collaborative board (Step 1 and Step 2). One researcher used the affinity diagramming technique to organise and categorise these ideas, which another researcher later reviewed and discussed to ensure agreement [41]. In Step 2, we summarised the prioritisation of systems selected by students based on the learning phases in the learning context, see Section 4.1. We created a metric to prioritise ideated systems, inspired by gener- ative HCD methods assessment metrics [54]. The metric assigns a numerical value (from 0 to 100) to each envisaged LA system, based on how well it addresses the challenges highlighted by students (Eq. 2). The metric is zero if the system is not connected to any challenges. From the collaborative board, we gathered all systemsâ€™ importance tags (k, see Eq.1 and Section 3.2), and their number of connections to the challenges (N ) along with each challengesâ€™ importance tags (k). Min-max normalisation enabled comparison and analysis against other systems (Eq. 3). Expressed mathemat- ically, let ğ‘¥ = a generated system idea, ğ‘ = number of connected challenges, and also ğ‘˜ = importance tag of an idea. With ğ‘˜, the de- gree of importance of an idea can be quantified (high=3, medium=2, low=1, and unsorted=0) in Eq. 1 as weight (ğ‘¤). ğ‘¤ğ‘¥ is the current systemâ€™s importance weight, and ğ‘ğ‘¥ is the number of the systemâ€™s connected challenges. Hence: ğ‘¤ğ‘ = ï£±ï£´ï£´ï£² ï£´ï£´ ï£³ Ãğ‘ğ‘¥ ğ‘— =1 ğ‘¤ğ‘¥ ğ‘— ğ‘ğ‘¥ if ğ‘ğ‘¥ â‰  0 0 if ğ‘ğ‘¥ = 0 (1) ğ‘¥ â€² = ğ‘¤ğ‘¥ Â· ğ‘ğ‘¥ + ğ‘¤ğ‘ (2) ğ‘¥ğ‘šğ‘’ğ‘¡ğ‘Ÿğ‘–ğ‘ = ğ‘¥ â€² âˆ’ ğ‘¥ğ‘šğ‘–ğ‘› ğ‘¥ğ‘šğ‘ğ‘¥ âˆ’ ğ‘¥ğ‘šğ‘–ğ‘› Â· 100 (3) To address RQ2, we extracted quotes describing the proposed and selected system features from studentsâ€™ responses (Step 3). To provide a systemâ€™s formal names, descriptions were analysed through a hybrid inductive-deductive thematic analysis [21]. In the inductive analysis, two researchers jointly provided a name for the proposed system by analysing each description. In the deductive analysis, we examined the extant LA literature for similar systems and descriptions, using established systems to refine the featuresâ€™ descriptions of the proposed systems. To identify the degrees of human control and computer automation, two researchers coded the envisaged systemsâ€™ features (Table 3) by assigning them to one of the quadrants in Section 3.2. After conducting several interpretation sessions to discuss conflicts [24], researchers agreed and assigned a final quadrant to each feature. To analyse elicitation on studentsâ€™ perceptions towards safety, reliability, and trustworthi- ness of their systems for an LA system (RQ3), we analysed studentsâ€™ responses to questions in Step 4 (see Section 3.4) following a de- ductive thematic analysis [8]. Two researchers jointly analysed and coded each quote with one of these three HCAI themes (safety, reli- ability, and trustworthiness). Then, quotes that could be articulated as design requirements were grouped and reported accordingly. 5 RESULTS 5.1 Ideas elicitation and systems prioritisation (RQ1) From Step 1, we identified 148 learning challenges and 81 system ideas and categorised them with emerging categories through affin- ity diagramming, summarised in Tables 1 and 2, respectively. Task prioritisation and management were perceived as the most common challenges by students (ğ‘› = 45), while most ideated sys- tems were either data visualisations (n=17) or clinical-related sys- tems (n=16). In Figure 2, we presented the summary of participantsâ€™ prioritisation of generated ideas (Step 2). Three learning phases where the envisaged LA systems could be used emerged from the analysis, namely during clinical simulation, during debriefing, and after class. System metrics (from Eq. 1â€“3) were mapped by system category and students for each learning phase. Most ideated systems by students were positively skewed (M=37, Median=31, Mode=28, SD=19), with the average metric being in the medium importance category. Regarding systems prioritisation, only three participants (P2, P3, and P4) did not pick the system with the highest metric while other students (n=17) picked the tool with the highest metric among their ideated tools, as shown in Figure 2. This indicates that most students had already selected systems to address their personal learning challenges, facilitating meaningful discussion in subsequent steps (Steps 3 and 4). 28Downloaded from the ACM Digital Library on April 8, 2025. LAK â€™24, March 18â€“22, 2024, Kyoto, Japan Alfredo et al. Table 1: Distribution of ideated \"Challenges\" Category Coverage (%) Examples Tasks prioritisation 45 (30.20%) \"Difficulty in tasks delegation & prioritisation.\" (P16â€“P21); \"Working with new people, unsure of their knowledge and skills\". (P2â€“P6) Clinical knowledge 39 (26.17%) \"Medication management and how to use\"(P4, P10), \"Using Electronic Medical Record\"(P13), \"The best timing to do an action (e.g., assessing cardiac)\" (P9) Learning expectation 38 (25.50%) \"Unclear teachersâ€™ expectations or rules in simulation.\" (P1, P2, P10, P19); \"Lack of objectives and guidance in simulation.\" (P2, P13); \"Simulation is different from the real work environ- ment\" (P16-P18). Personal feeling 37 (24.83%) \"Feeling pressured, nervous, and afraid to be judged for being observed by other students.\"(P3â€“ P7, P14, P18), \"Tend to forget the recommended actions or best practices when stressed.\" (P15, P18, P20), Communication 29 (19.46%) \"Lack of effective and open communication between team members\" (P8, P11â€“P14, P17â€“P19); \"Unaddressed questions or misunderstanding during the scenario\" (P4) Reflection 24 (16.11%) \"In reflection, it was hard to recall my actions without any aid or systems\" (P5, P7, P9, P14, P16); \"Reflecting together with strangers may limit honest feedback\" (P5, P11, P19) Awareness 9 (6.04%) \"Lack of awareness regarding personal actions or the actions of others, whether already done or not\"(P2, P3, P10), \"Others cannot see my perspective and what I am doing now.\" (P2, P7); Table 2: Distribution of ideated \"systems\". Category Coverage (%) Examples Data visualisations 17 (20.99%) \"Visualisation of speakers and their locations\" (P13, P18, P19), \"Visualisation that shows stress/nervous data\" (P2, P10), \"An interface to access all visualisations and data for a reflection\" (P11) Clinical-related systems 16 (19.75%) \"Smart electronic medical record and nursing information database\" (P1, P6); \"Automated respiratory counting system.\" (P18); \"Smart manikin that allows full clinical assessment and detects mistakes\" (P15) Instructional aid systems 14 (17.28%) \"Prompts of instructions, following best practices.\" (P7, P15); \"AI tasks prioritisation system\" (P6, P8); \"Automated questions generator for reflection\"(P3) Video/Audio systems 10 (12.35%) \"Wearable body camera recording\" (P2, P5); \"Video recording of the simulation with prompt questions\" (P10, P12) Smartglasses (VR/AR) 7 (8.64%) \"VR environment to repeat simulation\"(P3, P8, P13); \"Additional vision to see team membersâ€™ tasks progress\" (P16); Other systems 19 (23.46%) \"Studentâ€™s survey after group discussion\" (P11, P19); \"Reflection system with teaching staff\"(P6); \"Feedback and demonstration system to improve communication skills\" (P17); \"Mind reading systems\" (P1,P7) Most ideated systems were in the first learning phase, during clinical simulation (n=46, M=38, SD=21). A total of 14 participants chose the systems from this learning phase for the subsequent de- sign step. In the second learning phase, during debriefing (n=15, M=30, SD=14), eight participants ideated data visualisation systems despite the majority (7 out of 8) perceiving their importance as low. Lastly, the average metrics for the last learning phase, after class (n=20, M=40, SD=16), was higher than the other two. Based on steps 1 and 2, we discovered that although students were en- couraged to generate ideas and establish connections with \"data needs\", the resulting connections were superficial and inadequate for meaningful analysis. This finding is further discussed later in the paper. In summary, combining divergent and convergent steps could systematically elicit and prioritise LA systems ideated by students. Decision-making can be supported by system metrics that measure ideated systemsâ€™ importance and potential challenges, en- suring that prioritised LA systems are innovative, student-centred, and can be mapped with student learning challenges. 5.2 Human control and computer automation (RQ2) We analysed and summarised ideated features from each proposed system by participants in the previous step (Step 2) â€“ systems T1- T21, except T8 due to an incomplete session. Figure 3 illustrates the final categorisation of systemsâ€™ features and their corresponding degrees of control and computer automation (HCAI quadrants, Q1- 4, ğ‘ = 35 features). Two primary target users were identified when participants defined features: students (green hexagon â€“ e.g., see T2 in Q3) and teachers (yellow hexagon â€“ e.g., see T18 in Q4). Features envisaged to offer control features for both roles (teachers and students) are represented as a green hexagon with a yellow stripe at the bottom (e.g., see T11 in Q2). Most participants envisaged highly automated systems that offer high human control (ğ‘› = 27 in Q2 and Q4 out of ğ‘›ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ = 35 features). Four participants (P1, P3, P9, and P15) did not describe any features for teachers to control. 29Downloaded from the ACM Digital Library on April 8, 2025. A Method for Designing Human-Centred LA Systems LAK â€™24, March 18â€“22, 2024, Kyoto, Japan Figure 2: A mapping of the systemsâ€™ importance metrics, categorised by participant, system categories, and across three learning phases. The findings indicate that most students could pick systems that cover most of the learning challenges among their ideated systems (n=17), meaning prioritised systems are important and could address important learning challenges students identified. Regarding Q1 (bottom-left) (n=1), only P14 described a feature with both low human control and low automation (i.e., a video camera recorder; see Figure 3 Q1 and Table 3, row 3). This offers low computer automation because it only records the activities of the students. P14 explained that students would have limited interaction with the system, as follows: \"Students only need to work on the simulation, and it is recorded, so they have no control over [the system]â€. Q2 (top-left) (n=13) includes features that enable control by teachers and students. The primary characteristic of student fea- tures was to empower studentsâ€™ freedom in operating the system without computer automation, for example, in the form of a video replay and a VR video, as described by P7 and P13. In contrast, suggested teacher features included those enabling teachers to take control of the system and provide meaningful explanations or feedback to students. For example, P14 explained the expected interaction with a video footage player as follows: \"The teacher is the one who would like to have control of things such as start-stop-pause and showing [a specific good/bad action] to us.\" P13 also emphasised teachersâ€™ agency as designers: \"Teachers probably need full control because they need to be able to align the visual simulation with the actual simulationâ€. Q1 and Q2 results indicate that despite com- puter automation, the LA systems still require teacher control for reflection activities. In contrast, most of the features in Q3 (bottom-right) (n=7), which involved some degree of computer automation and little human control, were targeted at supporting students. Most referred to using sensors that unobtrusively collect multimodal data, such as physiological data (T2 â€“ Table 3 row 2, T15), interactions with the learning environment (T5), position and timing (T6) and audio data (T16), to make these data available for reflection (T3, Table 3 row 4). There was only one teacher feature in Q3 (T18). The student envisioned it as a system that could help teachers monitor when students pause the learning activity without requiring an intervention by the teacher: \"Teachers have low control because they only observe the usage of the system by students; [students] must not overuse it.\"(P18). Most ideated features were classified into Q4 (top-right) (n=14). These can be categorised as features of the LA system that support decision-making, requiring that the user perform further actions as a response to interacting with AI/analytics outputs. The main characteristics of student features included allowing students to choose what to do next based on automated recommendations. For example, P1 (Table 3, row 1) described a system that automatically analyses a studentâ€™s spoken question and the system provides rec- ommended instructions to complete a learning task. On the other hand, most teacher-focused features require teachers to act as de- signers to configure and evaluate automation outcomes that may eventually improve future learning activities (P10). These features require teachers to interpret data representations or prediction re- sults and act upon them to benefit students (P2, P4, P5, P6, and P17). 5.3 Perceptions of a safe, reliable, and trustworthy LA system (RQ3) 5.3.1 Safety. Regarding safety, nine students (ğ‘› = 9) related this to privacy concerns, explaining they had no concerns as long 30Downloaded from the ACM Digital Library on April 8, 2025. LAK â€™24, March 18â€“22, 2024, Kyoto, Japan Alfredo et al. Figure 3: A mapping of studentsâ€™ envisaged features into the two-dimensional HCAI framework. Each hexagon represents a feature aimed at supporting students (green), teachers (yellow) or both (stacked green-yellow). Examples of these features and their descriptions are presented in Table 3. The complete list can be found here. ID System Name Description Studentâ€™s features Teacherâ€™s features T1 Nursing Virtual Assistant A conversational â€™virtual assistantâ€™ agent that ac- cesses the nursing database and manages tasks. It helps with task handover, prioritises, and pro- vides clinical information when students query verbally during the simulation. [e.g., 23] Students have the agency to ask questions and use the information to support their actions (Q4) The teacherâ€™s feature was not dis- cussed. (N/A) T2 Subjective data chart for reflec- tion A system that analyses & displays â€™subjectiveâ€™ in- formation (e.g., stress level) apart from â€™pedagog- icalâ€™ information (e.g., task prioritisation) during debriefing. It analyses studentsâ€™ audio & stress level information and displays it visually. [e.g., 1] Students can only see the informa- tion and listen to teachersâ€™ expla- nations (Q3) In debriefing, teachers will make sense of this automatically visu- alised information and explain its meaning to students. (Q4) T14 Video footage player Video recording and replaying system on a clini- cal simulation to support reflection in debriefing. [e.g., 38, 42] Students donâ€™t have any control over the system. (Q1) Teachers replay the video to help students reflect on their actions. (Q2) T3 Communication skills report A system that analyses speech data to give feed- back about communication skills. It detects speak- ers, analyses communication, categorises leader- ship styles, and generates & sends a feedback re- port for reflection. [e.g., 17, 19] Students can only sensemaking the information but donâ€™t have any control over the systemâ€™s decision-making or other interac- tions with it (Q3). The teacherâ€™s features were not discussed (N/A) Table 3: A sample of selected LA systems (ğ‘› = 4, ğ‘ = 20 systems; ğ‘› = 6, ğ‘ = 35 features) from human control and computer automation features generation step. Examples of the LA system from literature are provided for each ideated system. The complete list of other systems and features is available in here as students fully consented to the data collection and data were anonymised or de-identified. For instance, P15 stated: â€œIf results are de-identified, and also having an informed conversation and getting consent, then it would be fine.â€ P6 added: â€œAt the end of the day, it is peopleâ€™s data. But I have no concerns because it is all anonymous, as far as I know.â€ Another key topic associated to safety was data sharing. P6 ex- plained that their data should be shared with their teacher: â€œTeachers should be able to see student data to know who to communicate with. I think it is just a teacher-student relationship like they are there to support you, learn, and go in the right direction. So there are no privacy concerns with themâ€. P10 and P19 envisaged the privacy of systems when they are used beyond the current learning setting. P10 said, â€œIn real-world practice, any information about the patient was mostly a piece of sensitive information [...]. The only people who can hear/see it should be us, i.e., nurses and doctors. So, [systemâ€™s name] must ensure this private information is not leaked.â€ Although P4, P7, P9, and P14 explicitly mentioned that they had little concern for their privacy, they still considered the privacy of the data of other students. For example: â€œI wouldnâ€™t be concerned myself, but maybe other students involved may be concerned and may not want the video footage to be shown to the class or given to people to watch 31Downloaded from the ACM Digital Library on April 8, 2025. A Method for Designing Human-Centred LA Systems LAK â€™24, March 18â€“22, 2024, Kyoto, Japan at home.â€ (P7). Another example is from P4, who explained: \"I donâ€™t mind [sharing my data] because I know itâ€™s for my benefit. And I am sure there would have to be some privacy regulations protecting my privacy and distribution or download.\" 5.3.2 Reliability. Most students (ğ‘› = 15) associated the expected high accuracy of the information provided by the LA system to their perceptions of reliability, especially concerning features that require critical or immediate action in this clinical learning context. For example, P5 and P21 explained that the information must be highly accurate to minimise misinterpretation: â€œIt needs to be able to tell exactly when blood pressure was taken or when a respiratory rate was counted. If it is inaccurate, that would be uselessâ€ (P5) and â€œIt needs to be highly accurate. Because when we performed a vital check manually, we need to check the trend, and I donâ€™t think the trend can lie to usâ€ (P21). P20 added the importance of accurate information in the design or re-design of the learning task: â€œWe have the resources to make it as accurate as possible. So, I donâ€™t think we should settle for any worse. When we have the information available, the most up- to-date and relevant information should be utilised in the simulation design by healthcare staff.â€ In contrast, some students (ğ‘› = 6) suggested that the LA system can provide limited or approximate information and still promote learning opportunities, primarily if the data is related to their own previous experiences. P17 explained \"It sort of gives us a reminder, time to act and improve on certain areas of nursing and communica- tion. And that is why even if it is just an approximation, it can still help us improve\". P16 also added \"If itâ€™s just like a rough indicator of what you need to do, then itâ€™s okay so you are on the right track, but itâ€™s not telling me step by step. So you get to learn\". 5.3.3 Trustworthiness. Although many students envisaged systems with high human control and computer automation (Q4, n=11), they indicated they could not fully trust the information from the LA system because they needed to have control. For example: \"our clinical judgement should always come first. These can be used as guidance and are great systems to incorporate into your clinical prac- tice. But we must always judge and make decisions using our [critical thinking]. We canâ€™t always trust technology\"(P20). \"I would not trust it. For example, if I see it is true, but then [the system] says something else, I think thatâ€™s a sign of confirmation bias if I must follow it. That could affect how you [see and use] the information.\"(P1) Three students (P7, P12, & P14), who envisaged systems with features of high human control and low computer automation (Q2 in Figure 3), commented that they trusted the information as long as they are commonly known as factual information or when they are in control. \"It is video footage, so itâ€™s either there or not. So I still trust it.\"(P7); \"Because itâ€™s more like collecting live footage. So, you cannot go wrong with live footage unless there is a [lag/video issue].\"(P14); \"I think itâ€™s [manually analysing] the data with technology, and in that sense, I will trust the technology. Again, because I am in control.\" (P12). In sum, students strongly associated trustworthiness with human control. 6 DISCUSSION 6.1 Research Questions Regarding RQ1, the elicitation and prioritisation of potential LA systems with students can be accomplished through both divergent and convergent thinking. Divergent thinking can be operationalised to identify learning challenges and envisage LA solutions, while convergent thinking can prioritise ideas of LA systems tailored to most learning challenges. While the double diamond design thinking (divergent-convergent) approach has been applied in an LA study in the past [34], the authors had already pre-committed to creating a specific tool (an LA teacher dashboard) instead of allowing educational stakeholders to suggest alternative solutions. While a pre-commitment can be valid and practical, SLADE can also engage students or other stakeholders in the ideation process and in mapping solution ideas to authentic problems. Ultimately, it may be possible that a dashboard might not be the optimal solution for such issues. Moreover, students struggled to articulate their data needs in Step 1. This aligns with a key challenge previously identified in HCLA research [39]: students lacking data analytics expertise may struggle to envision the full range of possibilities for ideated LA systems, thereby impeding innovation. Yet, students are the ex- perts in the learning challenges that they experience. Hence, our results can be used as empirical evidence to advocate for stakeholder involvement and acknowledge their lived experiences as valuable and essential expertise they can bring to the design process of LA systems [5]. Notably, our system metric in Step 2 can provide a systematic approach to quantitatively assess studentsâ€™ LA system ideas, allowing further analysis and exploration. This metric can be used as one of several approaches to evaluate design outcomes. The lack of measures to assess HCLA design processes and outcomes has indeed been highlighted by Lang and Davis [37] as a significant current shortcoming in most HCLA studies. Regarding RQ2, we identified the human-controlled and auto- mated features of LA systems with the participation of students (see Figure 3). Most features emphasised the need for preserving teacher and student agency in LA systems (Q2 and Q4). It aligns with the primary intention of the HCAI framework of designing automated systems that provide high human control aiming to empower users [55, 57]. Moreover, when students are asked about generating fea- tures in their chosen LA system, they want to control the system for themselves and their teachers. This supports the suggestion that using fully automated features, even for low-stakes tasks, should be approached cautiously [43]. For example, if studentsâ€™ data is inaccu- rately recorded, an automated system might generate an incorrect report, which could negatively impact the studentâ€™s perception of their performance [35]. Therefore, it is essential to exercise dis- cretion when relying on such features to ensure they empower humans effectively and do not cause unintended consequences. Regarding RQ3, most students believed that privacy and data sharing issues were relatively simple to mitigate via informed con- sent and data anonymisation, as found in a study with LA experts [53]. However, the LA literature indicates that much more work is still needed for students to understand the actual implications of using data in education [28, 31]. Although students suggested that they may easily trust the system and still learn from imperfect 32Downloaded from the ACM Digital Library on April 8, 2025. LAK â€™24, March 18â€“22, 2024, Kyoto, Japan Alfredo et al. analytics outputs, they provided little detail about how designers and developers may operationalise actual changes in the system to ensure reliability and trustworthiness. Yet, students can connect hu- man control with trustworthiness, highlighting students are more likely to trust AI systems that give them a sense of control over their data [56]. 6.2 Implications for Research and Practice This paper directly contributes to the emerging HCLA research community by providing a systematic design methodology to cre- ate safe, reliable, and trustworthy LA systems [10, 39]. In addition, SLADE is illustrated with empirical evidence in the context of collab- orative learning and clinical in healthcare education. Shneiderman [55] suggests that technical practices (e.g., recording audit trails, conducting bias testing, and creating explainable interfaces) and data management strategies can help to ensure that AI systems are reliable and trustworthy. However, envisioning adopting such strategies in education goes beyond the scope of studentsâ€™ expertise. The involvement of other more technical stakeholders, such as insti- tutional leaders, educational technologists, software developers and AI experts, is critical in the design process from the ideation stages [49]. It is recommended that the LA system should have separate computer automation (AI) and control features that are tailored to students and teachers. Rather than having a single monolithic LA system in one quadrant, different features of the same system should be placed in various HCAI quadrants to cater to the specific needs of each user group. The former can result in overlooking some stakeholdersâ€™ requirements [39, 58]. 6.3 Limitations and Future Directions Our study has several limitations. We used SLADE only with one group of stakeholders. Although the outcome of this paper informs the next steps in designing an actual LA system, we recommend that future work apply SLADE in other contexts and involve other critical educational and technical stakeholders (e.g., educators, LA designers, and AI developers). We also recommend considering other factors, such as pedagogical value and feasibility of the sys- tem in practice, that could support the learning process [e.g., 61]. Since SLADE focuses on informing the design of these systems, their full implementation details are beyond the scope of this paper. Despite these limitations, this paper should be seen as one of the first of several potential works needed to support researchers and designers in adopting and adapting methods to create HCLA sys- tems that are safe, reliable and trustworthy. In the future, we plan to expand SLADE by including teachers and learning designers, gaining practical perspectives in teaching and learning. 7 CONCLUDING REMARKS This paper contributes to the HCLA field by showcasing a design methodology in an authentic study about collaborative learning in healthcare. It involves working with students to generate and refine ideas using divergent-convergent thinking. The goal is to design a balanced approach to human control and automation towards LA systems that are safe, reliable, and trustworthy. This method bridges the gap between student learning challenges and ideated LA systems, explores the balance of human control and automation features to promote agency, and elicits studentsâ€™ perceptions of LA systemsâ€™ safety, reliability, and trustworthiness. This contributes to the discussion on designing LA systems with a two-dimensional HCAI framework and stakeholder involvement, particularly involv- ing students in the design process. ACKNOWLEDGMENTS Riordan Alfredo gratefully acknowledges Monash University for his PhD scholarship. This research was funded partially by the Australian Government through the Australian Research Council (project number DP210100060). The authors thank Lixiang Yan, Linxuan Zhao, and Xinyu Li for their valuable assistance in recruit- ing participants for this study. REFERENCES [1] R. D. Alfredo, L. Nie, P. Kennedy, T. Power, C. Hayes, H. Chen, C. McGregor, Z. Swiecki, D. GaÅ¡eviÄ‡, and R. Martinez-Maldonado. 2023. \"That Student Should Be a Lion Tamer!\" StressViz: Designing a Stress Analytics Dashboard for Teach- ers. In LAK23: 13th International Learning Analytics and Knowledge Conference (LAK2023). ACM, 57â€“67. https://doi.org/10.1145/3576050.3576058 [2] A. S. Alzahrani, Y.-S. Tsai, N. Aljohani, E. Whitelock-Wainwright, and D. Gasevic. 2023. Do teaching staff trust stakeholders and tools in learning analytics? A mixed methods study. Educational technology research and development (2023), 1â€“31. [3] P. An, K. Holstein, B. dâ€™Anjou, B. Eggen, and S. Bakker. 2020. The TA Frame- work: Designing Real-time Teaching Augmentation for K-12 Classrooms. In CHI Conference on Human Factors in Computing Systems. ACM, 1â€“17. https: //doi.org/10.1145/3313831.3376277 [4] R. S. Baker. 2016. Stupid Tutoring Systems, Intelligent Humans. Int. J. AI in Educ. 26, 2 (Jun 2016), 600â€“614. https://doi.org/10.1007/s40593-016-0105-0 [5] C. Barreiros, P. Leitner, M. Ebner, E. Veas, and S. Lindstaedt. 2023. Students in Focus â€“ Moving Towards Human-Centred Learning Analytics. In Practicable Learning Analytics, O. Viberg and Ã…. GrÃ¶nlund (Eds.). Springer Inter. Publ., Cham, 77â€“94. https://doi.org/10.1007/978-3-031-27646-0_5 [6] G. Biesta, M. Priestley, and S. Robinson. 2015. The Role of Beliefs in Teacher Agency. Teachers and Teaching 21, 6 (Aug. 2015), 624â€“640. [7] A. Bozkurt, A. Karadeniz, D. Baneres, A. E. Guerrero-RoldÃ¡n, and M. E. RodrÃ­guez. 2021. Artificial intelligence and reflections from educational landscape: a review of AI studies in half a century. Sustainability 13, 2 (2021), 800. [8] V. Braun and V. Clarke. 2012. Thematic Analysis. APA, Washington, DC, 57â€“71. https://doi.org/10.1037/13620-004 [9] R. Brennan and D. Perouli. 2022. Generating and Evaluating Collective Concept Maps. In LAK22: LAK22: 12th International Learning Analytics and Knowledge Conference. Association for Computing Machinery, New York, NY, USA, 570â€“576. https://doi.org/10.1145/3506860.3506918 [10] S. Buckingham Shum, R. Ferguson, and R. Martinez-Maldonado. 2019. Human- centred learning analytics. Journal of Learning Analytics 6, 2 (2019), 1â€“9. [11] B. Chen and H. Zhu. 2019. Towards Value-Sensitive Learning Analytics Design. In LAKâ€™19: Proc. of the 9th Int. Conf. on Learning Analytics & Knowledge. 343â€“352. https://doi.org/10.1145/3303772.3303798 arXiv:1812.08335 [cs] [12] T. H. Davenport. 2018. From Analytics to Artificial Intelligence. 1, 2 (2018), 73â€“80. https://doi.org/10.1080/2573234X.2018.1543535 [13] Design Council. 2023. Framework for Innovation. https://www.designcouncil.org. uk/our-resources/framework-for-innovation [Online; accessed 10. Aug. 2023]. [14] D. Di Mitri, J. Schneider, and H. Drachsler. 2022. Keep Me in the Loop: Real- Time Feedback with Multimodal Data. Int. J. Artif. Intell. Educ. 32, 4 (Dec. 2022), 1093â€“1118. https://doi.org/10.1007/s40593-021-00281-z [15] M. E. Dogan, T. Goru Dogan, and A. Bozkurt. 2023. The Use of Artificial In- telligence (AI) in Online Learning and Distance Education Processes: A Sys- tematic Review of Empirical Studies. Applied Sciences 13, 5 (Jan. 2023), 3056. https://doi.org/10.3390/app13053056 [16] M. Dollinger, D. Liu, N. Arthars, and J. M. Lodge. 2019. Working together in learning analytics towards the co-creation of value. Journal of Learning Analytics 6, 2 (2019), 10â€“26. [17] A. Dood, K. Das, Z. Qian, S. Finkenstaedt-Quinn, A. Gere, and G. Shultz. 2023. A Dashboard to Provide Instructors with Automated Feedback on Studentsâ€™ Peer Review Comments. In LAK2023: LAK23: 13th International Learning Analytics and Knowledge Conference. Association for Computing Machinery, New York, NY, USA, 619â€“625. https://doi.org/10.1145/3576050.3576087 [18] H. Drachsler and W. Greller. 2016. Privacy and analytics: itâ€™s a DELICATE issue a checklist for trusted learning analytics. In LAK â€™16: Proc. of the 6th Intl. 33Downloaded from the ACM Digital Library on April 8, 2025. A Method for Designing Human-Centred LA Systems LAK â€™24, March 18â€“22, 2024, Kyoto, Japan Conf. on Learning Analytics & Knowledge. ACM, New York, NY, USA, 89â€“98. https://doi.org/10.1145/2883851.2883893 [19] V. Echeverria, M. Wong-Villacres, X. Ochoa, and K. Chiluiza. 2022. An Exploratory Evaluation of a Collaboration Feedback Report. In LAK22: LAK22: 12th Interna- tional Learning Analytics and Knowledge Conference. ACM, New York, NY, USA, 478â€“484. https://doi.org/10.1145/3506860.3506890 [20] J. A. Erickson, A. F. Botelho, S. McAteer, A. Varatharaj, and N. T. Heffernan. 2020. The automated grading of student open responses in mathematics. In LAK â€™20: Proceedings of the 10th Intl. Conf. on Learning Analytics & Knowledge. ACM, New York, NY, USA, 615â€“624. [21] J. Fereday and E. Muir-Cochrane. 2006. Demonstrating Rigor Using Thematic Analysis: A Hybrid Approach of Inductive and Deductive Coding and Theme Development. Int. J. Qual. Methods 5, 1 (March 2006), 80â€“92. https://doi.org/10. 1177/160940690600500107 [22] G. M. Fernandez Nieto, K. Kitto, S. Buckingham Shum, and R. Martinez- Maldonado. 2022. Beyond the Learning Analytics Dashboard: Alternative Ways to Communicate Student Data Insights Combining Visualisation, Nar- rative and Storytelling. In LAK22. ACM, New York, NY, USA, 219â€“229. https: //doi.org/10.1145/3506860.3506895 [23] R. Gubareva and R. Lopes. 2020. Virtual Assistants for Learning: A Systematic Literature Review:. In Proc. of the 12th Intl. Conf. on Computer Supported Education. SCITEPRESS - Science and Technology Publications, Prague, Czech Republic, 97â€“103. https://doi.org/10.5220/0009417600970103 [24] B. Hanington and B. Martin. 2012. Universal Methods of Design: 100 Ways To Research Complex Problems, develop innovative ideas, and Design Effective Solutions. Rockport Publishers. [25] D. J. Haraway. 2016. Staying with the trouble: Making kin in the Chthulucene. Duke University Press. [26] M. Hartikainen, K. VÃ¤Ã¤nÃ¤nen, and T. Olsson. 2023. Towards a Human-Centred Artificial Intelligence Maturity Model. In Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems (Hamburg, Germany) (CHI EA â€™23). ACM, New York, NY, USA, Article 285, 7 pages. [27] P. Z. Harvard. 2023. Generate-Sort-Connect-Elaborate | Project Zero. https: //pz.harvard.edu/resources/generate-sort-connect-elaborate [28] W. Holmes, K. Porayska-Pomsta, K. Holstein, E. Sutherland, T. Baker, S. B. Shum, O. C. Santos, M. T. Rodrigo, M. Cukurova, I. I. Bittencourt, et al. 2022. Ethics of AI in education: Towards a community-wide framework. Int. J. AI in Educ. 32, 3 (2022), 504â€“526. [29] K. Holstein, V. Aleven, and N. Rummel. 2020. A Conceptual Framework for Humanâ€“AI Hybrid Adaptivity in Education. In AIED. Vol. 12163. 240â€“254. [30] K. Holstein, B. M. McLaren, and V. Aleven. 2019. Co-Designing a Real-Time Classroom Orchestration Tool to Support Teacherâ€“AI Complementarity. Learning Analytics 6, 2 (July 2019), 27â€“52. https://doi.org/10.18608/jla.2019.62.3 [31] D. Hooshyar, K. Tammets, T. Ley, K. Aus, and K. Kollom. 2023. Learning Analytics in Supporting Student Agency: A Systematic Review. Sustainability 15, 18 (Jan. 2023), 13662. https://doi.org/10.3390/su151813662 [32] P. JÃ¤Ã¤skelÃ¤, V. Heilala, T. KÃ¤rkkÃ¤inen, and P. HÃ¤kkinen. 2021. Student agency analytics: learning analytics as a tool for analysing student agency in higher education. Behaviour & Information Technology 40, 8 (June 2021), 790â€“808. https: //doi.org/10.1080/0144929X.2020.1725130 [33] C. Kaendler, M. Wiedmann, N. Rummel, and H. Spada. 2015. Teacher Compe- tencies for the Implementation of Collaborative Learning in the Classroom: a Framework and Research Review. Educ. Psychol. Rev. 27, 3 (Sept. 2015), 505â€“536. https://doi.org/10.1007/s10648-014-9288-9 [34] O. Karademir, A. Ahmad, J. Schneider, D. Di Mitri, I. Jivet, and H. Drachsler. 2021. Designing the Learning Analytics Cockpit - A Dashboard that Enables Interventions. In Method. and Intel. Systems for Technology Enhanced Learning, 11th Inter. Conf. Springer, Cham, Switzerland, 95â€“104. https://doi.org/10.1007/978- 3-030-86618-1_10 [35] K. Kitto, S. Buckingham Shum, and A. Gibson. 2018. Embracing imperfection in learning analytics. In LAK â€™18: Proc. of the 8th Int. Conf. on Learning Analytics and Knowledge. ACM, 451â€“460. https://doi.org/10.1145/3170358.3170413 [36] R. Koster, J. Balaguer, A. Tacchetti, A. Weinstein, T. Zhu, O. Hauser, D. Williams, L. Campbell-Gillingham, P. Thacker, M. Botvinick, et al. 2022. Human-centred mechanism design with Democratic AI. Nature Human Behaviour 6, 10 (2022), 1398â€“1407. [37] C. Lang and L. Davis. 2023. Learning Analytics and Stakeholder Inclusion: What Do We Mean When We Say \"Human-Centered\"?. In LAK23: 13th International Learning Analytics and Knowledge Conference. ACM, Arlington TX USA, 411â€“417. https://doi.org/10.1145/3576050.3576110 [38] X. Li, L. Yan, L. Zhao, R. Martinez-Maldonado, and D. Gasevic. 2023. CVPE: A Computer Vision Approach for Scalable and Privacy-Preserving Socio-spatial, Multimodal Learning Analytics. In LAK23: 13th Inter. Learning Analytics and Knowledge Conference. ACM, New York, NY, USA, 175â€“185. https://doi.org/10. 1145/3576050.3576145 [39] R. Martinez-Maldonado. 2023. Human-Centred Learning Analytics: Four Chal- lenges in Realising the Potential. Journal of Learning Letters (2023). https: //doi.org/10.59453/FIZJ7007 [40] R. Martinez-Maldonado, D. Elliott, C. Axisa, T. Power, V. Echeverria, and S. Buck- ingham Shum. 2022. Designing Translucent Learning Analytics with Teachers: An Elicitation Process. Interactive Learning Environments 30, 6 (July 2022), 1077â€“ 1091. https://doi.org/10.1080/10494820.2019.1710541 [41] N. McDonald, S. Schoenebeck, and A. Forte. 2019. Reliability and Inter-rater Reliability in Qualitative Research: Norms and Guidelines for CSCW and HCI Practice. Proceedings of the ACM on Human-Computer Interaction 3, CSCW (2019), 1â€“23. https://doi.org/10.1145/3359174 [42] N. Mohammadhassan and A. Mitrovic. 2022. Investigating the Effectiveness of Visual Learning Analytics in Active Video Watching. In AIED. 127â€“139. https: //doi.org/10.1007/978-3-031-11644-5_11 [43] I. Molenaar. 2022. Towards hybrid human-AI learning technologies. Eur. J. Educ. 57, 4 (Dec. 2022), 632â€“645. https://doi.org/10.1111/ejed.12527 [44] J. W. Moore. 2016. What Is the Sense of Agency and Why Does It Matter? Frontiers in Psychology 7 (Aug. 2016), 1272. [45] W. Morris, S. Crossley, L. Holmes, and A. Trumbore. 2023. Using Transformer Language Models to Validate Peer-Assigned Essay Scores in Massive Open Online Courses (MOOCs). In LAK23: 13th Inter. Learning Analytics and Knowledge Conf. ACM, 315â€“323. https://doi.org/10.1145/3576050.3576098 [46] A. Muslim, M. A. Chatti, T. Mahapatra, and U. Schroeder. 2016. A Rule-Based Indicator Definition Tool for Personalized Learning Analytics. Association for Computing Machinery, 264â€“273. https://doi.org/10.1145/2883851.2883921 [47] T. Nazaretsky, M. Cukurova, and G. Alexandron. 2022. An Instrument for Mea- suring Teachersâ€™ Trust in AI-Based Educational Technology. In LAK22: 12th International Learning Analytics and Knowledge Conference. ACM, Online USA, 56â€“66. https://doi.org/10.1145/3506860.3506866 [48] A. Pardo and G. Siemens. 2014. Ethical and privacy principles for learning analytics. Br. J. Educ. Technol. 45, 3 (May 2014), 438â€“450. [49] L. P. Prieto, M. J. RodrÃ­guez-Triana, R. MartÃ­nez-Maldonado, Y. Dimitriadis, and D. GaÅ¡eviÄ‡. 2019. Orchestrating learning analytics (OrLA): Supporting inter- stakeholder communication about adoption of learning analytics at the classroom level. AJET 35 (2019). https://doi.org/10.14742/ajet.4314 [50] C. G. Prieto-Alvarez, R. Martinez-Maldonado, and T. D. Anderson. 2018. Co- designing learning analytics tools with learners. In Learning analytics in the classroom, J. Lodge, K. Thompson, J. Horvath, P. De Barba, and M. Blumenstein (Eds.). Routledge, 93â€“110. [51] A. Renz and G. Vladova. 2021. Reinvigorating the discourse on human-centered artificial intelligence in educational technologies. Technology Innovation Man- agement Review 11, 5 (2021). [52] J. P. Sarmiento and A. F. Wise. 2022. Participatory and Co-Design of Learn- ing Analytics: An Initial Review of the Literature. In LAK22: 12th International Learning Analytics and Knowledge Conference, Vol. 1. ACM, Online USA, 535â€“541. https://doi.org/10.1145/3506860.3506910 [53] M. Scheffel, Y.-S. Tsai, D. GaÅ¡eviÄ‡, and H. Drachsler. 2019. Policy Matters: Expert Recommendations for Learning Analytics Policy. In Transforming Learning with Meaningful Technologies. Springer, Cham, Switzerland, 510â€“524. https: //doi.org/10.1007/978-3-030-29736-7_38 [54] J. J. Shah, S. M. Smith, and N. Vargas-Hernandez. 2003. Metrics for Measuring Ideation Effectiveness. 24, 2 (2003), 111â€“134. [55] B. Shneiderman. 2022. Human-Centered AI. Oxford University Press, Oxford, New York. [56] S. Slade, P. Prinsloo, and M. Khalil. 2019. Learning Analytics at the Intersections of Student Trust, Disclosure and Benefit. In Proc. of the 9th Intl. Conf. on Learning Analytics & Knowledge (New York, NY, USA, 2019-03-04) (LAK19). ACM, 235â€“244. https://doi.org/10.1145/3303772.3303796 [57] U. A. Usmani, A. Happonen, and J. Watada. 2023. Human-Centered Artificial Intelligence: Designing for User Empowerment and Ethical Considerations. In 2023 5th International Congress on Human-Computer Interaction, Optimization and Robotic Applications (HORA). IEEE, Istanbul, Turkiye, 01â€“05. https://doi.org/ 10.1109/HORA58378.2023.10156761 [58] O. Viberg, I. Jivet, and M. Scheffel. 2023. Designing Culturally Aware Learning Analytics: A Value Sensitive Perspective. Springer Inter. Publ., Cham, 177â€“192. https://doi.org/10.1007/978-3-031-27646-0_10 [59] K. B. Yang, V. Echeverria, Z. Lu, H. Mao, K. Holstein, N. Rummel, and V. Aleven. 2023. Pair-Up: Prototyping Human-AI Co-orchestration of Dynamic Transitions between Individual and Collaborative Learning in the Classroom. In CHI â€™23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. Association for Computing Machinery, New York, NY, USA, 1â€“17. https://doi. org/10.1145/3544548.3581398 [60] S. J. H. Yang, H. Ogata, T. Matsui, and N.-S. Chen. 2021. Human-Centered Artificial Intelligence in Education: Seeing the Invisible through the Visible. Computers and Education: Artificial Intelligence 2 (2021). https://doi.org/10.1016/j.caeai.2021. 100008 [61] F. Zhao, G.-Z. Liu, J. Zhou, and C. Yin. 2023. A Learning Analytics Framework Based on Human-Centered Artificial Intelligence for Identifying the Optimal Learning Strategy to Intervene in Learning Behavior. 26, 1 (2023), 132â€“146. jstor:48707972 https://www.jstor.org/stable/48707972 34Downloaded from the ACM Digital Library on April 8, 2025.","libVersion":"0.3.2","langs":""}