Shared on 2025-04-09

Artificial Intelligence (AI) is increasingly central to contemporary information systems, offering transformative potential in how knowledge is generated, organized, and retrieved. However, this same capacity entails critical risks when such systems are developed from biased datasets or without an awareness of their broader sociotechnical implications. In the context of women’s representation and that of other <mark class="hltr-green">minoritized identities</mark>, AI can either reinforce historical inequalities or serve as a tool for redress—if designed through ethical, inclusive, and feminist frameworks. This section examines how AI models—particularly generative and neuro-symbolic approaches—can contribute to more equitable knowledge infrastructures. Drawing on the HerStory project, it explores the intersection of technological innovation and epistemic responsibility, highlighting opportunities to reimagine historical memory through interdisciplinary and gender-aware methodologies. 

The HerStory research project, funded by the Plan Nacional I+D+I of the Ministry of Science and Innovation of Spain (PID2023-147673OB-I00, 2024–2027), arises from a pressing need to bridge both historical and technological silences. At its core, the project addresses the marginalization of women and gender minorities in the representation of historical knowledge, as well as the structural limitations of current digital infrastructures. Although it is situated within an academic research context, the implications of HerStory extend far beyond, encompassing the ethical, social, and epistemological challenges posed by the integration of AI into historical and cultural domains. 

The project adopts an interdisciplinary perspective, drawing from Library and Information Science (LIS), Communication, Computer Science, the Humanities, Law, and active collaboration with the Wikipedia and Wikidata communities. Its main focus lies in the interconnected processes of content creation, curation, and access—particularly as they relate to digital databases documenting Francoist repression and censorship in Spain (1936–1975). These sources are examined through a gender and intersectional lens, with the aim of making visible the voices historically rendered unheard. 

A key <mark class="hltr-orange">challenge</mark> identified by HerStory lies in the fragmented nature of digital historical and humanities databases—especially those developed within academic environments. Each research initiative, whether a doctoral thesis or a European-funded project, often creates its own standalone database. These <mark class="hltr-yellow">data silos hinder the ability to identify shared entities across projects</mark>. For example, if the same woman appears in multiple archives, there is no automatic mechanism to detect or connect her presence. In research focusing on women’s narratives—already marked by structural invisibility—establishing links between projects becomes essential for reconstructing <mark class="hltr-yellow">fragmented life stories and fostering a more coherent and inclusive collective memory. </mark>

Beyond this local fragmentation, a broader issue arises: many databases do not interface with large external repositories such as Wikidata, and thus the knowledge they contain often remains confined to academic silos, failing to reach public platforms like Wikipedia. This disconnection limits the visibility and usability of research, especially for those engaged in gender history and digital heritage. 

To confront these issues, HerStory structures its work around three <mark class="hltr-orange">key areas</mark>: 

- <mark class="hltr-orange">Content creation:</mark> enriching Wikidata entities, generating Wikipedia articles, and linking records across databases. 
    

- <mark class="hltr-orange">Content curation:</mark> improving the classification, summarization, and indexing of information for future retrieval. 
    

- <mark class="hltr-orange">Content access: </mark> ensuring inclusive and efficient search and browsing within both public and academic platforms. 
    

These activities are <mark class="hltr-green">hindered by infrastructural and epistemic limitations</mark>, particularly with respect to gender and minoritized identities. 

What emerges is <mark class="hltr-red">the understanding that the invisibility of women and gender minorities is not simply a historical oversight but a structural consequence of how data have been collected, described, and categorized</mark>. Traditional historical processes often devalued or omitted women’s contributions, leading to their persistent underrepresentation in both primary and digital sources. The use of AI in these contexts risks amplifying these silences if the systems are trained on biased data and deployed without critical oversight. 

In today’s data-driven knowledge ecosystems, where AI increasingly mediates access to information, there is a pressing need for inclusive, transparent, and reflexive design. Without such intentionality, digital infrastructures will continue to exclude those already marginalized from dominant narratives. In this regard, <mark class="hltr-red">HerStory does not simply seek to apply AI—it aims to reorient its purpose toward feminist, interdisciplinary, and socially just knowledge practices</mark>. 

This need for intentional, inclusive design becomes even more urgent when considering the specific capacities and limitations of generative artificial intelligence (GAI). #tech/Ai/GAI is a subset of #tech/Ai AI technologies designed not to classify or predict, but to generate content—text, images, audio—based on learned examples. Unlike traditional AI, which focuses on recognition and rule-based processing, generative systems aim to <mark class="hltr-orange">simulate human-like creativity and responsiveness</mark>. This shift is enabled by massive neural networks capable of learning from internet-scale data and generating plausible responses. 

As <mark class="hltr-yellow">these systems increasingly mediate the way historical knowledge is produced and disseminated</mark>, they introduce not only new opportunities for visibility, but also complex challenges related to bias, opacity, and epistemic harm. To better understand these dynamics, the HerStory project turns its attention to the inner workings of generative models, asking how their architectures and training data shape what—and who—can be represented. In particular, it interrogates how AI systems might either replicate or disrupt dominant gender narratives, depending on how they are trained, fine-tuned, and integrated into digital heritage infrastructures. 

 To critically engage with these challenges, it is essential to understand the underlying paradigms that shape how AI systems operate. Artificial Intelligence (AI) has evolved through distinct paradigms, each offering unique strengths and limitations—particularly when applied to socially sensitive domains such as gender representation. Broadly speaking, we can identify <mark class="hltr-red">two dominant approaches: symbolic AI and sub-symbolic AI, with a third, hybrid paradigm—neural-symbolic AI—now emerging as a promising alternative.</mark> #tech/symbolic_reasoning #tech/Neural_reasoning 

<mark class="hltr-green">Symbolic AI</mark> is grounded in the use of explicit rules, formal logic, and structured representations. It is particularly valued for its interpretability and predictability, as its reasoning processes are transparent and can be traced step by step. For example, in the context of gender classification within archival systems, a symbolic AI approach might rely on a fixed taxonomy—such as “man,” “woman,” and “other”—defined through ontologies or metadata schemas. While this structure provides clarity and consistency, it often lacks the flexibility to accommodate non-binary identities, evolving terminology, or intersectional dimensions of gender, thus risking the reinforcement of normative frameworks. 

In contrast, <mark class="hltr-green">sub-symbolic AI</mark>, notably deep learning, learns from data patterns without relying on predefined rules. These models can process vast datasets and uncover complex relationships, making them highly effective in tasks like natural language processing or image recognition. In gender-related applications, for instance, a deep learning model trained on millions of online biographies might learn to infer the gender of a person based on names, pronouns, or associated activities. However, this approach often suffers from opacity—it becomes difficult to understand why the model made a certain prediction—and it is prone to bias, especially if the training data reflect gender stereotypes or underrepresent certain groups. A typical consequence is the misclassification of women in STEM fields due to learned associations between scientific language and male identities. 

To address the shortcomings of<mark class="hltr-green"> both symbolic rigidity and sub-symbolic opacity,</mark> a <mark class="hltr-red">third paradigm—neural-symbolic AI—has gained traction</mark>. This hybrid approach seeks to combine the pattern recognition capacity of neural networks with the logical structure of symbolic systems. In doing so, it enables more robust, flexible, and explainable AI models. For example, a neural-symbolic system designed to support gender-inclusive information retrieval might use deep learning to process natural language queries while relying on symbolic ontologies (e.g., Wikidata properties) <mark class="hltr-yellow">to ensure that results reflect a broader spectrum of gender identities and avoid exclusionary logic. Such systems could also support fairer visibility of women and non-binary individuals in digital heritage platforms</mark> , by ensuring that recommendations, visualizations, or classifications are <mark class="hltr-red">not only data-driven but epistemologically sound</mark>. 

In sum, while symbolic and sub-symbolic paradigms offer distinct contributions, their integration in neural-symbolic AI holds the greatest potential for advancing inclusive, accountable, and critically aware AI applications, particularly in domains where gender equity and epistemic justice are at stake. 

Building on this theoretical framework, it is crucial to examine how these paradigms materialize in current AI technologies—particularly in the case of Large Language Models (LLMs), which exemplify the capabilities and risks of sub-symbolic approaches. Large Language Models such as <mark class="hltr-yellow">GPT, BERT, or LLaM</mark>A are examples of sub-symbolic generative AI systems. They learn from vast corpora of text to predict the next most likely word in a sequence. These systems convert text into numerical vectors, allowing them to process language in terms of <mark class="hltr-green">statistical relationships and semantic proximity.</mark> 

Tokens—basic units of language like words or subwords—are embedded into high-dimensional vector spaces. Relationships between tokens are learned through patterns in data, enabling the models to approximate meaning and context. <mark class="hltr-green">Transformers</mark>, the architecture underlying these models, use attention mechanisms to prioritize relevant tokens based on their contextual importance. 

While generative AI systems offer remarkable capabilities, they also pose a number of significant limitations that are particularly consequential when applied to socially sensitive domains such as gender representation. These challenges—often structural and epistemological in nature—must be critically examined to ensure that technological innovation does not reproduce the very exclusions it seeks to overcome. 

First, ==data voracity== represents a foundational limitation. Generative models require vast amounts of training data to perform effectively; however, these datasets are frequently unevenly distributed and inaccessible. In the context of gender, this means that biographies of men—especially white, Western, and institutionally prominent men—are far more prevalent in digital archives than those of women or non-binary individuals. As a result, when a language model is trained on such imbalanced data, it ==tends to center dominant narratives and marginalize less documented voices==. For instance, a generative AI tasked with summarizing academic contributions in a historical context may overlook women scholars simply because their presence in the data is sparse or fragmented. 

Second, these systems suffer from ==opacity==. Unlike rule-based approaches, the decision-making process of deep learning models is often difficult to trace or explain—a phenomenon commonly referred to as the =="black box"== problem. This lack of interpretability becomes particularly problematic when the system is used in heritage or academic settings where accountability and transparency are essential. For example, if a model produces a biographical summary that excludes a woman's pedagogical work but highlights her male peers’ research output, it may be difficult to determine whether this outcome stems from biased input data, flawed weightings in the model, or a combination of both. 

A third critical issue is ==hallucination==—the tendency of generative models to fabricate information that sounds plausible but is factually incorrect. In gender-sensitive applications, hallucinations can result in erroneous or even harmful representations. For instance, a model generating Wikipedia-style content might falsely attribute achievements to male figures while omitting or distorting the contributions of women. Such outputs not only misinform users but also perpetuate the epistemic erasure that feminist scholars have long critiqued. 

Finally, and perhaps most pervasively, is the issue of ==bias==. Because generative AI systems learn from human language and existing corpora, they inevitably inherit the gender, racial, and cultural ==stereotypes== embedded in those sources. When these #themes/HerStory/bias remain unaddressed, they can be amplified rather than mitigated. A notable example is the association of leadership terms (e.g., "CEO," "scientist," "expert") predominantly with male names and pronouns, while terms like "assistant" or "teacher" are more frequently linked to women. Such associations not only reflect societal inequities but also risk reinforcing them when integrated into systems that shape public knowledge and institutional memory. #op/explore 

In sum, the powerful affordances of generative AI must be weighed against these systemic limitations. Addressing them is not merely a technical challenge but an ethical imperative—particularly in domains concerned with representation, inclusion, and historical justice. As such, any use of these technologies must be accompanied by critical frameworks and inclusive design principles that prioritize fairness, transparency, and the amplification of marginalized voices. 

To effectively address these ethical imperatives, it is necessary to ground theoretical concerns in empirical evidence. Recent research has increasingly focused on diagnosing the specific mechanisms through which generative AI systems perpetuate bias—revealing that such biases are not incidental but structurally embedded in data, design, and deployment practices. In this context, a comprehensive review of the literature offers ==valuable insights into how and where these problems arise==. 

A scoping review of 73 academic articles published between 2014 and 2024 confirms that bias in generative AI tends to originate from four primary sources #op/question. 
1. First, <mark class="hltr-yellow">social norms and stereotypes embedded</mark> in training data can influence how models interpret and reproduce language, often reinforcing dominant narratives and marginalizing non-normative identities. 
2. Second, skewed datasets—in which certain groups are underrepresented or systematically misrepresented—further distort the model's output, contributing to a digital environment where voices from the margins are less visible or inaccurately portrayed. Third, algorithmic decisions made during model design frequently prioritize computational efficiency or scalability over ethical considerations such as equity, resulting in architectures that encode structural bias by default. Lastly, human interactions in the form of fine-tuning, prompt engineering, or reinforcement learning can inadvertently reinforce these biases, especially when evaluators or trainers operate without explicit frameworks for diversity and inclusion. 

These issues affect not only the accuracy of the generated outputs, but also broader questions of trust, fairness, and legitimacy in AI systems—particularly when they are used to shape public knowledge or institutional memory. If left unaddressed, such limitations risk perpetuating the same forms of exclusion that have historically silenced women and minoritized communities in traditional archives and information systems. 

In response to these challenges, the HerStory project adopts a two-tiered strategy to mitigate hallucination and bias in large language models, combining retrieval-augmented generation (RAG) with knowledge graph integration. This hybrid architecture grounds generative outputs in verifiable sources while enabling more nuanced and explainable reasoning—shifting AI from probabilistic prediction toward epistemic accountability and inclusion. 

The first layer, RAG, integrates a retrieval component into the generative process. Rather than relying solely on the model’s training data, it retrieves relevant documents from a curated corpus. This enhances output quality in four key ways: it reduces hallucination through semantic search and real-time access to trusted texts; mitigates bias by prioritizing peer-reviewed or community-validated sources; improves transparency by making each output traceable to its sources; and ensures domain specificity by grounding generation in contextually relevant information. 

The second layer complements RAG by integrating knowledge graphs (KGs)—structured databases that represent entities and their relationships through nodes and edges, grounded in formal ontologies. When embedded within a RAG-based system, knowledge graphs enhance the generative process in four key ways. First, they provide semantic structure, enabling the model to retrieve information that is not only textually relevant but also logically connected through defined relationships. Second, they support contextual coherence, allowing the AI to generate outputs that reflect accurate temporal, spatial, and thematic associations—crucial when dealing with historically underrepresented groups. Third, they allow for real-time updates and interoperability, as new entities and relationships can be continuously added and linked across datasets. Finally, KGs improve explainability, offering users a transparent view of how outputs are generated by tracing connections between facts, concepts, and sources. 

Together, the combination of retrieval-augmented generation and knowledge graph integration shifts the system from probabilistic language prediction to a more accountable, traceable, and semantically grounded model of knowledge generation. Rather than producing plausible-sounding but unverifiable content, the system becomes capable of generating outputs that are context-aware, transparent, and responsive to the epistemic demands of inclusion and representation. 

Building on this architecture, the HerStory project is developing a generative AI prototype that operationalizes this hybrid model. The prototype will integrate a large language model (LLM) with a custom knowledge graph to support three main functions: the creation and management of inclusive and transparent databases; the facilitation of interactions between humans and bots within historical data environments; and the automated generation of Wikipedia articles, drawing from structured, validated data. This last function is particularly significant for addressing the visibility gap of women and minoritized identities on widely consulted knowledge platforms. 

At the core of the prototype lies a two-layered knowledge graph architecture. The first layer, the ontology layer, builds upon existing semantic frameworks and is enriched by transforming and aligning data from multiple historical databases. The second, the entity layer, is composed of individuals and events drawn from diverse research projects related to the Francoist dictatorship, and is further supplemented with information from Wikidata and Wikipedia. This dual structure allows the system to capture not only the factual content of historical narratives but also the semantic relationships and conceptual categories that frame them—thereby addressing bias not only in data but also in representation. 

This infrastructure has been explicitly designed to mitigate bias at the representational level, while ensuring that generated outputs remain transparent, traceable, and epistemically robust. By embedding ontological precision into a generative architecture, HerStory seeks to transform AI from a neutral-seeming technical tool into a vehicle for historical and social accountability. 

A key enabler of this vision is Wikidata, which plays a central role in the project's knowledge architecture. Unlike Wikipedia—where editorial dynamics often reflect sociocultural biases and negotiation—Wikidata operates through a more formalized and scalable system of knowledge organization. Its community is actively working to address issues of ontological inconsistency and representational bias, developing tools that maintain coherence across properties, classes, and labels. Of particular importance is Wikidata’s growing capacity to represent gender diversity, offering properties that accommodate non-binary and non-traditional gender identities—something that remains difficult to implement in many mainstream data infrastructures. HerStory both leverages and contributes to this ongoing evolution, aligning its design with the broader ecosystem of open, participatory, and inclusive knowledge infrastructures. 

To ensure that this technical development is grounded in meaningful collaboration and ethical practice, HerStory follows a roadmap articulated through five interconnected phases. The process begins with conceptualization, where the research questions and technical requirements are framed. This is followed by the design phase, which focuses on developing the knowledge graph schema and the AI architecture. In the development phase, the core components—the LLM and KG—are built and integrated. These are then subject to implementation and testing, validated in real-world archival and research scenarios to ensure usability and accuracy. Finally, the project enters a phase of co-creation and citizen science, engaging communities in participatory design and evaluation. Through this iterative and inclusive methodology, HerStory aims not only to build better tools, but to cultivate a model of feminist AI grounded in dialogue, transparency, and collective memory. 

In line with the principles outlined above, the HerStory project adopts a human-centered design approach that shapes both its methodology and the architecture of its AI and knowledge graph. This approach combines advanced techniques for ontology learning with participatory, user-centered methods. The resulting knowledge graph integrates a content-centered dimension—ensuring the most unbiased possible representation of historical events and figures—and a user-centered dimension, tailored to the informational needs and behaviors of those who create, curate, or consult the data. This dual perspective enables the generation of responses that are fairer, more contextually grounded, and aligned with the diversity of users and communities involved. 

Connected to the previous points, we observe that the very conception of the Hertory project is characterized by Human-Centered Design, specifically focusing on user-centered design. This emphasis profoundly impacts both the methodology applied throughout the various phases of the project and the nature of the AI and Knowledge Graph system being developed, as well as the design of its fundamental component—the knowledge graph.  

Regarding methodology, during the design phase of the knowledge graph schema and AI architecture, advanced methodologies for semi-automatic learning of ontologies and knowledge graphs from documentary corpora will be combined with user experience and information architecture methods applied to information systems. Here, user-centered design and, consequently, user participation not only define the goals but also shape the design and implementation processes. Engaging with users through both qualitative and quantitative research, as well as conceptual design and prototyping methods such as scenarios and card sorting processes, will be integral. This integration of user-centered and content-focused methods will continue into the development phase (building and integrating LLM and KG components) as well as during implementation and testing (validating in real-world archival and research scenarios). 

In this same vein, the knowledge graph, serving as the knowledge base of the RAG (Retrieval-Augmented Generation), incorporates two dimensions: a content-focused dimension and a user-centered dimension. The content-focused dimension ensures that entities (people, events, places, etc.) involved in the narrative are represented without bias, or at least with minimal bias reflecting the historical underrepresentation of certain groups. Meanwhile, the user-centered dimension of the knowledge graph represents the informational needs and behaviors of individuals who create, curate, search, and utilize the data integrated into the content-focused knowledge graph, again striving to be free from bias. The interplay of these two dimensions will facilitate responses to inquiries within the AI system that are (mostly) unbiased and tailored to the specific needs and preferences of its users.