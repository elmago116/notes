# Proceso T√©cnico del Knowledge Graph - Cultura y Censura

**Sistema de procesamiento autom√°tico de datos hist√≥ricos y culturales**

---

## Resumen Ejecutivo

El sistema desarrollado por Pen√≠nsula para el proyecto Cultura y Censura implementa un pipeline de procesamiento autom√°tico que transforma datos estructurados y semi-estructurados en un grafo de conocimiento navegable y consultable mediante inteligencia artificial.

### Arquitectura General
- **Input**: Archivos RDF, CSV, SQL
- **Procesamiento**: Google Generative AI (Gemini) + rdflib
- **Storage**: Neo4j (grafo de propiedades)
- **Output**: Consultas sem√°nticas + visualizaciones interactivas

---

## Flujo de Procesamiento Completo

### Fase 1: Ingesta y Validaci√≥n
El sistema implementa un mecanismo robusto de detecci√≥n autom√°tica de formatos y validaci√≥n de datos:

```python
# Pseudo-c√≥digo del proceso de ingesta
def ingest_file(file_path):
    format = detect_format(file_path)  # Por extensi√≥n y contenido
    if validate_file(file_path, format):
        if not already_processed(file_path):
            copy_to_permanent_storage(file_path)
            return True
    return False
```

### Fase 2: An√°lisis Estructural Diferenciado

#### Para RDF/Turtle:
- Parsing completo con `rdflib`
- An√°lisis de ontolog√≠as y espacios de nombres
- Identificaci√≥n de patrones de datos (Dublin Core, FOAF, etc.)
- Extracci√≥n de estad√≠sticas de predicados

#### Para CSV:
- Detecci√≥n autom√°tica de columnas geogr√°ficas
- Reconocimiento de jerarqu√≠as administrativas
- Identificaci√≥n de coordenadas geoespaciales
- An√°lisis de tipos de datos por columna

#### Para SQL:
- Parsing de sentencias INSERT
- Extracci√≥n de foreign keys y relaciones
- Normalizaci√≥n de identificadores
- Reconstrucci√≥n del esquema relacional

### Fase 3: Extracci√≥n Inteligente de Entidades

El sistema utiliza **Google Generative AI (Gemini)** con prompts especializados que:

1. **Detectan autom√°ticamente** el tipo de datos sin configuraci√≥n previa
2. **Aplican reglas espec√≠ficas** seg√∫n el formato detectado:
   - **RDF**: Interpreta N-Triples y relaciones sem√°nticas
   - **CSV**: Procesa datos tabulares y geogr√°ficos
   - **SQL**: Reconstruye relaciones normalizadas
3. **Generan JSON estructurado** con entidades y relaciones extra√≠das
4. **Normalizan identificadores** para consistencia cross-documento

### Fase 4: Transformaci√≥n a Grafo de Conocimiento

**Almacenamiento en Neo4j:**
- Entidades ‚Üí Nodos con propiedades tipificadas
- Relaciones ‚Üí Aristas con metadatos
- Embeddings vectoriales ‚Üí B√∫squeda sem√°ntica
- √çndices especializados ‚Üí Consultas eficientes

### Fase 5: Enriquecimiento y V√≠nculos

**Procesos autom√°ticos:**
- **Entity linking**: Vinculaci√≥n de entidades similares entre fuentes
- **Co-occurrence analysis**: Relaciones temporales y contextuales
- **Geographic clustering**: Conexiones por proximidad geogr√°fica
- **Source tracking**: Trazabilidad completa hacia documentos originales

---

## Diagrama de Flujo del Sistema

```mermaid
graph TD
    A["üìÑ Datos de Entrada<br/>‚Ä¢ Archivos RDF<br/>‚Ä¢ Archivos CSV<br/>‚Ä¢ Dumps SQL"] --> B["üîç Fase 1: Ingesta<br/>‚Ä¢ Validaci√≥n de formatos<br/>‚Ä¢ Normalizaci√≥n<br/>‚Ä¢ Detecci√≥n de encoding"]
    
    B --> C["ü§ñ Fase 2: An√°lisis<br/>‚Ä¢ Google Gemini AI<br/>‚Ä¢ Fragmentaci√≥n en chunks<br/>‚Ä¢ An√°lisis de contenido"]
    
    C --> D["‚ö° Fase 3: Extracci√≥n<br/>‚Ä¢ Detecci√≥n de entidades<br/>‚Ä¢ Identificaci√≥n de relaciones<br/>‚Ä¢ Tipificaci√≥n sem√°ntica"]
    
    D --> E["üíæ Fase 4: Almacenamiento<br/>‚Ä¢ Neo4j Knowledge Graph<br/>‚Ä¢ Embeddings vectoriales<br/>‚Ä¢ √çndices especializados"]
    
    E --> F["üîó Fase 5: Enriquecimiento<br/>‚Ä¢ Entity linking<br/>‚Ä¢ An√°lisis de co-ocurrencia<br/>‚Ä¢ Clustering geogr√°fico"]
    
    F --> G["üéØ Salidas del Sistema<br/>‚Ä¢ Consultas sem√°nticas<br/>‚Ä¢ Visualizaciones<br/>‚Ä¢ APIs REST/SPARQL"]
    
    subgraph "Tipos de Entidades"
        H1["üë§ Person"]
        H2["üìã Document"] 
        H3["üè¢ Organization"]
        H4["üìç Location"]
        H5["‚öîÔ∏è Event"]
        H6["üí≠ Concept"]
    end
    
    D --> H1
    D --> H2
    D --> H3
    D --> H4
    D --> H5
    D --> H6
    
    subgraph "Optimizaci√≥n y Monitoreo"
        I1["üìä M√©tricas de Rendimiento<br/>‚Ä¢ 100 chunks/proceso<br/>‚Ä¢ &lt;2s latency<br/>‚Ä¢ 99% accuracy"]
        I2["üõ°Ô∏è Gesti√≥n de Errores<br/>‚Ä¢ Resilient processing<br/>‚Ä¢ Automatic rollback<br/>‚Ä¢ Intelligent retry"]
    end
    
    E --> I1
    E --> I2
```

---

## Modelo de Datos y Ontolog√≠a

### Tipos de Entidades Principales

| **Tipo** | **Descripci√≥n** | **Propiedades Clave** | **Casos de Uso** |
|----------|-----------------|----------------------|------------------|
| **Person** | Individuos hist√≥ricos | `full_name`, `nationality`, `birth_date`, `military_unit` | Brigadistas, intelectuales, v√≠ctimas |
| **Document** | Registros archiv√≠sticos | `title`, `content_type`, `author`, `creation_date` | Expedientes, cartas, informes |
| **Organization** | Instituciones | `name`, `type`, `founding_date`, `historical_context` | Brigadas, partidos, sindicatos |
| **Location** | Lugares geogr√°ficos | `name`, `coordinates`, `administrative_level` | Municipios, comarcas, provincias |
| **Site** | Sitios espec√≠ficos | `name`, `conservation_state`, `historical_period` | Fosas comunes, monumentos |
| **Event** | Acontecimientos | `name`, `date`, `location`, `participants` | Batallas, represi√≥n, exile |
| **Concept** | Ideas y temas | `name`, `description`, `domain` | Ideolog√≠as, movimientos, conceptos |

### Taxonom√≠a de Relaciones

#### **Relaciones Documentales**
```cypher
// Ejemplos en Cypher (Neo4j)
(:Document)-[:DOCUMENTS]->(:Person)
(:Person)-[:AUTHORED]->(:Document)
(:Document)-[:MENTIONS]->(:Location|:Event|:Concept)
(:Document)-[:CONTAINS_INFORMATION_ABOUT]->(:Topic)
```

#### **Relaciones Geoespaciales**
```cypher
(:Site)-[:LOCATED_IN]->(:Location)
(:Location)-[:PART_OF]->(:Location)  // Jerarqu√≠a administrativa
(:Entity)-[:COORDINATES_AT]->(coordinates:Point)
(:Person)-[:BORN_IN|:DIED_IN|:ACTIVE_IN]->(:Location)
```

#### **Relaciones Temporales y Sociales**
```cypher
(:Person)-[:BELONGS_TO]->(:Organization)
(:Person)-[:SERVED_IN]->(:Organization)
(:Person)-[:CONTEMPORARY_OF]->(:Person)
(:Person)-[:PARTICIPATED_IN]->(:Event)
```

---

## Especificaciones T√©cnicas

### Rendimiento del Sistema

| **M√©trica** | **Especificaci√≥n** | **Observaciones** |
|-------------|-------------------|-------------------|
| **Throughput** | 100 chunks/proceso | L√≠mite por archivo grande |
| **Latency** | < 2s consulta simple | B√∫squeda sem√°ntica optimizada |
| **Accuracy** | 99% datos procesados | 1% tasa de error aceptable |
| **Scalability** | Procesamiento as√≠ncrono | M√∫ltiples archivos paralelo |

### √çndices y Optimizaci√≥n

```cypher
// √çndices especializados en Neo4j
CREATE INDEX FOR (p:Person) ON (p.full_name)
CREATE INDEX FOR (l:Location) ON (l.coordinates)
CREATE FULLTEXT INDEX entity_search FOR (n:Person|Location|Organization) ON EACH [n.name, n.description]
CREATE VECTOR INDEX embeddings FOR (n:Document) ON (n.embedding) OPTIONS {indexConfig: {`vector.dimensions`: 768}}
```

### Motor de Consultas Sem√°nticas

#### Workflow de B√∫squeda en 5 Pasos:

1. **Vectorizaci√≥n**: `user_query ‚Üí embedding_vector`
2. **Similarity Search**: `cosine_similarity(query_vector, document_vectors)`
3. **Entity Extraction**: `extract_entities_from_similar_chunks()`
4. **Graph Expansion**: `traverse_relationships(entities, max_depth=3)`
5. **Response Generation**: `generate_contextual_response(expanded_context)`

#### C√≥digo de Ejemplo:
```python
async def semantic_search(query: str, session_context: dict):
    # Paso 1: Vectorizaci√≥n
    query_embedding = await gemini_ai.embed(query)
    
    # Paso 2: B√∫squeda vectorial
    similar_chunks = await neo4j.vector_search(
        query_embedding, 
        limit=10, 
        similarity_threshold=0.8
    )
    
    # Paso 3: Extracci√≥n de entidades
    relevant_entities = extract_entities(similar_chunks)
    
    # Paso 4: Expansi√≥n del contexto
    expanded_context = await expand_graph_context(
        relevant_entities,
        relationship_types=['LOCATED_IN', 'CONTEMPORARY_OF', 'PARTICIPATED_IN']
    )
    
    # Paso 5: Generaci√≥n de respuesta personalizada
    response = await gemini_ai.generate_response(
        query=query,
        context=expanded_context,
        session_history=session_context
    )
    
    return response
```

---

## Integraci√≥n y APIs

### Compatibilidad SPARQL

El sistema mantiene compatibilidad con est√°ndares sem√°nticos:

```sparql
# Ejemplo: Consulta SPARQL sobre el grafo
PREFIX dc: <http://purl.org/dc/elements/1.1/>
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
PREFIX geo: <http://www.w3.org/2003/01/geo/wgs84_pos#>

SELECT ?person ?name ?location ?coordinates
WHERE {
    ?person a foaf:Person ;
            foaf:name ?name ;
            dc:spatial ?location .
    ?location geo:lat ?lat ;
              geo:long ?long .
    FILTER(contains(?name, "Fern√°ndez"))
}
```

### API REST Endpoints

```http
# Endpoint principal de consultas
POST /api/v1/query
Content-Type: application/json
{
    "query": "¬øQu√© brigadistas estuvieron en Tarragona?",
    "context": "historical_research",
    "session_id": "user_123_session_456"
}

# Endpoint de ingesta de datos
POST /api/v1/ingest
Content-Type: multipart/form-data
files: [archivo.rdf, datos.csv, dump.sql]

# Endpoint de visualizaci√≥n de grafo
GET /api/v1/graph/visualize?entity_id=person_123&depth=2
```

---

## Robustez y Monitoreo

### Gesti√≥n de Errores

- **Resilient Processing**: Contin√∫a ante errores menores (1% tasa aceptable)
- **Automatic Rollback**: Recuperaci√≥n autom√°tica de operaciones fallidas
- **Intelligent Retry**: Reintentos adaptativos con backoff exponencial
- **Detailed Logging**: Trazabilidad completa para debugging

### Validaci√≥n Cruzada

```python
def validate_extraction_quality(entities, relations, source_document):
    """Validaci√≥n multi-fuente de calidad de extracci√≥n"""
    quality_metrics = {
        'entity_completeness': calculate_entity_coverage(entities, source_document),
        'relationship_accuracy': validate_relationships(relations, domain_ontology),
        'cross_source_consistency': check_consistency_across_sources(entities),
        'semantic_coherence': verify_semantic_coherence(entities, relations)
    }
    
    return quality_metrics['overall_score'] > 0.85  # Threshold de calidad
```

---

## Roadmap T√©cnico

### Pr√≥ximas Implementaciones

1. **Cache Din√°mico**: Optimizaci√≥n de consultas frecuentes
2. **Context Window Expansion**: Escalado de chunks para documentos complejos
3. **Specialized Strategies**: Algoritmos espec√≠ficos para datos geoespaciales
4. **Real-time Updates**: Actualizaci√≥n incremental del grafo
5. **Multi-language Support**: Procesamiento en catal√°n, espa√±ol, ingl√©s

### M√©tricas de Evaluaci√≥n

| **KPI** | **Target** | **Medici√≥n** |
|---------|------------|--------------|
| **Precision** | > 90% | Entity extraction accuracy |
| **Recall** | > 85% | Relationship detection coverage |
| **F1-Score** | > 87% | Balanced precision-recall |
| **User Satisfaction** | > 4.0/5.0 | Query relevance rating |
| **System Uptime** | > 99.5% | Service availability |

---

*Documento t√©cnico actualizado - Sistema Knowledge Graph Cultura y Censura*  
*Versi√≥n 1.0 - Julio 2025* 