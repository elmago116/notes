---
title: Fine-tuning Language Models for Triple Extraction with Data Augmentation
source: https://aclanthology.org/2024.kallm-1.12/
author:
  - "[[ACL Anthology]]"
published: 
created: 2025-05-15
description: Yujia Zhang, Tyler Sadler, Mohammad Reza Taesiri, Wenjie Xu, Marek Reformat. Proceedings of the 1st Workshop on Knowledge Graphs and Large Language Models (KaLLM 2024). 2024.
tags:
  - op/suggestedBytutors
  - tech/Ai/LLM
---
[Yujia Zhang](https://aclanthology.org/people/y/yujia-zhang/),[Tyler Sadler](https://aclanthology.org/people/t/tyler-sadler/),[Mohammad Reza Taesiri](https://aclanthology.org/people/m/mohammad-reza-taesiri/),[Wenjie Xu](https://aclanthology.org/people/w/wenjie-xu/),[Marek Reformat](https://aclanthology.org/people/m/marek-reformat/)

---

##### Abstract

Advanced language models with impressive capabilities to process textual information can more effectively extract high-quality triples, which are the building blocks of knowledge graphs. Our work examines language models’ abilities to extract entities and the relationships between them. We use a diverse data augmentation process to fine-tune large language models to extract triples from the text. Fine-tuning is performed using a mix of trainers from HuggingFace and five public datasets, such as different variations of the WebNLG, SKE, DocRed, FewRel, and KELM. Evaluation involves comparing model outputs with test-set triples based on several criteria, such as type, partial, exact, and strict accuracy.The obtained results outperform ChatGPT and even match or exceed the performance of GPT-4.

Anthology ID:

2024.kallm-1.12

Volume:

[Proceedings of the 1st Workshop on Knowledge Graphs and Large Language Models (KaLLM 2024)](https://aclanthology.org/volumes/2024.kallm-1/)

Month:

August

Year:

2024

Address:

Bangkok, Thailand

Editors:

[Russa Biswas](https://aclanthology.org/people/r/russa-biswas/),[Lucie-Aimée Kaffee](https://aclanthology.org/people/l/lucie-aimee-kaffee/),[Oshin Agarwal](https://aclanthology.org/people/o/oshin-agarwal/),[Pasquale Minervini](https://aclanthology.org/people/p/pasquale-minervini/),[Sameer Singh](https://aclanthology.org/people/s/sameer-singh/),[Gerard de Melo](https://aclanthology.org/people/g/gerard-de-melo/)

Venues:

[KaLLM](https://aclanthology.org/venues/kallm/) | [WS](https://aclanthology.org/venues/ws/)

SIG:

Publisher:

Association for Computational Linguistics

Note:

Pages:

116–124

Language:

URL:

[https://aclanthology.org/2024.kallm-1.12/](https://aclanthology.org/2024.kallm-1.12/)

DOI:

[10.18653/v1/2024.kallm-1.12](https://doi.org/10.18653/v1/2024.kallm-1.12 "To the current version of the paper by DOI")

Bibkey:

Cite (ACL):

Yujia Zhang, Tyler Sadler, Mohammad Reza Taesiri, Wenjie Xu, and Marek Reformat. 2024. [Fine-tuning Language Models for Triple Extraction with Data Augmentation](https://aclanthology.org/2024.kallm-1.12/). In *Proceedings of the 1st Workshop on Knowledge Graphs and Large Language Models (KaLLM 2024)*, pages 116–124, Bangkok, Thailand. Association for Computational Linguistics.

Cite (Informal):

[Fine-tuning Language Models for Triple Extraction with Data Augmentation](https://aclanthology.org/2024.kallm-1.12/) (Zhang et al., KaLLM 2024)

Copy Citation:

PDF:

[https://aclanthology.org/2024.kallm-1.12.pdf](https://aclanthology.org/2024.kallm-1.12.pdf)

---