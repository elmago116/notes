---
title: "Neural-Symbolic Reasoning over Knowledge Graphs: A Survey from a Query Perspective"
source: https://arxiv.org/html/2412.10390v1
author: 
published: 
created: 2025-03-28
description: 
tags:
  - Tech/KG
  - tech/Neural_reasoning
  - Tech/queries
---
Lihui Liu [lihuil2@illinois.edu](https://arxiv.org/html/) Wayne State University Detroit Michigan USA, Zihao Wang [zihaow@illinois.edu](https://arxiv.org/html/) University of Illinois at Urbana Champaign Urbana Illinois USA and Hanghang Tong [htong@illinois.edu](https://arxiv.org/html/) University of Illinois at Urbana Champaign Urbana Illinois USA (2024)

###### Abstract.

Knowledge graph reasoning is pivotal in various domains such as data mining, artificial intelligence, the Web, and social sciences. These knowledge graphs function as comprehensive repositories of human knowledge, facilitating the inference of new information. Traditional symbolic reasoning, despite its strengths, struggles with the challenges posed by incomplete and noisy data within these graphs. In contrast, the rise of Neural Symbolic AI marks a significant advancement, merging the robustness of deep learning with the precision of symbolic reasoning. This integration aims to develop AI systems that are not only highly interpretable and explainable but also versatile, effectively bridging the gap between symbolic and neural methodologies. Additionally, the advent of large language models (LLMs) has opened new frontiers in knowledge graph reasoning, enabling the extraction and synthesis of knowledge in unprecedented ways. This survey offers a thorough review of knowledge graph reasoning, focusing on various query types and the classification of neural symbolic reasoning. Furthermore, it explores the innovative integration of knowledge graph reasoning with large language models, highlighting the potential for groundbreaking advancements. This comprehensive overview is designed to support researchers and practitioners across multiple fields, including data mining, AI, the Web, and social sciences, by providing a detailed understanding of the current landscape and future directions in knowledge graph reasoning.

Knowledge graph reasoning

## 1\. Introduction

A knowledge graph is a graph structure that contains a collection of facts, where nodes represent real-world entities, events, and objects, and edges denote the relationships between two nodes. It is a powerful tool for organizing and connecting information in a way that mimics human thought and learning. Since its debut in 2012,<sup>1</sup> <sup>1</sup> 1 [https://en.wikipedia.org/wiki/Knowledge\_graph](https://en.wikipedia.org/wiki/Knowledge_graph) a variety of knowledge graphs have been generated, including Freebase, Yago, and Wikidata.

Knowledge graph reasoning refers to the process of deriving new knowledge or insights from existing knowledge graphs in response to a query. In essence, the knowledge graph reasoning pipeline comprises three key elements: the input query, background knowledge, and reasoning model, each posing unique challenges. The background knowledge may vary in completeness, influencing the system’s ability to accurately interpret and utilize information. Meanwhile, input queries range from clear and specific to ambiguous or dynamically changing, demanding robust mechanisms for understanding user intent. Furthermore, the reasoning model’s approach, whether inductive or deductive, impacts the system’s ability to draw meaningful conclusions from the available data. Addressing these challenges necessitates adaptable algorithms and techniques to ensure the efficacy and reliability of knowledge graph reasoning across diverse contexts and applications.

Recently, there is a trend to utilize neural-symbolic artificial intelligence to enhance reasoning on knowledge graphs. Since knowledge graphs can be viewed as discrete symbolic representations of knowledge, it is natural to integrate knowledge graphs with neural models to unleash the full potential of neural-symbolic reasoning. Traditional symbolic reasoning is intolerant of ambiguous and noisy data, but knowledge graphs are often incomplete, which brings difficulties to symbolic reasoning. On the contrary, the recent advances in deep learning promote neural reasoning on knowledge graphs, which is robust to ambiguous and noisy data but lacks interpretability compared to symbolic reasoning. Considering the advantages and disadvantages of both methodologies, recent efforts have been made to combine the two reasoning methods for better reasoning on knowledge graphs.

Last but not least, the emergence of large language models (LLMs) has shown impressive natural language capabilities. However, they struggle with logical reasoning that requires structured knowledge. The integration of LLMs with knowledge graphs during the reasoning process represents a promising area of exploration. While some methods have been proposed in this regard, a large part of this topic is unexplored or under-explored.

This survey provides a comprehensive exploration of knowledge graph reasoning for different types of queries. We introduce knowledge graph reasoning for single-hop queries, complex logical queries, and natural language queries. Furthermore, the integration of neural symbolic artificial intelligence techniques is investigated, highlighting innovative methodologies for enhancing reasoning capabilities within knowledge graphs. Lastly, the survey delves into the fusion of Large Language Models (LLMs) with knowledge graph reasoning, showcasing advancements at the intersection of natural language processing and structured data reasoning.

While numerous surveys have explored knowledge graph embedding, few have explicitly addressed knowledge graph reasoning from both query types and neural symbolic perspectives. This paper aims to fill this gap by introducing different topics and offering a comprehensive overview of knowledge graph reasoning from diverse viewpoints. Through detailed classification and elaboration within each category, this paper provides a holistic understanding of the complexities and advancements in knowledge graph reasoning, bridging the gap between different query types and neural symbolic reasoning, and offering insights into future directions for this field.

The remainder of the article is structured as follows. Section [2](https://arxiv.org/html/2412.10390v1#S2 "2. Preliminary ‣ Neural-Symbolic Reasoning over Knowledge Graphs: A Survey from a Query Perspective") provides an overview of the background knowledge closely associated with knowledge graph reasoning and neural symbolic reasoning. Section [3](https://arxiv.org/html/2412.10390v1#S3 "3. Reasoning for Single Hop Query ‣ Neural-Symbolic Reasoning over Knowledge Graphs: A Survey from a Query Perspective") delves into knowledge graph reasoning for single-hop queries, examining various perspectives. Following this, Section [4](https://arxiv.org/html/2412.10390v1#S4 "4. Reasoning for Complex Logic Query ‣ Neural-Symbolic Reasoning over Knowledge Graphs: A Survey from a Query Perspective") explores the intricacies of reasoning with complex logical queries. In Section [5](https://arxiv.org/html/2412.10390v1#S5 "5. Reasoning For Natural Language Query ‣ Neural-Symbolic Reasoning over Knowledge Graphs: A Survey from a Query Perspective"), we scrutinize knowledge graph reasoning for both single-turn and multi-turn queries in natural language. Finally, we draw conclusions based on the insights gained from our survey.

## 2\. Preliminary

In this section, we first formally define knowledge graphs and knowledge graph reasoning.

### 2.1. brief history of knowledge graph #Tech/KG 

Knowledge representation has experienced a long-period history of development in the fields of logic and AI. The idea of graphical knowledge representation firstly dated back to 1956 as the concept of semantic net proposed by Richens \[10\], while the symbolic logic knowledge can go back to the General Problem Solver \[1\] in 1959. The knowledge base is firstly used with knowledge-based systems for reasoning and problemsolving. MYCIN \[2\] is one of the most famous rule-based expert systems for medical diagnosis with a knowledge base of about 600 rules. Later, the community of human knowledge representation saw the development of frame-based language, rule-based, and hybrid representations. Approximately at the end of this period, the Cyc project1 began, aiming at assembling human knowledge. Resource description framework (RDF)2 and Web Ontology Language ( #Tech/KG/OWL)3 were released in turn, and became important standards of the Semantic Web4. Then, many  #Tech/KG open knowledge bases or ontologies #Tech/KG/ontology were published, such as <mark class="hltr-yellow">WordNet, DBpedia, YAGO, and Freebase</mark>. Stokman and Vries \[7\] proposed a modern idea of structure knowledge in a graph in 1988. However, it was in 2012 that the concept of knowledge graph gained great popularity since its first launch by Google’s search engine5, where the knowledge fusion framework called Knowledge Vault \[3\] was proposed to build large-scale knowledge graphs. <mark class="hltr-green">A brief road map of knowledge base history is illustrated in Fig. 10</mark> in Appendix A. Many general knowledge graph databases and domain-specific knowledge bases have been released to facilitate research. We introduce more general and domain-specific knowledge bases in Appendices F-A1 and F-A2.

### 2.2. Definition and Notation

Knowledge graph reasoning refers to the process of using a knowledge graph, a structured representation of knowledge, as the basis for making logical inferences and drawing conclusions. More formally, the research question can be defined as

###### Definition 0.

(Knowledge Graph) Let $G = \left(\right. V , E , R \left.\right)$ be a knowledge graph, where $V$ is the set of entities, $E$ is the set of relationships, $R$ is the set of triples $\left(\right. v_{i} , r_{j} , v_{k} \left.\right)$ denoting relationships between entities, where $v_{i} , v_{k} \in V$ and $r_{j} \in E$ .Knowledge graph reasoning: Answer queries by traversing and reasoning over the graph.

###### Definition 0.

(Knowledge Graph Reasoning) Let $G = \left(\right. V , E , R \left.\right)$ be a knowledge graph, where $V$ is the set of entities, $E$ is the set of relationships, $R$ is the set of triples $\left(\right. v_{i} , r_{j} , v_{k} \left.\right)$ denoting relationships between entities, where $v_{i} , v_{k} \in V$ and $r_{j} \in E$ .Knowledge graph reasoning: Answer queries by traversing and reasoning over the graph.

### 2.3. Symbolic Reasoning

Symbolic reasoning #tech/symbolic_reasoning in knowledge graphs refers to the process of deriving logical conclusions and making inferences based on symbolic representations of entities, relationships, and rules within the graph structure. In this context, symbols represent entities or concepts, while relationships denote connections or associations between them. Symbolic reasoning involves applying logical rules and operations to manipulate these symbols, enabling the system to perform tasks such as deductive reasoning, semantic inference, and knowledge integration. By leveraging symbolic representations and logical reasoning, knowledge graphs can facilitate complex problem-solving, semantic understanding, and decision-making in various domains, ranging from natural language processing to artificial intelligence applications.

![Refer to caption](https://arxiv.org/html/extracted/6036006/pics/survey_framework.png)

Figure 1. Survey framework.

### 2.4. Neural Reasoning

#tech/Neural_reasoning in knowledge graphs refers to the utilization of neural network models to perform reasoning tasks directly on the graph structure. Unlike traditional symbolic reasoning approaches, which rely on explicit rules and logical operations, neural reasoning leverages the power of deep learning techniques to learn implicit patterns and relationships within the graph. Through the use of neural networks, knowledge graphs can capture complex, non-linear dependencies between entities and infer higher-level knowledge from the graph’s interconnected nodes. Neural reasoning methods often involve embedding entities and relationships into continuous vector spaces, allowing neural networks to efficiently process and reason over large-scale knowledge graphs. This approach enables knowledge graphs to handle uncertainty, noise, and incompleteness in the data, making neural reasoning a promising paradigm for various applications, including question answering, recommendation systems, and semantic understanding.

### 2.5. Neural Symbolic Reasoning

Neural symbolic reasoning represents a fusion of neural network-based approaches with symbolic reasoning techniques, aiming to leverage the strengths of both paradigms in handling complex reasoning tasks. In this framework, neural networks are used to learn representations of symbolic entities and relationships within a knowledge graph, capturing both their semantic meanings and structural dependencies. These learned representations are then combined with symbolic reasoning mechanisms to perform logical inference and reasoning tasks. By integrating neural and symbolic components, neural symbolic reasoning approaches strive to overcome the limitations of each individual approach. Neural networks offer the ability to learn from data and handle uncertainty, while symbolic reasoning provides formal logic-based reasoning and interpretability. This hybrid approach has shown promise in various domains, including natural language understanding, knowledge graph reasoning, and automated theorem proving, by enabling more robust and flexible reasoning capabilities.

### 2.6. Deductive Reasoning

Knowledge graph deductive reasoning is a method used to derive new information from existing data within a knowledge graph by applying logical rules. A knowledge graph structures information as a network of entities and their interrelationships, represented in triples of subject-predicate-object. Deductive reasoning in this context involves using established logical rules to infer new facts. For instance, if the knowledge graph contains the triples ”Alice works at XYZ Corp” and ”XYZ Corp is located in New York,” a rule might deduce that ”Alice works in New York.” This process leverages the structured nature of the graph and the logical relationships between entities to expand the knowledge base, ensuring that new conclusions are logically consistent with the existing data.

### 2.7. Inductive Reasoning

Knowledge graph inductive reasoning involves deriving generalized conclusions from specific instances within a knowledge graph. Unlike deductive reasoning, which applies general rules to specific cases, inductive reasoning identifies patterns and regularities in the data to formulate broader generalizations or hypotheses. For example, if a knowledge graph contains numerous instances where employees of various companies in New York tend to have a high turnover rate, inductive reasoning might lead to the hypothesis that companies in New York generally experience higher employee turnover. This approach allows for the generation of new insights and predictive models by examining trends and correlations in the data, providing a foundation for further exploration and hypothesis testing within the structured framework of a knowledge graph.

### 2.8. Abductive Reasoning

Knowledge graph abductive reasoning is a process used to infer the most likely explanation for a given set of observations within a knowledge graph. This type of reasoning seeks to find the best hypothesis that explains the observed data, often dealing with incomplete or uncertain information. For example, if a knowledge graph shows that a person has visited multiple cities known for tech conferences, abductive reasoning might infer that the person is likely involved in the tech industry. Unlike deductive reasoning, which guarantees the truth of the conclusion if the premises are true, or inductive reasoning, which generalizes from specific instances, abductive reasoning focuses on finding the most plausible explanation. This method is particularly useful in situations where there are multiple possible explanations, and it aims to identify the one that best fits the available evidence within the structured relationships of a knowledge graph.

### 2.9. Paper Organization

In this section, we’ve laid the groundwork by defining knowledge graph reasoning and discussing the related background knowledge. Moving forward, we’ll delve into three distinct perspectives: reasoning for single-hop queries, complex logical queries, and natural language questions. Each perspective offers valuable insights into how knowledge graph reasoning operates and evolves in different contexts. By examining these perspectives, we aim to provide a comprehensive understanding of the diverse challenges and advancements within the field of knowledge graph reasoning.The taxonomy of this paper is illustrated in Figure [1](https://arxiv.org/html/2412.10390v1#S2.F1 "Figure 1 ‣ 2.3. Symbolic Reasoning ‣ 2. Preliminary ‣ Neural-Symbolic Reasoning over Knowledge Graphs: A Survey from a Query Perspective").

## 3\. Reasoning for Single Hop Query

Reasoning for single-hop queries is a common task in the field of knowledge graphs, often referred to as knowledge graph completion. The objective here is to predict the tail entity $t$ given the head entity $h$ and the relation $r$ , or conversely, to predict the head entity $h$ given the tail entity $t$ and the relation $r$ . In addition to entity prediction, there is the relation prediction task, where the goal is to predict the relationship $r$ between a given head entity $h$ and a tail entity $t$ . This task can also be considered a specialized form of single-hop query.

A variety of methods have been proposed to address these tasks. In this section, we categorize the different approaches into three main types: symbolic, neural, and neural-symbolic methods, which will be elaborated in the following subsections.

### 3.1. Symbolic Methods

#### 3.1.1. Hard symbolic rule based reasoning.

Symbolic rule reasoning methods rely on logical reasoning and explicit rules within the knowledge graph. They often utilize rule-based or path-based inference techniques to deduce the missing entity or relation. Examples include rule mining algorithms and path ranking methods and so on.

![Refer to caption](https://arxiv.org/html/extracted/6036006/pics/rule_expert_system.png)

Figure 2. Rule based expert system.

One of the earliest symbolic rule reasoning methods can be track back to 1970s, which is the rule-based expert system <sup><a href="https://arxiv.org/html/#fn:2">2</a></sup> as shown in Figure [2](https://arxiv.org/html/2412.10390v1#S3.F2 "Figure 2 ‣ 3.1.1. Hard symbolic rule based reasoning. ‣ 3.1. Symbolic Methods ‣ 3. Reasoning for Single Hop Query ‣ Neural-Symbolic Reasoning over Knowledge Graphs: A Survey from a Query Perspective"). The key idea of rule-based expert system is to apply hard rules iteratively to generate new facts. It is a very straightforward method. Generally, there are three primary components in ruled-based expert system: the inference engine, the knowledge base and the rules defined by experts. The inference engine can be treated as the brain of the reasoning system. It utilizes two methods to infer new knowledge according to the rules. The first method is called forward chaining. The idea is to start with facts and repeatedly apply the given rules to generate new facts, until no more rules can be applied. The second method is called backward chaining. It is goal oriented, where a goal usually refers to the query from the users. The backward chaining approach will apply the given rules by matching the goal to infer the answer. Besides rule-based expert system,

Alongside rule-based expert systems, Prolog <sup><a href="https://arxiv.org/html/#fn:11">11</a></sup> serves as a logic programming language adept at symbolic reasoning through facts and rules, commonly employed in AI applications for knowledge graph querying and manipulation. Similarly, Datalog <sup><a href="https://arxiv.org/html/#fn:9">9</a></sup> functions as a declarative logic programming language extensively used for knowledge graph reasoning. It excels in representing complex relationships and hierarchical structures succinctly, enabling tasks such as inference and consistency checking within knowledge graphs. Its logical foundations offer a robust framework for extracting meaningful insights from interconnected data.

Lastly, OWL (Web Ontology Language) <sup><a href="https://arxiv.org/html/#fn:39">39</a></sup> is a formal language for defining and sharing ontologies within knowledge graphs on the Semantic Web. OWL enables detailed descriptions of entity classes, properties, and relationships, supporting automated reasoning to ensure data consistency and classification. By providing a standard framework, OWL facilitates interoperability and data integration across systems, enhancing the semantic richness and utility of knowledge graphs.

#### 3.1.2. Soft symbolic rule based reasoning.

Despite the idea of hard rule-based reasoning is quite intuitive, it’s not the best solution most of the time. Because it requires experts to build rules based on their past experiences and intuitions. So, sometimes, mistakes may be made. Besides, the reasoning method can be very slow because it adds new facts to the knowledge base repeatedly. More importantly, it is not suitable for real time applications. It also lacks flexibility. For example, The reason process will give a world zero probability even if it only violates one formula. But this is not the real-world case. On example is “smoking causes cancer and friends have similar smoking habits”. These two rules are not always true, because not everyone who smokes gets cancer. And not all friends have similar smoking habits. And soft rules are more useful in this case because if a word violates a formula, it becomes less probable, not impossible.

![Refer to caption](https://arxiv.org/html/extracted/6036006/pics/markov_logic_network.png)

Figure 3. Example of markov logic network.

Symbolic reasoning methods like Markov logic network <sup><a href="https://arxiv.org/html/#fn:49">49</a></sup> is a representitive work to reason based on soft rules. The intuition of Markov Logic Network is to give Each formula an associated weight to reflect how strong a constraint it is. If the weight is higher, it’s a strong constraint, otherwise, it’s a weak constraint. When the weights go to infinite, it becomes hard rule reasoning. Formally, Markov network is an undirected graphical model with Node represent variables. And a potential function is defined for each clique in the graph. The distribution of the Markov network is defined as the multiplication of all the potential functions. When applying Markov network to soft rule reasoning, the idea is to treat the first order logic rules as the templates of Markov networ, and reason according to their weights. For example, “smoking causes cancer and friends have similar smoking habits” are two formulas, and they have weight 1.5 and 1.1 respectively, as shown in Figure [3](https://arxiv.org/html/2412.10390v1#S3.F3 "Figure 3 ‣ 3.1.2. Soft symbolic rule based reasoning. ‣ 3.1. Symbolic Methods ‣ 3. Reasoning for Single Hop Query ‣ Neural-Symbolic Reasoning over Knowledge Graphs: A Survey from a Query Perspective").

Different from Markov logic networks which repeatedly use existing rules to infer knowledge, TensorLog <sup><a href="https://arxiv.org/html/#fn:12">12</a></sup> aims to find answers for a given query by matrix multiplication. The idea of Tensorlog is to formulate the reasoning process as a function and represent each entity as a one-hot vector. When applying the function to the input vector, the result is an n by 1 vector where the ith element denotes the probability that entity i is the answer.

![Refer to caption](https://arxiv.org/html/extracted/6036006/pics/tensorlog.png)

Figure 4. Example of Tensorlog.

An example of Tensorlog is inferring family relations like “uncle” in Figure [4](https://arxiv.org/html/2412.10390v1#S3.F4 "Figure 4 ‣ 3.1.2. Soft symbolic rule based reasoning. ‣ 3.1. Symbolic Methods ‣ 3. Reasoning for Single Hop Query ‣ Neural-Symbolic Reasoning over Knowledge Graphs: A Survey from a Query Perspective"). Given the logic rule $\text{parent} ⁢ \left(\right. X , W \left.\right) \land \text{brother} ⁢ \left(\right. W , Y \left.\right) \rightarrow \text{uncle} ⁢ \left(\right. X , Y \left.\right) \text{parent} \text{brother} \text{uncle}$ , Tensorlog takes the vector of $X$ and multiplies it by the matrix of parent. Then we get $W$ . We multiply the matrix of brother and get the output $Y$ . When the length of the rule becomes longer, and if there are multiple rules, the result is the summation of all the matrix multiplication sequences, and each matrix multiplication sequence has a weight $a$ .

#### 3.1.3. symbolic path based reasoning.

For the rule based reasoning methods, they require the rule to be given. However, it’s not the case usually. We don’t have any rules. One possible solution is path-based reasoning. For the path based reasoning method, no rule is needed. It utilizes different paths in the knowledge graph to infer new information. These paths can be directed or undirected.

The very first method of path reasoning is path ranking algorithm <sup><a href="https://arxiv.org/html/#fn:29">29</a></sup> which treats the random walks around a query node as the relational features. The idea of PRA is to use random walk to find many different paths between two nodes and treat these paths as the feature of the relation, and use a logistic regression model to learn a classifier to predict the truthfulness of the triplet.

After PRA, ProPPR <sup><a href="https://arxiv.org/html/#fn:20">20</a></sup> incorporates vector similarity into random walk inference over KGs to mitigate the feature sparsity inherent in using surface text. Specifically, when conducting a series of edge traversals in a random walk, ProPPR allows the walk to explore edges that exhibit semantic similarity to the given edge types, as defined by vector space embeddings of the edge types. This integration of distributional similarity and symbolic logical inference aims to alleviate the sparsity of the feature space constructed by PRA.

#### 3.1.4. symbolic rule mining.

Rule mining can also be treated as a special type of single hop query answering. Instead of directly answer which entity might be the correct answer, Rule mining aims at deducing general logic rules from the knowledge graphs. The entities derived from the given head entity and the query relation following the logic rules are returned as the answers.

AMIE <sup><a href="https://arxiv.org/html/#fn:19">19</a></sup> delves into logic rule exploration through a two-step process. Initially, it engages in Rule Extending, wherein candidate rules undergo expansion via three distinct operations: Add Dangling Atom, Add Instantiated Atom and Add Closing Atom. Subsequently, in the Rule Pruning phase, it sifts through the rules, discarding those deemed faulty, and selects the confident ones based on predefined evaluation metrics. In terms of implementation, the approach leverages SPARQL queries on graph databases to sift through the knowledge graphs (KGs), identifying suitable facts $\left(\right. h , r , t \left.\right)$ that adhere to the extended rules from the first step and surpass the specified metric thresholds from the second step.

After AMIE, the subsequent algorithm, AMIE+ <sup><a href="https://arxiv.org/html/#fn:18">18</a></sup>, enhances the efficiency of AMIE’s implementation through adjustments to both the Rule Extending process and the evaluation metrics in the Rule Pruning phase. In the Rule Extending stage, AMIE+ selectively extends a rule only if it can be completed before reaching the predefined maximum rule length. To elaborate, it refrains from appending dangling atoms in the final step, which would introduce fresh variables and lead to non-closure. Instead, AMIE+ waits until the last step to incorporate instantiated atoms and closing atoms, thereby ensuring rule closure. Additionally, the SPARQL queries employed for rule search are streamlined. For instance, when appending a dangling atom to a parent rule $R_{p}$ to generate a child rule $R_{c}$ , if the predicate of the new atom already exists in $R_{p}$ , the support for $R_{c}$ remains the same as that of $R_{p}$ . Consequently, recalculating support for $R_{c}$ becomes unnecessary, thus expediting the SPARQL query process.

While AMIE and AMIE+ have found extensive use across various scenarios, they still face scalability challenges when dealing with large knowledge graphs (KGs). This limitation stems from their reliance on projection queries executed via SQL or SPARQL, where reducing the vast search space remains challenging. In response, RLvLR <sup><a href="https://arxiv.org/html/#fn:44">44</a></sup> employs an embedding technique to sample relevant entities and facts pertaining to the target predicate/relation, significantly curtailing the search space. Firstly, RLvLR samples a sub-knowledge graph relevant to the target predicate. Secondly, it utilizes the RESCAL knowledge graph embedding model <sup><a href="https://arxiv.org/html/#fn:43">43</a></sup> to generate embeddings for entities and relations in the subgraph, with the embedding for a predicate augmentation being the average value of associated entity embeddings. Thirdly, RLvLR employs a scoring function based on these embeddings to guide and prune rule search, proving effective in rule extraction. Finally, candidate rules are evaluated based on metrics such as $h ⁢ c$ and $c ⁢ o ⁢ n ⁢ f$ , akin to AMIE, computed efficiently through matrix multiplication. By incorporating the embedding technique, RLvLR significantly enhances the efficiency of the rule search process. Another method RuLES <sup><a href="https://arxiv.org/html/#fn:24">24</a></sup> utilizes the embedding technique to assess the quality of learned rules. It incorporates external text information of entities to derive their embeddings, enabling the calculation of confidence scores for facts. RuLES defines the external quality of a learned rule as the average confidence score of all derived facts. Ultimately, RuLES integrates statistical and embedding measures to more precisely evaluate the quality of learned rules.

#### 3.1.5. Summary

In this section, we explore various methods relevant to symbolic reasoning. We discuss rule-based approaches, which leverage logical rules for inference, as well as path-based methods, which analyze patterns within knowledge graphs. Additionally, we delve into rule mining techniques, which aim to extract logical rules from structured data sources. Each method offers unique insights and capabilities in the realm of symbolic reasoning.

### 3.2. Neural-Symbolic Methods

Neural-symbolic methods aim to combine the strengths of both symbolic and neural methods. They often incorporate symbolic reasoning within a neural framework or use neural networks to enhance symbolic inference.

#### 3.2.1. Knowledge graph embedding

Knowledge graph embedding usually encodes entities as low-dimensional vectors and encodes relations as parametric algebraic operations in the continuous space. The basic idea is to design a score function $f$ which takes the triplet embedding as input, so that a true triplet receives a higher score than a false one. There are a lot of applications which utilize knowledge graph embedding. One of the most common applications is knowledge graph completion. For example, given a head entity and a tail entity, the missing relation is the one which maximizes the score function value. Likewise, given a head entity and a relation, the missing tail entity is the one which, again, maximizes the score function value.

Many KG embedding methods have been developed. The basic idea of TransE <sup><a href="https://arxiv.org/html/#fn:7">7</a></sup> is to view the relation $r$ as the transition from the head entity to the tail entity. Mathematically, it means that ideally, the tail entity $t$ should be the summation of the head entity and the relation. Another method is DistMult <sup><a href="https://arxiv.org/html/#fn:71">71</a></sup>. Similar as TransE, DistMult also embed entities and relations into vectors in the real/encludience space. Different from TransE, DistMult views the relation $r$ as the elementwise weights of the head entity $h$ . Its score function is defined as the weighted sum over all elements of the head entity by the corresponding elements in the relation. So, in DistMult, the ideal tail entity should be $h ⁢ \overset{\cdot}{r}$ . Another method ComplEx <sup><a href="https://arxiv.org/html/#fn:58">58</a></sup> embeds entities and relations in Complex vector space. Each embedding now has a real part and an imaginary part. Given a point $z$ which is $x + i ⁢ y$ in the embedding space, its conjugate $\bar{Z}$ is $x - i ⁢ y$ . The scoring function used in complex is very similar to that of distmult. But We replace $t$ by its conjugate we only taking the real part of the function. Different from dot product, in Complex <sup><a href="https://arxiv.org/html/#fn:58">58</a></sup> Hermitian dot product $< h , r , \bar{t} >$ is asymmetric, where $\bar{t}$ is the complex conjugate of $t$ . Thus it naturally captures the anti-symmetry. Another method is called RotatE <sup><a href="https://arxiv.org/html/#fn:55">55</a></sup>. The key idea of rotatE is to solve the limitions in previous methods. similar to complex, rotatE also represent head and tail entities and relation in complex vector space, Different from complex, all the relation embedding are modelled as rotation from the head entity $h$ to the tail entity $t$ . Compared with other methods, RotatE can support different relation properties, such as symmetry/antisymmetry, inversion, and composition. Other methods such as TransH <sup><a href="https://arxiv.org/html/#fn:68">68</a></sup> embeds knowledge graph into the hyperplane of a specific relationship to measure the distance. TransR <sup><a href="https://arxiv.org/html/#fn:32">32</a></sup> represents entities and relationships in separate entity and relationship spaces. The semantic matching model uses semantic similarity to score the relationship between head entities and tail entities. RESCAL <sup><a href="https://arxiv.org/html/#fn:43">43</a></sup> treats each entity as a vector to capture its implied semantics and uses the relationship matrix to model the interactions between latent factors. QuatE <sup><a href="https://arxiv.org/html/#fn:81">81</a></sup> uses two rotating planes to model the relations to a hyper-complex space. HolE <sup><a href="https://arxiv.org/html/#fn:42">42</a></sup> employs cyclic correlation to represent the composition of the graph. However, neither of these methods captures the structure information of the graph which should be important to the graph.

In addition to point embeddings, recent methods have explored using geometric regions to represent knowledge graphs in embedding spaces. Geometric embedding techniques include regions like boxes and spheres, which are effective in modeling relationships and hierarchical structures within knowledge graphs <sup><a href="https://arxiv.org/html/#fn:60">60</a></sup>. Other approaches employ probabilistic embeddings to represent knowledge graphs. For instance, probability-based embeddings, such as Gaussian distributions, capture uncertainty and variability in the data, providing a probabilistic interpretation of the embeddings <sup><a href="https://arxiv.org/html/#fn:23">23</a></sup>. These methods enhance the expressiveness and flexibility of embeddings, enabling more robust reasoning and inference in various applications.

#### 3.2.2. Neural symbolic rule based reasoning

Neural LP <sup><a href="https://arxiv.org/html/#fn:73">73</a></sup>, which is a generalization of Tensorlog that focuses on learning logical rules with confidence scores. In Tensorlog, the reasoning process is a sequence of matrix multiplication operations. Tensorlog denotes the input query entity as a one-hot vector and each relation as a matrix $R$ . The reasoning results are computed by matrix multiplication, retrieving entities whose entries are non-zero as answers. Neural LP adopts the same idea as Tensorlog. In Neural LP, the authors found that when the rule length increases from 2 to $L$ , the original first matrix multiplication then summation process can be rewritten as the first summation then multiplication process. After changing the order of the operations, the original matrix multiplication process becomes learning the combination of relationships at each step. This process can be modeled by a recurrent neural network (RNN) for $T$ steps. The candidate pool of Neural LP is very large, which leads to a huge search space. So, it’s hard to identify useful rules in the search space. Most of the time the weights may not reflect the importance of rules precisely. To solve these limitations, RNNLogic <sup><a href="https://arxiv.org/html/#fn:45">45</a></sup> treats all logic rules as latent variables. That is, to answer a query, there may be more than 10 or 20 related rules, and we treat all these logic rules as latent variables. In this way, the rule mining problem becomes a rule inference problem. RNNLogic contains two components: the rule generator, which will generate some candidate logic rules for a specific query, and a reasoning predictor, which is used to predict how likely we can find the answer given a logic rule. Different from Neural LP, the search space of RNNLogic is much smaller. Because all logic rules are treated as latent variables, the EM algorithm can be used for inference.

#### 3.2.3. Neural symbolic path based methods

Previously, we introduced symbolic path-based reasoning methods. Neural symbolic path-based methods aim to answer single-hop queries by combining neural and symbolic techniques, utilizing path information for more robust reasoning.

Existing symbolic path ranking methods consider only the direct path information between two entities, neglecting the rich context information surrounding entities. This often leads to suboptimal solutions. To address this, PathCon <sup><a href="https://arxiv.org/html/#fn:62">62</a></sup> incorporates both relational context and relational paths in the reasoning process. Relational context refers to the k-hop neighboring relations of a given entity, while relational paths are the connections between two entities. PathCon encodes relational context using Relational Message Passing to aggregate k-hop content information around a predicate. For encoding relational paths, PathCon identifies all paths between entities $h$ and $t$ of length no greater than k, and then uses an RNN to learn the representation of each path. After learning both context and path information, PathCon combines them. It concatenates the final embeddings of entities $h$ and $t$ and processes them through a neural network to obtain the final context embedding. An attention mechanism aggregates the relational path information. The final output is a probability distribution, computed based on the combined context and path embeddings.

Unlike previous methods, DeepPath <sup><a href="https://arxiv.org/html/#fn:69">69</a></sup> uses reinforcement learning to predict missing links. DeepPath learns paths rather than relying solely on random walks, framing the path-finding process as a Markov decision process. It trains a reinforcement learning agent to discover paths, using these paths as horn clauses for knowledge graph reasoning. In DeepPath, the agent is represented by a neural network, and the answer-finding process is modeled as a Markov decision process. The states are defined as the concatenation of the current entity embedding and the distance between the target and the current entity in the embedding space, while the action space consists of all unique relation types in the knowledge graph.

#### 3.2.4. Neural symbolic rule mining

For all previous symbolic rule mining methods, the focus is on mining Horn clauses. In GraIL <sup><a href="https://arxiv.org/html/#fn:56">56</a></sup>, the authors propose using subgraphs for reasoning, based on the idea that useful rules are contained in the subgraph around the query. GraIL applies graph neural networks (GNNs) to subgraphs surrounding the candidate edge, hypothesizing that subgraph structure provides evidence for inferring relations between nodes. This GNN-based method avoids explicit rule induction while remaining expressive enough to capture logical rules. The reasoning process in GraIL comprises three steps: first, extracting a subgraph around the candidate edge; second, assigning structural labels to nodes based on their distances from the target nodes to encode graph substructure; and third, running a GNN on the extracted subgraph to capture the rules. The GNN in GraIL uses the traditional combine-and-aggregate paradigm, where each node aggregates information from its neighbors using an aggregate function. To distinguish different relation types in the knowledge graph, GraIL employs relation-specific attention to weigh information from different neighbors. During inference, given a triplet $\left(\right. u , r_{t} , v \left.\right)$ , GraIL obtains the subgraph representation through average pooling of all latent node representations. It then concatenates this subgraph representation with the target nodes’ latent representations and a learned embedding of the target relation. These concatenated representations are passed through a linear layer to obtain the score. The model is trained using gradient descent.

#### 3.2.5. Summary

Neural-symbolic reasoning combines the strengths of neural networks and symbolic reasoning to tackle the complexities of knowledge graphs. Traditional symbolic reasoning methods, like rule-based expert systems, employ predefined rules for inference, while path-based approaches like the Path Ranking Algorithm utilize random walks to infer relationships. Hybrid methodologies, such as PathCon, integrate relational context and path information through neural networks, enhancing reasoning capabilities. Embedding techniques, including geometric and probabilistic embeddings, represent entities and relationships in continuous vector spaces, facilitating more flexible knowledge graph operations. Reinforcement learning-based methods, like DeepPath, utilize trained agents to navigate knowledge graphs and predict missing links. By merging neural and symbolic techniques, neural-symbolic reasoning offers a comprehensive approach to understanding and reasoning over complex knowledge graph structures, promising advancements in various applications requiring automated reasoning and inference.

## 4\. Reasoning for Complex Logic Query

In this section, we generalize the query into logically more complex forms <sup><a href="https://arxiv.org/html/#fn:38">38</a></sup> and explain how to solve them using neural and symbolic methods. Compared to simple-hop queries, the additional complexity is introduced by involving more “elements” in logical language, such as multiple predicates, quantifiers, and variables. The fundamental motivation of complex queries also follows the narrative of single-hop query prediction, where we want to derive new knowledge but with more logical constraints. We also refer readers to the earlier survey <sup><a href="https://arxiv.org/html/#fn:67">67</a></sup> in this direction. Both single-hop queries and multi-hop queries fall under the broader category of complex queries. However, to differentiate knowledge graph completion from complex logical query answering, we treat single-hop queries and complex logical queries as two distinct components.

### 4.1. What is Complex Query?

General logical queries follow the definitions of mathematical logic and model theory <sup><a href="https://arxiv.org/html/#fn:38">38</a></sup>. The previous but perhaps not up-to-date survey provides more rigorous definitions and discussions <sup><a href="https://arxiv.org/html/#fn:67">67</a></sup>. Regarding logical language coverage, complex queries studied on knowledge graphs are still preliminary compared to parallel studies in databases <sup><a href="https://arxiv.org/html/#fn:30">30</a></sup> and semantic web research <sup><a href="https://arxiv.org/html/#fn:39">39</a></sup>.

In the literature, a general term describing complex queries on knowledge graphs is the general “first-order query” or “first-order logical query” <sup><a href="https://arxiv.org/html/#fn:47">47</a></sup>. However, recent rigorous characterization <sup><a href="https://arxiv.org/html/#fn:77">77</a></sup> distinguished the queries discussed in the definition, and queries studied in empirical methods and benchmarks are two overlapping query families. The first is existential first-order queries that appeared in definitions of many works <sup><a href="https://arxiv.org/html/#fn:48">48</a></sup>, and the second is the tree-formed queries widely adopted in the empirical construction of benchmarks <sup><a href="https://arxiv.org/html/#fn:66">66</a></sup>. Most empirical results remain credible despite the fine-grained differences in query families.

We hereby introduce the definitions of two queries based on a fragment of the first-order language. A term is either an entity $e \in V$ or a variable. A variable is a non-determined entity whose value can be taken from $V$ . A variable can be either existentially quantified or not. Universally quantified variables are not considered yet in the literature. An atomic formula is $r ⁢ \left(\right. a , b \left.\right)$ where $r \in E$ is the relation. Then, we define the Existential First-Order (EFO) formulae.

###### Definition 0 (Existential First Order Formula).

The set of the existential formulas is the smallest set $\Phi$ that satisfies the following property:

- For atomic formula $r ⁢ \left(\right. a , b \left.\right)$ , itself and its negation $r ⁢ \left(\right. a , b \left.\right) , \neg r ⁢ \left(\right. a , b \left.\right) \in \Phi$ , where $a , b$ are either entities in $V$ or variables, $r$ is the relation in $E$ .
- If $\phi , \psi \in \Phi$ , then $\left(\right. \phi \land \psi \left.\right) , \left(\right. \phi \lor \psi \left.\right) \in \Phi$
- If $\phi \in \Phi$ and $x_{i}$ is any variable, then $\exists x_{i} ⁢ \phi \in \Phi$ .

When there is more than one free variable, an EFO formula is an EFO query. In most previous studies, only one existential variable is considered, leading to the family of EFO-1, denoted as $\Phi$ . The families with more than one variable are titled EFO-k <sup><a href="https://arxiv.org/html/#fn:76">76</a></sup>; so far, there is no specific method targeting EFO-k. The key feature of EFO queries is that the logical negation is only attached to atomic formulas, defined by rule (i). Consequently, one can always move existential quantifiers at the beginning of the formula as the prenex normal form <sup><a href="https://arxiv.org/html/#fn:38">38</a></sup>. Moreover, it is always convenient to reorganize the logical conjunctions and disjunctions into either Disjunctive Normal Form (DNF) or Conjunctive Normal Form (CNF). One common way to define the EFO-1 query is by the DNF and the conjunctive queries.

Specifically, the queries $q$ can be written as follows.

###### Definition 0 (EFO-1 Query).

An EFO-1 query is defined as

<table><tbody><tr><td rowspan="1"><span>(1)</span></td><td></td><td><math><semantics><mrow><mrow><mrow><mi>q</mi> <mo>⁢</mo> <mrow><mo>(</mo> <mi>y</mi> <mo>)</mo></mrow></mrow> <mo>=</mo> <mrow><mrow><mo>∃</mo> <msub><mi>x</mi> <mn>1</mn></msub></mrow><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>x</mi> <mi>k</mi></msub><mo>,</mo><mrow><mrow><msub><mi>π</mi> <mn>1</mn></msub> <mo>⁢</mo> <mrow><mo>(</mo> <mi>y</mi> <mo>)</mo></mrow></mrow> <mo>∨</mo> <mi>⋯</mi> <mo>∨</mo> <mrow><msub><mi>π</mi> <mi>n</mi></msub> <mo>⁢</mo> <mrow><mo>(</mo> <mi>y</mi> <mo>)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation-xml><apply><apply><ci>𝑞</ci> <ci>𝑦</ci></apply> <list><apply><apply><csymbol>subscript</csymbol> <ci>𝑥</ci> <cn>1</cn></apply></apply> <ci>…</ci> <apply><csymbol>subscript</csymbol> <ci>𝑥</ci> <ci>𝑘</ci></apply> <apply><apply><apply><csymbol>subscript</csymbol> <ci>𝜋</ci> <cn>1</cn></apply> <ci>𝑦</ci></apply> <ci>⋯</ci> <apply><apply><csymbol>subscript</csymbol> <ci>𝜋</ci> <ci>𝑛</ci></apply> <ci>𝑦</ci></apply></apply></list></apply></annotation-xml> <annotation>\displaystyle q(y)=\exists x_{1},...,x_{k},\pi_{1}(y)\lor\cdots\lor\pi_{n}(y),</annotation><annotation>italic_q ( italic_y ) = ∃ italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, …, italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT, italic_π start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_y ) ∨ ⋯ ∨ italic_π start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_y ),</annotation></semantics></math></td><td></td></tr></tbody></table>

where $y$ is the variable to be queried, $x_{1} , \ldots , x_{k}$ are existentially quantified variables, and $\pi_{n} ⁢ \left(\right. y \left.\right)$ is the conjunctive query to be defined in the following parts.

###### Definition 0 (Conjunctive Query).

A conjunctive logical query is written as

|  | $\pi_{i} ⁢ \left(\right. y \left.\right) = \exists x_{1} , \ldots , x_{k} : \varrho_{1}^{i} \land \varrho_{2}^{i} \land \ldots \land \varrho_{m}^{i} ,$ |  |
| --- | --- | --- |

where each $\varrho_{j}^{i}$ is the atomic formula $r ⁢ \left(\right. a , b \left.\right)$ or its negation $\neg r ⁢ \left(\right. a , b \left.\right)$ .

Another query family that is well studied is formally defined as the Tree-Formed (TF) queries $\Phi_{tf}$ .

###### Definition 0 (Tree-Form Query).

The set of the Tree-Form queries is the smallest set $\Phi_{tf}$ such that:

- If $\phi ⁢ \left(\right. y \left.\right) = r ⁢ \left(\right. a , y \left.\right)$ , where $a \in E$ , $r \in R$ , then $\phi ⁢ \left(\right. y \left.\right) \in \Phi_{tf}$ ;
- If $\phi ⁢ \left(\right. y \left.\right) \in \Phi_{tf} , \neg \phi ⁢ \left(\right. y \left.\right) \in \Phi_{tf}$ ;
- If $\phi ⁢ \left(\right. y \left.\right) , \psi ⁢ \left(\right. y \left.\right) \in \Phi_{tf}$ , then $\left(\right. \phi \land \psi \left.\right) ⁢ \left(\right. y \left.\right) \in \Phi_{tf}$ $\left(\right. \phi \lor \psi \left.\right) ⁢ \left(\right. y \left.\right) \in \Phi_{tf}$ ;
- If $\phi ⁢ \left(\right. y \left.\right) \in \Phi_{tf}$ and $y^{'}$ is any variable, then $\psi ⁢ \left(\right. y^{'} \left.\right) = \exists y . r ⁢ \left(\right. y , y^{'} \left.\right) \land \phi ⁢ \left(\right. y \left.\right) \in \Phi_{tf}$ .

One key feature of $\Phi_{tf}$ is that the answer set can be constructed recursively through set operations, such as union, intersection, and compliment. As specifically shown in Definition  [4](https://arxiv.org/html/2412.10390v1#S4.Thmtheorem4 "Definition 0 (Tree-Form Query). ‣ 4.1. What is Complex Query? ‣ 4. Reasoning for Complex Logic Query ‣ Neural-Symbolic Reasoning over Knowledge Graphs: A Survey from a Query Perspective"), rule (ii) corresponds to the set complement against an implicit universe set; rule (iii) relates to the set intersection and union to logical conjunction and disjunction; and rule (iv) for set projection. Under the context of tree-form queries, we use logical connectives (conjunction, disjunction, and negation) and set operations (intersection, union, and complement) interchangeably.

EFO-1 and tree-form query families are different but not mutually exclusive (EFO-1 $\cap$ TF $\neq \emptyset$ ). There are also queries in the tree-form family but not in EFO-1 and vice versa. Detailed discussions of query syntax can be found in <sup><a href="https://arxiv.org/html/#fn:77">77</a></sup>. The follow-up part then explains neural and neural-symbolic methods for TF and EFO-1 queries.

### 4.2. Neural Methods

Neural methods conduct logical reasoning in a fixed-dimensional embedding space, where previous insights from knowledge graph embeddings can be applied. The methods for tree-formed queries and EFO-1 queries differ significantly in two ways. In short, methods targeting tree-formed queries emphasize the modeling of set operations <sup><a href="https://arxiv.org/html/#fn:66">66</a></sup>. Methods for the EFO-1 query stress the DNF formulation and the conjunctive query.

#### 4.2.1. Tree-form query

The first attempt to solve a tree-form query begins with the logical conjunction, or more specifically; the final answer set can be derived by set projection and set intersection. As the earliest example, GQE <sup><a href="https://arxiv.org/html/#fn:22">22</a></sup> embeds graph nodes in a low-dimensional space and represents set projection and intersection as neural operations in the embedding space. Consequently, terms in the query, including constant entities, existential variables, and free variables, can be represented or computed as the embedding. Then, the nearest neighbor search is used to find answers. The embeddings and neural models are trained on synthetic datasets by an end-to-end auto-differentiation. Follow-up methods followed the key design principles: (1) represent the terms into low-dimensional embeddings; (2) set operations are modeled by differentiable operations in the embedding spaces; (3) identify the final answers by nearest neighbor search. Moreover, the supported set operations are expanded to set union (logical disjunction) and set complement (logical negation).

Notably, the set intersection, union, and negation provide some natural properties and intuitions. An example is the box-embedding space and various query embedding methods. Query2Box <sup><a href="https://arxiv.org/html/#fn:47">47</a></sup> proposes to model queries as boxes (i.e., hyper-rectangles), where a set of points inside the box corresponds to a set of answer entities of the query. Set intersections can be naturally represented as geometric intersections of boxes in the space. On the other hand, the set union cannot be modeled by the geometric union of boxes because the resulting shape is not a box. This issue can be indirectly addressed by transforming queries into a DNF. Furthermore, NewLook <sup><a href="https://arxiv.org/html/#fn:34">34</a></sup> adopts a neural network to relearn the box embedding at each projection operation to mitigate the cascading error problem and also introduces a new logic operator, *set difference*, so that the set compliment can be modeled by the equivalently. Besides the box-embedding space, other kinds of embedding spaces are also widely explored, including the space of convex cones <sup><a href="https://arxiv.org/html/#fn:83">83</a></sup>, parametric probabilistic distributions <sup><a href="https://arxiv.org/html/#fn:48">48</a></sup>, and vectors <sup><a href="https://arxiv.org/html/#fn:10">10</a></sup>.

#### 4.2.2. EFO-1 query

This part only focuses on methods that are capable of solving queries that are EFO-1 but not tree-form. Such queries are characterized by the query graph of the sub-conjunctive query. Specifically, such a query can be represented as a simple (with multi-edges) or cyclic graph, which prohibits the perspective that regards the logical query as compositional operations <sup><a href="https://arxiv.org/html/#fn:77">77</a></sup>. Instead, a more natural framework is to consider the atomic formulas or their negation in the conjunctive query as constraints in constraint satisfaction problems. One practical approach is to adopt the graph neural networks on the query graph. LMPNN injects the logical constraint in the message and connects the complex query to message-passing networks <sup><a href="https://arxiv.org/html/#fn:65">65</a></sup>. More sophisticated neural architectures like transformers are investigated on the query graph <sup><a href="https://arxiv.org/html/#fn:70">70</a></sup> and messages <sup><a href="https://arxiv.org/html/#fn:79">79</a></sup>.

### 4.3. Neural-Symbolic Methods

The neural-symbolic methods for complex query answering integrate symbolic algorithms, which have been extensively studied in the database community. The neural part of such approaches, on the other hand, is less capable than that of neural approaches. In practice, the neural part of neural symbolic approaches is mainly chosen as link predictors or knowledge graph embeddings. Different from the previous discussions on the neural approach, neural symbolic approaches rely heavily on the symbolic algorithm; thus, they can solve a more extensive set of queries.

Almost all symbolic algorithms search for a proper assignment of variables $x_{1} = e_{1} , \ldots , x_{k} = e_{k}$ , and $y = a$ to satisfy the logical constraints in queries. Combined with neural link predictors, the boolean value of satisfaction is turned into a continuous score or normalized into $\left[\right. 0 , 1 \left]\right.$ as fuzzy truth values. The preliminary approach models the adjacent matrices of KG for each relation, with elements as the fuzzy truth value of triples. The details of how to normalize the output of the link predictor into fuzzy truth values vary in different methods. However, it does not change the nature of the problem as a search process. Several search strategies can be seamlessly applied to such a problem. For example, beam search realized for the acyclic query graph is known as CQD <sup><a href="https://arxiv.org/html/#fn:4">4</a></sup>, and search on acyclic query graph with additional backtracking is proposed as QTO <sup><a href="https://arxiv.org/html/#fn:6">6</a></sup>. The generalization from acyclic to cyclic and multi-edge query graphs is known as FIT <sup><a href="https://arxiv.org/html/#fn:77">77</a></sup>. Many algorithmic results are also available to accelerate the algorithm, such as using a count-min sketch in EMQL <sup><a href="https://arxiv.org/html/#fn:52">52</a></sup> to store the entity set for each query node and using vector similarity to find similar results during the search process.

Neural link predictors can be deeply engaged with search and not just materialized as adjacency matrices. CQD-CO relaxed the search problem from symbolic assignment spaces into neural embedding space, thus enabling gradient-based optimization with differentiable link predictions <sup><a href="https://arxiv.org/html/#fn:4">4</a></sup>. CQD-A, as a more advanced method, can adapt knowledge graph embeddings from the feedback of the training data <sup><a href="https://arxiv.org/html/#fn:5">5</a></sup>. A similar but technically different approach is the GNN-QE, where the link predictor is not just embeddings but a graph neural network to be learned from the feedback of the search results <sup><a href="https://arxiv.org/html/#fn:85">85</a></sup>.

Recently, some approaches have attempted to combine large language models (LLMs) with neural-symbolic methods to address this problem. For instance, in <sup><a href="https://arxiv.org/html/#fn:37">37</a></sup>, a framework is proposed that decomposes complex questions into multiple subquestions, which are then individually answered by LLMs. Simultaneously, neural-symbolic methods are applied to incomplete knowledge graphs. At each time step, the results from the LLMs and knowledge graph reasoning are integrated to produce a cohesive response.

## 5\. Reasoning For Natural Language Query

### 5.1. Reasoning for Single-turn Query

When the input query is a natural language sentence, existing methods can be divided into several categories, such as semantic parsing-based methods, information retrieval-based methods, and embedding-based methods.

For example, PullNet <sup><a href="https://arxiv.org/html/#fn:53">53</a></sup> is information retrieval-based methods that retrieve a subgraph of candidate answers from the knowledge base to guide prediction. KV-Mem <sup><a href="https://arxiv.org/html/#fn:40">40</a></sup> and EmbedKGQA <sup><a href="https://arxiv.org/html/#fn:50">50</a></sup> are embedding and deep learning-based methods that use deep learning networks to embed the question into a point in the embedding space and find answers according to a similarity function. In PrefNet <sup><a href="https://arxiv.org/html/#fn:33">33</a></sup>, a reranking based method is used to rerank all candidate answers to get better results. In semantic parsing-based methods, a general strategy to answer the question is to transform the question into a query graph and search for the answer according to the query graph. For example, in <sup><a href="https://arxiv.org/html/#fn:75">75</a></sup>, Xi et al. propose a model with candidate query graphs ranking and true query graph generation components. By iteratively updating these components, their performance improves. The query graph generated can then be used to search the KG. In <sup><a href="https://arxiv.org/html/#fn:35">35</a></sup>, Liu et al. propose a multi-task model to tackle KGQA and KGC simultaneously. They transform the multi-hop query into a path in the knowledge graph and use it to complete the knowledge graph, jointly training the model for both tasks.

Recently, reinforcement learning-based methods have been used to answer natural language questions on the knowledge graph. Zhang et al. <sup><a href="https://arxiv.org/html/#fn:80">80</a></sup> use a KG as the environment and propose an RL-based agent model to navigate the KG to find answers to input questions. Similarly, in <sup><a href="https://arxiv.org/html/#fn:13">13</a></sup>, authors use RL models to find paths in the KG for answering input queries. Other studies, such as <sup><a href="https://arxiv.org/html/#fn:41">41</a></sup>, integrate RL with other methods to create more human-like systems.

### 5.2. Reasoning for Multi-turn Query

Various approaches have been used to reason for multi-turn questions. For instance, Conquer <sup><a href="https://arxiv.org/html/#fn:27">27</a></sup> notices that whenever there is a reformulation, it is likely that the previous answer was wrong, and when there is a new intent, it’s more likely that the previous answer was correct. Based on this idea, Conquer treats reformulations as implicit feedback to train a reinforcement learning model. The idea is to position multiple reinforcement learning agents on relevant entities, allowing these agents to walk over the knowledge graph to answer in its neighborhood.

Despite the common use of reinforcement learning in conversational question answering, it has many disadvantages. For example, the paths in the knowledge graph found by agents are often very similar, making them hard to distinguish. In this example, all these five paths lead to the answer entity, but they have different intermediate nodes. Besides, the reward is sparse, making the model hard to train. To solve these problems, PRALINE <sup><a href="https://arxiv.org/html/#fn:26">26</a></sup> uses a contrastive representation learning approach to rank KG paths for retrieving the correct answers effectively. Extracted KG paths leading to correct answers are marked as positive, while others are negative. Contrastive learning is ideal since it allows us to identify positive KG paths and answer conversational questions. Besides path information, the entire dialog history, the verbalized answers, and domain information of the conversation are also used to help learn better path embeddings. Continuing along this trajectory, CornNet <sup><a href="https://arxiv.org/html/#fn:36">36</a></sup> advocates for the utilization of language models to generate additional reformulations. This approach aids in enhancing the model’s comprehension of original, concise, and potentially challenging questions, ultimately leading to improved performance.

In <sup><a href="https://arxiv.org/html/#fn:8">8</a></sup>, the authors employed reinforcement learning to train an agent that reformulates input questions to aid the system’s understanding. In <sup><a href="https://arxiv.org/html/#fn:21">21</a></sup>, an encoder-decoder model is used to transform natural language questions into logical queries for finding answers. In <sup><a href="https://arxiv.org/html/#fn:25">25</a></sup>, a Transformer model is used to generate logical forms, and graph attention is introduced to identify entities in the query context. Other systems, such as Google’s Lambda <sup><a href="https://arxiv.org/html/#fn:57">57</a></sup>, Amazon Alexa <sup><a href="https://arxiv.org/html/#fn:3">3</a></sup>, Apple’s Siri, and OpenAI’s ChatGPT, are also pursuing this task.

Question rewriting, which aims to reformulate an input question into a more salient representation, is also used in multi-turn question answering. This can improve the accuracy of search engine results or make a question more understandable for a natural language processing (NLP) system. In <sup><a href="https://arxiv.org/html/#fn:59">59</a></sup>, a unidirectional Transformer decoder is proposed to automatically rewrite a user’s input question to improve the performance of a conversational question answering system. In <sup><a href="https://arxiv.org/html/#fn:15">15</a></sup>, authors propose a Seq2Seq model to rewrite the current question according to the conversational history and introduce a new dataset named CANARD. In <sup><a href="https://arxiv.org/html/#fn:17">17</a></sup>, query rewriting rules are mined from a background KG, and a query rewriting operator is used to generate a new question.

## 6\. LLM with Knowledge Graph Reasoning

![Refer to caption](https://arxiv.org/html/extracted/6036006/pics/llm.png)

Figure 5. Three ways to combine LLMs with knowledge graph reasoning.

with the advent of ChatGPT, large language models have demonstrated great performance in many downstream tasks. Previously, we introduced knowledge graph reasoning. We know that knowledge graphs contain accurate structural knowledge and are very interpretable, however, most knowledge graphs are incomplete, lack language understanding. While language models are general knowledge, they are good at language understanding. However, they suffer from hallucination, lack interpretation, lacking new knowledge. By combining them together, we can build a model that is not only accurate but also interpretable.

When combining knowledge graphs with large language models, there are three different ways. The first category is letting LLMs enhance knowledge graph reasoning, where LLMs serve as a component in the reasoning process. The second category is Knowledge graph reasoning enhance LLMs where knowledge graph reasoning can be used to mitigate the LLM’s hallucination problem. Finally, integrating knowledge graph reasoning with LLMs in a mutually beneficial way, so that they can help each other. Figure [5](https://arxiv.org/html/2412.10390v1#S6.F5 "Figure 5 ‣ 6. LLM with Knowledge Graph Reasoning ‣ Neural-Symbolic Reasoning over Knowledge Graphs: A Survey from a Query Perspective") shows the classification.

#### 6.0.1. Knowledge graph enhances LLMs.

Many methods have been proposed to utilize knowledge graph to boost the LLMs performance. In QA-GNN <sup><a href="https://arxiv.org/html/#fn:74">74</a></sup>, it combines LLM and knowledge graph to answer multi-choice questions. First of all, given a QA context, it will use the language model to encode question to a vector presentation, then use stand method to retrieve a knowledge graph subset, like linking entities and get their neighbors, and reasoning according to the subgraph. Then QA-GNN is based on two core ideas. First, in order to better identify which knowledge graph nodes relevant to current question, they propose language condition KG nodes scoring where they use a pretrained language model to compute the probability of each entity condition on the current question. Secondly, to jointly reason with language models, they connect the question text and kg to form a joint graph, which we call working graph, they mutually update their representations, through graph neural networks. Finally, they combine the representation of the language model and kg to predict the final answer. Following this direction, GreaseLM <sup><a href="https://arxiv.org/html/#fn:82">82</a></sup> merges LM with graph neural network by using a merging layer. To encode the knowledge graph structure information, multiple merging layers are used.

Now, we have introduced how to use knowledge graphs to help retrain language models to better answer different types of questions. However, when the size of the model becomes large, retrain or finetune the model will be very time consuming. An alternative way is to retrieval knowledge from external sources to help the language model generate correct answers. This approach allows for more targeted adjustments without the need for extensive retraining. This type of method is called Retrieval-Augmented Generation short for RAG.

In KG-GPT <sup><a href="https://arxiv.org/html/#fn:28">28</a></sup>, the authors propose a method to utilize language models and knowledge graphs to answer more complex natural language questions. The idea is that a sentence of question, it uses llms to decompose the original sentence to several sub-sentences and find answers for each sub-sentence. And finally, find the answer for the whole sentence. In REPLUG <sup><a href="https://arxiv.org/html/#fn:51">51</a></sup>, a new retrieval-augmented LM framework where the language model is viewed as a black box and the retrieval component is added as a tunable plug-and-play module. Given an input context, REPLUG first retrieves relevant documents from an external corpus. The retrieved documents are prepended to the input context and fed into the black-box LM to make the final prediction. Because the LM context length limits the number of documents that can be prepended, they also introduce a new ensemble scheme that encodes the retrieved documents in parallel with the same black-box LM. Then we pass the concatenation of each retrieved document with the input context through the LM in parallel, and ensemble the predicted probabilities. They have also developed REPLUG LSR. Instead of adjusting the language model to fit the retriever, they adapt the retriever to the language model. they train it to find documents that help improve the language model’s understanding, while keeping the language model unchanged. The training process involves four steps: (1) finding and assessing the relevance of documents, (2) scoring these documents using the language model, (3) adjusting the retrieval model based on how different the retrieval and language model scores are, and (4) updating the index of the data in real-time.

Previous approaches of retrieval-augmented language models can only answer questions where answers are contained locally within regions of text and fail on answering global questions such as “what are the main themes in the dataset?” To solve these questions, the work proposes a method called GraphRAG <sup><a href="https://arxiv.org/html/#fn:14">14</a></sup> which is a two-step process. The approach uses an LLM to build a graph-based text index in two stages: first to derive an entity knowledge graph from the source documents, then to pregenerate community summaries for all groups of closely-related entities. Given a question, each community summary is used to generate a partial response, before all partial responses are again summarized in a final response to the user.

#### 6.0.2. LLMs enhances knowledge graph reasoning.

Traditional methods retrieve information from KGs, augment the prompt accordingly, and feed the increased prompt into LLMs. In this paradigm, LLM plays the role of translator which transfers input questions to machine-understandable command for KG searching and reasoning, but it does not participate in the graph reasoning process directly. Its success depends heavily on the completeness and high quality of KG, which means that if the knowledge graph contains many missing edges, the answer won’t be good. So, recently, some people tried to explore how to mitigate the knowledge graph incompleteness problem by treating the LLM as an agent to travel KGs and perform reasoning based on paths. In Think-on-Graph <sup><a href="https://arxiv.org/html/#fn:54">54</a></sup>, LLM is treated as an agent to traverse on knowledge graph to find answers. Now, let’s see an example, given question “What is the majority party now in the country where Canberra is located?” The llm identitied the topic entity is Canberra, by checking its local neighbourhood, Australia has the highest score. But the LLM notices that there is not enough information to get the answer. So it travels to Australia and check its neighbors.

#### 6.0.3. LLMs and knowledge graph reasoning mutually help each other.

Finally, let us introduce the third part, how to let knowledge graph reasoning and large language models mutually help each other.

One of the most important tasks in knowledge graph reasoning is to learn the embedding for the KG entities. It has been shown that the learned embedding from KG can be used to improve the performance of Pre-trained Language Models. On the other hand, for each node in the knowledge graph, we can find many text descriptions to describe it. This text information can be used to learn the node embedding. The strong ability of pre-trained language model can help us learn the embedding. So, in KEPLER <sup><a href="https://arxiv.org/html/#fn:63">63</a></sup>, they propose a model which can also solve two problems at the same time. In this paper, Roberta is chosen as the encoder. Given a triplet, each node has a text description. Roberta is used to learn entity embedding. And the learned embedding will be used to calculate the knowledge graph embedding loss. On the other hand, conventional masked language model is used. It requires the model to predict the masked token within the text. The final loss is the summation of these two losses.

To leverage the structure information during the reasoning process, In <sup><a href="https://arxiv.org/html/#fn:78">78</a></sup>, this work proposes to use a graph attention network to provide structure-aware entity embeddings for language modeling. More specifically, the language module produces contextual representations as initial embeddings for KG entities and relations given their descriptive text. Then, graph neural network is used to update the node embedding and relation embedding. Next, the learned knowledge graph embedding will be used as the input of the language model. And the output of the language model will be used to solve different downstream tasks. There are several advantages of this method. For example, the joint pre-training effectively projects entities/relations and text into a shared semantic latent space, which eases the semantic matching between them.

## 7\. Conclusion and Future Directions

### 7.1. Conclusion

Knowledge graphs serve as intuitive repositories of human knowledge, facilitating the inference of new information. However, traditional symbolic reasoning struggles with incomplete and noisy data often found in these graphs. Neural Symbolic AI, a fusion of deep learning and symbolic reasoning, offers a promising solution by combining the interpretability of symbolic methods with the robustness of neural approaches. Furthermore, the advent of large language models (LLMs) has opened new frontiers, enhancing knowledge graph reasoning capabilities. We explore these advancements by categorizing knowledge graph reasoning into four main areas: single-hop queries, complex logical queries, natural language questions and LLMs with knowledge graph reasoning. For single-hop queries, we review techniques that efficiently handle direct relationships between entities. Complex logical query reasoning involves multi-hop inference, where we discuss methods that manage intricate relationships and multiple steps of reasoning. Finally, we delve into reasoning for natural language questions, including both single-turn and multi-turn dialogues. This survey aims to provide a comprehensive overview, bridging the gap between different reasoning approaches and offering insights into future research directions.

### 7.2. Future Directions

Despite significant progress in knowledge graph reasoning, several challenges remain unsolved. Current research primarily focuses on reasoning within a single knowledge graph, often overlooking the potential of integrating knowledge from different languages and modalities. To address these gaps, we outline future directions that could advance the field.

The first future direction is reasoning on multi-modal knowledge graphs. These graphs combine structured knowledge with unstructured data such as images, videos, and audio. Reasoning on multi-modal knowledge graphs can uncover valuable insights that single-modal data cannot. For example, it can predict whether two images contain the same building or associate text descriptions with relevant multimedia content. Integrating multiple data types enhances the robustness and applicability of knowledge graph reasoning, making it possible to address more complex and diverse queries.

The second future direction is reasoning on cross-lingual knowledge graphs. Many knowledge graphs exhibit similar patterns despite being described in different languages. Mining these patterns can reveal useful information that transcends linguistic boundaries. Potential research directions include language-agnostic representation learning, multilingual logical rule extraction, and cross-lingual link prediction. These approaches aim to create models that understand and reason across different languages, enabling more inclusive and comprehensive knowledge graph applications. This can significantly enhance global information retrieval, allowing for more effective use of knowledge graphs in multilingual contexts.

## References

1. Ajith Abraham. 2005.Rule-Based expert systems.*Handbook of measuring system design* (2005).[↩](https://arxiv.org/html/#fnref:2 "return to article")
2. A Acharya and S Adhikari. 2021.Alexa Conversations: An Extensible Data-driven Approach for Building Task-oriented Dialogue Systems.(2021).[↩](https://arxiv.org/html/#fnref:3 "return to article")
3. Erik Arakelyan, Daniel Daza, Pasquale Minervini, and Michael Cochez. 2020.Complex query answering with neural link predictors.*arXiv preprint arXiv:2011.03459* (2020).[↩](https://arxiv.org/html/#fnref:4 "return to article") [↩](https://arxiv.org/html/#fnref:4-2 "return to article")
4. Erik Arakelyan, Pasquale Minervini, Daniel Daza, Michael Cochez, and Isabelle Augenstein. 2024.Adapting neural link predictors for data-efficient complex query answering.*Advances in Neural Information Processing Systems* 36 (2024).[↩](https://arxiv.org/html/#fnref:5 "return to article")
5. Yushi Bai, Xin Lv, Juanzi Li, and Lei Hou. 2023.Answering Complex Logical Queries on Knowledge Graphs via Query Computation Tree Optimization. In *Proceedings of the 40th International Conference on Machine Learning*.[↩](https://arxiv.org/html/#fnref:6 "return to article")
6. Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. 2013.Translating embeddings for modeling multi-relational data.*Advances in neural information processing systems* 26 (2013).[↩](https://arxiv.org/html/#fnref:7 "return to article")
7. C Buck and J Bulian. 2017.Ask the Right Questions: Active Question Reformulation with Reinforcement Learning.(2017).[↩](https://arxiv.org/html/#fnref:8 "return to article")
8. Stefano Ceri, Georg Gottlob, Letizia Tanca, et al. 1989.What you always wanted to know about Datalog(and never dared to ask).*IEEE transactions on knowledge and data engineering* 1, 1 (1989), 146–166.[↩](https://arxiv.org/html/#fnref:9 "return to article")
9. Xuelu Chen, Ziniu Hu, and Yizhou Sun. 2022.Fuzzy logic based logical query answering on knowledge graphs. In *Proceedings of the AAAI Conference on Artificial Intelligence*, Vol. 36. 3939–3948.[↩](https://arxiv.org/html/#fnref:10 "return to article")
10. William F Clocksin and Christopher S Mellish. 2003.*Programming in PROLOG*.Springer Science & Business Media.[↩](https://arxiv.org/html/#fnref:11 "return to article")
11. William W Cohen. 2016.Tensorlog: A differentiable deductive database.*arXiv preprint arXiv:1605.06523* (2016).[↩](https://arxiv.org/html/#fnref:12 "return to article")
12. R Das, S Dhuliawala, and M Zaheer. 2017.Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning.[↩](https://arxiv.org/html/#fnref:13 "return to article")
13. Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson. 2024.From Local to Global: A Graph RAG Approach to Query-Focused Summarization.*arXiv preprint arXiv:2404.16130* (2024).[↩](https://arxiv.org/html/#fnref:14 "return to article")
14. A Elgohary, D Peskov, and J Boyd-Graber. 2019.Can You Unpack That? Learning to Rewrite Questions-in-Context. Association for Computational Linguistics.[↩](https://arxiv.org/html/#fnref:15 "return to article")
15. Katrin Erk. 2009.Representing Words as Regions in Vector Space. In *Proceedings of the Thirteenth Conference on Computational Natural Language Learning* (Boulder, Colorado) *(CoNLL ’09)*. Association for Computational Linguistics, USA, 57–65.
16. A Fader, L Zettlemoyer, and O Etzioni. 2014.Open Question Answering over Curated and Extracted Knowledge Bases *(KDD ’14)*. Association for Computing Machinery.[↩](https://arxiv.org/html/#fnref:17 "return to article")
17. Luis Galárraga, Christina Teflioudi, Katja Hose, and Fabian M Suchanek. 2015.Fast rule mining in ontological knowledge bases with AMIE+.*The VLDB Journal* 24, 6 (2015), 707–730.[↩](https://arxiv.org/html/#fnref:18 "return to article")
18. Luis Antonio Galárraga, Christina Teflioudi, Katja Hose, and Fabian Suchanek. 2013.AMIE: association rule mining under incomplete evidence in ontological knowledge bases. In *Proceedings of the 22nd international conference on World Wide Web*. 413–422.[↩](https://arxiv.org/html/#fnref:19 "return to article")
19. Matt Gardner, Partha Talukdar, Jayant Krishnamurthy, and Tom Mitchell. 2014.Incorporating Vector Space Similarity in Random Walk Inference over Knowledge Bases. In *Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)*, Alessandro Moschitti, Bo Pang, and Walter Daelemans (Eds.). Association for Computational Linguistics, Doha, Qatar, 397–406.[https://doi.org/10.3115/v1/D14-1044](https://doi.org/10.3115/v1/D14-1044) [↩](https://arxiv.org/html/#fnref:20 "return to article")
20. D Guo, D Tang, N Duan, M Zhou, and J Yin. 2018.Dialog-to-Action: Conversational Question Answering Over a Large-Scale Knowledge Base. Curran Associates, Inc.[↩](https://arxiv.org/html/#fnref:21 "return to article")
21. Will Hamilton, Payal Bajaj, Marinka Zitnik, Dan Jurafsky, and Jure Leskovec. 2018.Embedding logical queries on knowledge graphs.*Advances in neural information processing systems* 31 (2018).[↩](https://arxiv.org/html/#fnref:22 "return to article")
22. Shizhu He, Kang Liu, Guoliang Ji, and Jun Zhao. 2015.Learning to Represent Knowledge Graphs with Gaussian Embedding. In *Proceedings of the 24th ACM International on Conference on Information and Knowledge Management* (Melbourne, Australia) *(CIKM ’15)*. Association for Computing Machinery, New York, NY, USA, 623–632.[↩](https://arxiv.org/html/#fnref:23 "return to article")
23. Vinh Thinh Ho, Daria Stepanova, Mohamed H Gad-Elrab, Evgeny Kharlamov, and Gerhard Weikum. 2018.Rule learning from knowledge graphs guided by embedding models. In *The Semantic Web–ISWC 2018: 17th International Semantic Web Conference*. Springer, 72–90.[↩](https://arxiv.org/html/#fnref:24 "return to article")
24. E Kacupaj, J Plepi, K Singh, and H Thakkar. \[n. d.\].Conversational Question Answering over Knowledge Graphs with Transformer and Graph Attention Networks. Association for Computational Linguistics.[↩](https://arxiv.org/html/#fnref:25 "return to article")
25. Endri Kacupaj, Kuldeep Singh, Maria Maleshkova, and Jens Lehmann. 2022.Contrastive Representation Learning for Conversational Question Answering over Knowledge Graphs. In *Proceedings of the 31st ACM International Conference on Information & Knowledge Management* *(CIKM ’22)*. Association for Computing Machinery, New York, NY, USA.[↩](https://arxiv.org/html/#fnref:26 "return to article")
26. Magdalena Kaiser, Rishiraj Saha Roy, and Gerhard Weikum. 2021.Reinforcement learning from reformulations in conversational question answering over knowledge graphs. In *Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval*. 459–469.[↩](https://arxiv.org/html/#fnref:27 "return to article")
27. Jiho Kim, Yeonsu Kwon, Yohan Jo, and Edward Choi. 2023.KG-GPT: A general framework for reasoning on knowledge graphs using large language models.*arXiv preprint arXiv:2310.11220* (2023).[↩](https://arxiv.org/html/#fnref:28 "return to article")
28. Ni Lao, Tom Mitchell, and William Cohen. 2011.Random walk inference and learning in a large scale knowledge base. In *Proceedings of the 2011 conference on empirical methods in natural language processing*. 529–539.[↩](https://arxiv.org/html/#fnref:29 "return to article")
29. Leonid Libkin. 2004.*Elements of finite model theory*. Vol. 41.Springer.[↩](https://arxiv.org/html/#fnref:30 "return to article")
30. X Lin and R Socher. \[n. d.\].Multi-Hop Knowledge Graph Reasoning with Reward Shaping. In *EMNLP 2018*.
31. Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. 2015.Learning entity and relation embeddings for knowledge graph completion. In *Proceedings of the AAAI conference on artificial intelligence*, Vol. 29.[↩](https://arxiv.org/html/#fnref:32 "return to article")
32. Lihui Liu, Yuzhong Chen, Mahashweta Das, Hao Yang, and Hanghang Tong. 2023.Knowledge Graph Question Answering with Ambiguous Query. In *Proceedings of the ACM Web Conference 2023*.[↩](https://arxiv.org/html/#fnref:33 "return to article")
33. Lihui Liu, Boxin Du, Heng Ji, ChengXiang Zhai, and Hanghang Tong. 2021.Neural-Answering Logical Queries on Knowledge Graphs. In *Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining* *(KDD ’21)*.[↩](https://arxiv.org/html/#fnref:34 "return to article")
34. Lihui Liu, Boxin Du, Jiejun Xu, Yinglong Xia, and Hanghang Tong. 2022.Joint Knowledge Graph Completion and Question Answering. In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining* (Washington DC, USA) *(KDD ’22)*. Association for Computing Machinery, New York, NY, USA, 1098–1108.[↩](https://arxiv.org/html/#fnref:35 "return to article")
35. Lihui Liu, Blaine Hill, Boxin Du, Fei Wang, and Hanghang Tong. 2024a.Conversational Question Answering with Language Models Generated Reformulations over Knowledge Graph. In *Findings of the Association for Computational Linguistics: ACL 2024*, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 839–850.[https://doi.org/10.18653/v1/2024.findings-acl.48](https://doi.org/10.18653/v1/2024.findings-acl.48) [↩](https://arxiv.org/html/#fnref:36 "return to article")
36. Lihui Liu, Zihao Wang, Ruizhong Qiu, Yikun Ban, Eunice Chan, Yangqiu Song, Jingrui He, and Hanghang Tong. 2024b.Logic Query of Thoughts: Guiding Large Language Models to Answer Complex Logic Queries with Knowledge Graphs.arXiv:2404.04264 \[cs.IR\] [https://arxiv.org/abs/2404.04264](https://arxiv.org/abs/2404.04264) [↩](https://arxiv.org/html/#fnref:37 "return to article")
37. David Marker. 2006.*Model theory: an introduction*. Vol. 217.Springer Science & Business Media.[↩](https://arxiv.org/html/#fnref:38 "return to article") [↩](https://arxiv.org/html/#fnref:38-2 "return to article") [↩](https://arxiv.org/html/#fnref:38-3 "return to article")
38. Deborah L McGuinness, Frank Van Harmelen, et al. 2004.OWL web ontology language overview.*W3C recommendation* 10, 10 (2004), 2004.[↩](https://arxiv.org/html/#fnref:39 "return to article") [↩](https://arxiv.org/html/#fnref:39-2 "return to article")
39. Alexander Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine Bordes, and Jason Weston. 2016.Key-Value Memory Networks for Directly Reading Documents.arXiv:1606.03126 \[cs.CL\] [↩](https://arxiv.org/html/#fnref:40 "return to article")
40. T Misu, K Georgila, A Leuski, and D Traum. 2012.Reinforcement Learning of Question-Answering Dialogue Policies for Virtual Museum Guides. Association for Computational Linguistics.[↩](https://arxiv.org/html/#fnref:41 "return to article")
41. Maximilian Nickel, Lorenzo Rosasco, and Tomaso Poggio. 2016.Holographic embeddings of knowledge graphs. In *Proceedings of the AAAI conference on artificial intelligence*, Vol. 30.[↩](https://arxiv.org/html/#fnref:42 "return to article")
42. Maximilian Nickel, Volker Tresp, Hans-Peter Kriegel, et al. 2011.A three-way model for collective learning on multi-relational data.. In *Icml*, Vol. 11. 3104482–3104584.[↩](https://arxiv.org/html/#fnref:43 "return to article") [↩](https://arxiv.org/html/#fnref:43-2 "return to article")
43. Pouya Ghiasnezhad Omran, Kewen Wang, and Zhe Wang. 2018.Scalable Rule Learning via Learning Representation.. In *IJCAI*. 2149–2155.[↩](https://arxiv.org/html/#fnref:44 "return to article")
44. Meng Qu, Junkun Chen, Louis-Pascal Xhonneux, Yoshua Bengio, and Jian Tang. 2020.Rnnlogic: Learning logic rules for reasoning on knowledge graphs.*arXiv preprint arXiv:2010.04029* (2020).[↩](https://arxiv.org/html/#fnref:45 "return to article")
45. A Radford, J Wu, and R Child. 2018.Language Models are Unsupervised Multitask Learners.(2018).
46. Hongyu Ren, Weihua Hu, and Jure Leskovec. 2020.Query2box: Reasoning over knowledge graphs in vector space using box embeddings.*arXiv preprint arXiv:2002.05969* (2020).[↩](https://arxiv.org/html/#fnref:47 "return to article") [↩](https://arxiv.org/html/#fnref:47-2 "return to article")
47. Hongyu Ren and Jure Leskovec. 2020.Beta embeddings for multi-hop logical reasoning in knowledge graphs.*Advances in Neural Information Processing Systems* 33 (2020), 19716–19726.[↩](https://arxiv.org/html/#fnref:48 "return to article") [↩](https://arxiv.org/html/#fnref:48-2 "return to article")
48. Matthew Richardson and Pedro Domingos. 2006.Markov logic networks.*Machine learning* 62 (2006), 107–136.[↩](https://arxiv.org/html/#fnref:49 "return to article")
49. Apoorv Saxena, Aditay Tripathi, and Partha Talukdar. 2020.Improving multi-hop question answering over knowledge graphs using knowledge base embeddings. In *Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics*. 4498–4507.[↩](https://arxiv.org/html/#fnref:50 "return to article")
50. Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023.Replug: Retrieval-augmented black-box language models.*arXiv preprint arXiv:2301.12652* (2023).[↩](https://arxiv.org/html/#fnref:51 "return to article")
51. Haitian Sun, Andrew Arnold, Tania Bedrax Weiss, Fernando Pereira, and William W Cohen. 2020.Faithful embeddings for knowledge base queries.*Advances in Neural Information Processing Systems* 33 (2020), 22505–22516.[↩](https://arxiv.org/html/#fnref:52 "return to article")
52. Haitian Sun, Tania Bedrax-Weiss, and William W. Cohen. 2019a.PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text.arXiv:1904.09537 \[cs.CL\] [↩](https://arxiv.org/html/#fnref:53 "return to article")
53. Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Heung-Yeung Shum, and Jian Guo. 2023.Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph.*arXiv preprint arXiv:2307.07697* (2023).[↩](https://arxiv.org/html/#fnref:54 "return to article")
54. Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. 2019b.Rotate: Knowledge graph embedding by relational rotation in complex space.*arXiv preprint arXiv:1902.10197* (2019).[↩](https://arxiv.org/html/#fnref:55 "return to article")
55. Komal Teru, Etienne Denis, and Will Hamilton. 2020.Inductive relation prediction by subgraph reasoning. In *International Conference on Machine Learning*. PMLR, 9448–9457.[↩](https://arxiv.org/html/#fnref:56 "return to article")
56. Romal Thoppilan, Daniel De Freitas, and Jamie Hall. 2022.LaMDA: Language Models for Dialog Applications. arXiv.[https://doi.org/10.48550/ARXIV.2201.08239](https://doi.org/10.48550/ARXIV.2201.08239) [↩](https://arxiv.org/html/#fnref:57 "return to article")
57. Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric Gaussier, and Guillaume Bouchard. 2016.Complex embeddings for simple link prediction. In *International conference on machine learning*. PMLR, 2071–2080.[↩](https://arxiv.org/html/#fnref:58 "return to article") [↩](https://arxiv.org/html/#fnref:58-2 "return to article")
58. S Vakulenko, S Longpre, Z Tu, and R Anantha. \[n. d.\].Question Rewriting for Conversational Question Answering *(WSDM ’21)*. Association for Computing Machinery.[↩](https://arxiv.org/html/#fnref:59 "return to article")
59. Luke Vilnis, Xiang Li, Shikhar Murty, and Andrew McCallum. 2018.Probabilistic Embedding of Knowledge Graphs with Box Lattice Measures. In *ACL*.[↩](https://arxiv.org/html/#fnref:60 "return to article")
60. Luke Vilnis and Andrew McCallum. 2015.Word Representations via Gaussian Embedding.arXiv:1412.6623 \[cs.CL\]
61. Hongwei Wang, Hongyu Ren, and Jure Leskovec. 2021b.Relational message passing for knowledge graph completion. In *Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining*. 1697–1707.[↩](https://arxiv.org/html/#fnref:62 "return to article")
62. Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan Zhang, Zhiyuan Liu, Juanzi Li, and Jian Tang. 2021a.KEPLER: A unified model for knowledge embedding and pre-trained language representation.*Transactions of the Association for Computational Linguistics* 9 (2021), 176–194.[↩](https://arxiv.org/html/#fnref:63 "return to article")
63. Zihao Wang, Weizhi Fei, Hang Yin, Yangqiu Song, Ginny Wong, and Simon See. 2023a.Wasserstein-Fisher-Rao Embedding: Logical Query Embeddings with Local Comparison and Global Transport. In *Findings of the Association for Computational Linguistics: ACL 2023*. 13679–13696.
64. Zihao Wang, Yangqiu Song, Ginny Wong, and Simon See. 2023b.Logical Message Passing Networks with One-hop Inference on Atomic Formulas. In *The Eleventh International Conference on Learning Representations*.[↩](https://arxiv.org/html/#fnref:65 "return to article")
65. Zihao Wang, Hang Yin, and Yangqiu Song. 2022a.Benchmarking the Combinatorial Generalizability of Complex Query Answering on Knowledge Graphs.*Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1 (NeurIPS Datasets and Benchmarks 2021)* (2022).[↩](https://arxiv.org/html/#fnref:66 "return to article") [↩](https://arxiv.org/html/#fnref:66-2 "return to article")
66. Zihao Wang, Hang Yin, and Yangqiu Song. 2022b.Logical Queries on Knowledge Graphs: Emerging Interface of Incomplete Relational Data.(2022).[↩](https://arxiv.org/html/#fnref:67 "return to article") [↩](https://arxiv.org/html/#fnref:67-2 "return to article")
67. Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. 2014.Knowledge graph embedding by translating on hyperplanes. In *Proceedings of the AAAI conference on artificial intelligence*, Vol. 28.[↩](https://arxiv.org/html/#fnref:68 "return to article")
68. Wenhan Xiong, Thien Hoang, and William Yang Wang. 2017.Deeppath: A reinforcement learning method for knowledge graph reasoning.*arXiv preprint arXiv:1707.06690* (2017).[↩](https://arxiv.org/html/#fnref:69 "return to article")
69. Yao Xu, Shizhu He, Cunguang Wang, Li Cai, Kang Liu, and Jun Zhao. 2023.Query2Triple: Unified Query Encoding for Answering Diverse Complex Queries over Knowledge Graphs.*arXiv preprint arXiv:2310.11246* (2023).[↩](https://arxiv.org/html/#fnref:70 "return to article")
70. Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. 2014.Embedding entities and relations for learning and inference in knowledge bases.*arXiv preprint arXiv:1412.6575* (2014).[↩](https://arxiv.org/html/#fnref:71 "return to article")
71. Dong Yang, Peijun Qing, Yang Li, Haonan Lu, and Xiaodong Lin. 2022.GammaE: Gamma Embeddings for Logical Queries on Knowledge Graphs. In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing*. 745–760.
72. Fan Yang, Zhilin Yang, and William W Cohen. 2017.Differentiable learning of logical rules for knowledge base completion.*CoRR, abs/1702.08367* (2017).[↩](https://arxiv.org/html/#fnref:73 "return to article")
73. Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, and Jure Leskovec. 2021.QA-GNN: Reasoning with language models and knowledge graphs for question answering.*arXiv preprint arXiv:2104.06378* (2021).[↩](https://arxiv.org/html/#fnref:74 "return to article")
74. Xi Ye, Semih Yavuz, Kazuma Hashimoto, Yingbo Zhou, and Caiming Xiong. 2021.RnG-KBQA: Generation Augmented Iterative Ranking for Knowledge Base Question Answering. In *Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)*.[↩](https://arxiv.org/html/#fnref:75 "return to article")
75. Hang Yin, Zihao Wang, Weizhi Fei, and Yangqiu Song. 2024b.$EFO_{k}$ -CQA: Towards Knowledge Graph Complex Query Answering beyond Set Operation.(2024).[↩](https://arxiv.org/html/#fnref:76 "return to article")
76. Hang Yin, Zihao Wang, and Yangqiu Song. 2024a.Rethinking Complex Queries on Knowledge Graphs with Neural Link Predictors. In *The Twelfth International Conference on Learning Representations*.[↩](https://arxiv.org/html/#fnref:77 "return to article") [↩](https://arxiv.org/html/#fnref:77-2 "return to article") [↩](https://arxiv.org/html/#fnref:77-3 "return to article") [↩](https://arxiv.org/html/#fnref:77-4 "return to article")
77. Donghan Yu, Chenguang Zhu, Yiming Yang, and Michael Zeng. 2020.JAKET: Joint Pre-training of Knowledge Graph and Language Understanding.arXiv:2010.00796 \[cs.CL\] [↩](https://arxiv.org/html/#fnref:78 "return to article")
78. Chongzhi Zhang, Zhiping Peng, Junhao Zheng, and Qianli Ma. 2024.Conditional logical message passing transformer for complex query answering. In *Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*. 4119–4130.[↩](https://arxiv.org/html/#fnref:79 "return to article")
79. Qixuan Zhang, Xinyi Weng, Guangyou Zhou, Yi Zhang, and Jimmy Xiangji Huang. 2022b.ARL: An adaptive reinforcement learning framework for complex question answering over knowledge base.*Information Processing and Management* 59, 3 (2022), 102933.[https://doi.org/10.1016/j.ipm.2022.102933](https://doi.org/10.1016/j.ipm.2022.102933) [↩](https://arxiv.org/html/#fnref:80 "return to article")
80. Shuai Zhang, Yi Tay, Lina Yao, and Qi Liu. 2019.Quaternion knowledge graph embeddings.*Advances in neural information processing systems* 32 (2019).[↩](https://arxiv.org/html/#fnref:81 "return to article")
81. Xikun Zhang, Antoine Bosselut, Michihiro Yasunaga, Hongyu Ren, Percy Liang, Christopher D Manning, and Jure Leskovec. 2022a.Greaselm: Graph reasoning enhanced language models for question answering.*arXiv preprint arXiv:2201.08860* (2022).[↩](https://arxiv.org/html/#fnref:82 "return to article")
82. Zhanqiu Zhang, Jie Wang, Jiajun Chen, Shuiwang Ji, and Feng Wu. 2021.Cone: Cone embeddings for multi-hop reasoning over knowledge graphs.*Advances in Neural Information Processing Systems* 34 (2021), 19172–19183.[↩](https://arxiv.org/html/#fnref:83 "return to article")
83. Li Zhou, Jianfeng Gao, Di Li, and Heung-Yeung Shum. 2020.The Design and Implementation of XiaoIce, an Empathetic Social Chatbot.*Computational Linguistics* 46, 1 (03 2020), 53–93.[https://doi.org/10.1162/coli\_a\_00368](https://doi.org/10.1162/coli_a_00368) arXiv:https://direct.mit.edu/coli/article-pdf/46/1/53/1847834/coli\_a\_00368.pdf
84. Zhaocheng Zhu, Mikhail Galkin, Zuobai Zhang, and Jian Tang. 2022.Neural-symbolic models for logical queries on knowledge graphs. In *International conference on machine learning*. PMLR, 27454–27478.[↩](https://arxiv.org/html/#fnref:85 "return to article")